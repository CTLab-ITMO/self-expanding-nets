{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPdzrZyCcsX1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_stg0KaA1t-8",
        "outputId": "b2f6813d-3941-4b2c-9834-82503bcf69d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBZya_BRPBUo",
        "outputId": "da165489-c734-4287-ab0c-1db0530238e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"self-expanding-nets\",\n",
        "    name=\"another test\"\n",
        ")"
      ],
      "metadata": {
        "id": "cBDk7-o1PE1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "73495b47-4bd6-4419-d850-c03a5ac66f2e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250209_200703-d5wosctz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/down-shift/self-expanding-nets/runs/d5wosctz' target=\"_blank\">another test</a></strong> to <a href='https://wandb.ai/down-shift/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/down-shift/self-expanding-nets' target=\"_blank\">https://wandb.ai/down-shift/self-expanding-nets</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/down-shift/self-expanding-nets/runs/d5wosctz' target=\"_blank\">https://wandb.ai/down-shift/self-expanding-nets/runs/d5wosctz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/down-shift/self-expanding-nets/runs/d5wosctz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ec6c10baf50>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "j0vPMyO9csX2"
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "\n",
        "SEED = 8642\n",
        "torch.manual_seed(8642)\n",
        "\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abCa34mWcsX3"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "5du0GTQ_csX3"
      },
      "outputs": [],
      "source": [
        "def dense_to_sparse(dense_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    indices = dense_tensor.nonzero(as_tuple=True)\n",
        "    values = dense_tensor[indices]\n",
        "    indices = torch.stack(indices)\n",
        "\n",
        "    sparse_tensor = torch.sparse_coo_tensor(indices, values, dense_tensor.size())\n",
        "    return sparse_tensor\n",
        "\n",
        "\n",
        "def convert_dense_to_sparse_network(model: nn.Module) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Converts a given dense neural network model to a sparse neural network model.\n",
        "\n",
        "    This function recursively iterate through the given model and replaces all instances of\n",
        "    `nn.Linear` layers with `SparseLinear` layers\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The dense neural network model to be converted.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: A new neural network model with sparse layers.\n",
        "    \"\"\"\n",
        "    new_model = model.__class__()\n",
        "\n",
        "    for name, module in model.named_children():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            sparse_weight = dense_to_sparse(module.weight.data)\n",
        "            sparse_bias = dense_to_sparse(module.bias.data)\n",
        "\n",
        "            setattr(new_model, name, ExpandingLinear(sparse_weight, sparse_bias))\n",
        "        else:\n",
        "            setattr(new_model, name, convert_dense_to_sparse_network(module))\n",
        "    return new_model\n",
        "\n",
        "\n",
        "def get_model_last_layer(model):\n",
        "    return model if isinstance(model, SparseModule) else list(model.children())[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "_UM_B9qUcsX3"
      },
      "outputs": [],
      "source": [
        "class NonlinearityMetric(ABC):\n",
        "    def __init__(self, loss_fn):\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @abstractmethod\n",
        "    def calculate(self, model, X_arr, y_arr):\n",
        "        pass\n",
        "\n",
        "\n",
        "# Метрика 1: Средний градиент для каждого ребра\n",
        "class GradientMeanEdgeMetric(NonlinearityMetric):\n",
        "    def calculate(self, model, X_arr, y_arr):\n",
        "        model.eval()\n",
        "        model.zero_grad()\n",
        "\n",
        "        y_pred = model(X_arr).squeeze()\n",
        "        loss = self.loss_fn(y_pred, y_arr)\n",
        "        loss.backward()\n",
        "\n",
        "        last_layer = get_model_last_layer(model)\n",
        "\n",
        "        # Градиенты для разреженных весов\n",
        "        edge_gradients = last_layer.weight_values.grad.abs()\n",
        "        model.zero_grad()\n",
        "        return edge_gradients\n",
        "\n",
        "\n",
        "# Метрика 3: Чувствительность к возмущению для каждого ребра\n",
        "class PerturbationSensitivityEdgeMetric(NonlinearityMetric):\n",
        "    def __init__(self, loss_fn, epsilon=1e-2):\n",
        "        super().__init__(loss_fn)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def calculate(self, model, X_arr, y_arr):\n",
        "        model.eval()\n",
        "\n",
        "        # Оригинальный вывод модели\n",
        "        original_output = model(X_arr).detach()\n",
        "\n",
        "        last_layer = get_model_last_layer(model)\n",
        "        sensitivities = torch.zeros_like(last_layer.weight_values)\n",
        "\n",
        "        # Возмущение каждого веса\n",
        "        for idx in range(last_layer.weight_values.size(0)):\n",
        "            with torch.no_grad():\n",
        "                original_value = last_layer.weight_values[idx].item()\n",
        "                last_layer.weight_values[idx] += self.epsilon\n",
        "\n",
        "                # Пересчет модели с возмущением\n",
        "                perturbed_output = model(X_arr)\n",
        "                sensitivity = (perturbed_output - original_output).abs().mean().item()\n",
        "                sensitivities[idx] = sensitivity\n",
        "\n",
        "                # Восстановление оригинального значения\n",
        "                last_layer.weight_values[idx] = original_value\n",
        "\n",
        "        return sensitivities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "NENK8UZ1csX4"
      },
      "outputs": [],
      "source": [
        "class EdgeFinder:\n",
        "    def __init__(self, metric: NonlinearityMetric, dataloader, device=torch.device('cpu')):\n",
        "        self.metric = metric\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "\n",
        "    def calculate_edge_metric_for_dataloader(self, model, categorical_label: bool = True):\n",
        "        accumulated_grads = None\n",
        "        for data, target in self.dataloader:\n",
        "            data, target = data.to(self.device), target.to(self.device)#.to(torch.float32)\n",
        "\n",
        "            if not categorical_label:\n",
        "                target = target.to(torch.float32)\n",
        "\n",
        "            metric = self.metric.calculate(model, data, target)\n",
        "\n",
        "            if accumulated_grads is None:\n",
        "                accumulated_grads = torch.zeros_like(metric).to(self.device)\n",
        "\n",
        "            accumulated_grads += metric\n",
        "\n",
        "        return accumulated_grads / len(self.dataloader)\n",
        "\n",
        "    def choose_edges_top_k(self, model, top_k: int):\n",
        "        avg_metric = self.calculate_edge_metric_for_dataloader(model)\n",
        "        sorted_indices = torch.argsort(avg_metric, descending=True)\n",
        "        last_layer = get_model_last_layer(model)\n",
        "        return last_layer.weight_indices[:, sorted_indices[:top_k]]\n",
        "\n",
        "    def choose_edges_top_percent(self, model, percent: float):\n",
        "        percent = min(max(percent, 0.0), 1.0)  # percent in [0, 1]\n",
        "        avg_metric = self.calculate_edge_metric_for_dataloader(model)\n",
        "        k = int(percent * avg_metric.numel())\n",
        "        sorted_indices = torch.argsort(avg_metric, descending=True)\n",
        "        last_layer = get_model_last_layer(model)\n",
        "        return last_layer.weight_indices[:, sorted_indices[:k]]\n",
        "\n",
        "    def choose_edges_threshold(self, model, threshold):\n",
        "        avg_metric = self.calculate_edge_metric_for_dataloader(model)\n",
        "        mask = avg_metric > threshold\n",
        "        last_layer = get_model_last_layer(model)\n",
        "        return last_layer.weight_indices[:, mask.nonzero(as_tuple=True)[0]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sparse_recursive(model, train_loader, val_loader, num_epochs, metric,\n",
        "                           edge_replacement_func=None, logging=True,\n",
        "                           expansion_criterion=None, metric_threshold: float = 0.05,\n",
        "                           delta_threshold: float = 0.25, n_prev_epochs: int = 3,\n",
        "                           get_n_neurons_func=None, device=None, step_epochs: int = 1):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss_history = []\n",
        "    prev_replacement_epoch = -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            if epoch % step_epochs == 0:\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_targets = []\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_targets, all_preds)\n",
        "        loss_history.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\\n\")\n",
        "\n",
        "        if logging:\n",
        "            wandb.log({\"val_accuracy\": val_accuracy, \"train_loss\": train_loss, \"val_loss\": val_loss})\n",
        "\n",
        "        if edge_replacement_func and (epoch - prev_replacement_epoch) >= n_prev_epochs and expansion_criterion:\n",
        "            if expansion_criterion(loss_history, n_prev_epochs, delta_threshold):\n",
        "                if get_n_neurons_func:\n",
        "                    n_neurons = get_n_neurons_func(loss_history, n_prev_epochs, delta_threshold)\n",
        "                else:\n",
        "                    n_neurons = 2\n",
        "                edge_replacement_func(model, optimizer, val_loader, metric,\n",
        "                                      metric_threshold, n_neurons)\n",
        "                prev_replacement_epoch = epoch\n",
        "                print(\"Replacement done\\n\")\n",
        "            else:\n",
        "                print(\"Replacement denied\\n\")\n"
      ],
      "metadata": {
        "id": "g_BtC_0XQ1nB"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New model"
      ],
      "metadata": {
        "id": "H-CFfvB_cOJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseModule(ABC, nn.Module):\n",
        "    def __init__(self, weight_size, device='cpu', eps: float = 1e-4):\n",
        "        super(SparseModule, self).__init__()\n",
        "        self.weight_indices = torch.empty(2, 0, dtype=torch.long, device=device)\n",
        "        self.weight_values = nn.Parameter(torch.empty(0, device=device))\n",
        "        self.weight_size = list(weight_size)\n",
        "        self.device = device\n",
        "        self.eps = eps\n",
        "\n",
        "    def add_edge(self, child, parent, n_neurons: int):\n",
        "        assert n_neurons >= 1\n",
        "\n",
        "        new_edge = torch.tensor([[child, parent]], dtype=torch.long, device=self.device).t()\n",
        "        self.weight_indices = torch.cat([self.weight_indices, new_edge], dim=1)\n",
        "\n",
        "        new_weight = torch.empty(1, device=self.device)\n",
        "        weight_value = 1 / n_neurons\n",
        "        new_weight.uniform_(weight_value - self.eps, weight_value + self.eps)  # TODO: not only ReLU\n",
        "        print(f\"new edge value: {new_weight}\")\n",
        "\n",
        "        self.weight_values.data = torch.cat([self.weight_values.data, new_weight])\n",
        "\n",
        "    def create_sparse_tensor(self):\n",
        "        return torch.sparse_coo_tensor(self.weight_indices, self.weight_values, self.weight_size, device=self.device)\n",
        "\n",
        "    @abstractmethod\n",
        "    def replace(self, child, parent, n_neurons: int = 2):\n",
        "        pass\n",
        "\n",
        "    def replace_many(self, children, parents, n_neurons: int = 2):\n",
        "        for c, p in zip(children, parents):\n",
        "            self.replace(c, p, n_neurons)\n",
        "\n",
        "\n",
        "class EmbedLinear(SparseModule):\n",
        "    def __init__(self, weight_size, activation=nn.ReLU(), device='cpu'):\n",
        "        super(EmbedLinear, self).__init__([0, weight_size], device=device)\n",
        "        self.child_counter = 0\n",
        "        self.activation = activation\n",
        "        self.device = device\n",
        "\n",
        "    def replace(self, child, parent, n_neurons: int = 2):\n",
        "        for i in range(n_neurons):\n",
        "            self.add_edge(self.child_counter + i, parent, n_neurons)\n",
        "        self.weight_size[0] += n_neurons\n",
        "        self.child_counter += n_neurons\n",
        "\n",
        "    def forward(self, input):\n",
        "        sparse_embed_weight = self.create_sparse_tensor()\n",
        "        # print(\"\\nEmbedLinear shapes: \", sparse_embed_weight.shape, input.shape)\n",
        "        output = torch.sparse.mm(sparse_embed_weight, input.t()).t()\n",
        "        return torch.cat([input, self.activation(output)], dim=1)\n",
        "\n",
        "\n",
        "class ExpandingLinear(SparseModule):\n",
        "    def __init__(self, weight: torch.sparse_coo_tensor, bias: torch.sparse_coo_tensor, device='cpu'):\n",
        "        super(ExpandingLinear, self).__init__(weight.size(), device=device)\n",
        "\n",
        "        weight = weight.coalesce()\n",
        "        self.weight_indices = weight.indices().to(device)\n",
        "        self.weight_values = nn.Parameter(weight.values().to(device))\n",
        "\n",
        "        self.embed_linears = []\n",
        "\n",
        "        bias = bias.coalesce()\n",
        "        self.bias_indices = bias.indices().to(device)\n",
        "        self.bias_values = nn.Parameter(bias.values().to(device))\n",
        "        self.bias_size = list(bias.size())\n",
        "\n",
        "        self.current_iteration = -1\n",
        "        self.device = device\n",
        "\n",
        "    def replace(self, child, parent, n_neurons: int = 2):\n",
        "        if self.current_iteration == -1:\n",
        "            self.current_iteration = 0\n",
        "\n",
        "        if len(self.embed_linears) <= self.current_iteration:\n",
        "            self.embed_linears.append(EmbedLinear(self.weight_size[1], device=self.device))\n",
        "\n",
        "        matches = (self.weight_indices[0] == child) & (self.weight_indices[1] == parent)\n",
        "\n",
        "        assert torch.any(matches), \"Edge must extist\"\n",
        "\n",
        "        max_parent = self.weight_indices[1].max().item() + 1  # n_neurons # before deleting edge\n",
        "\n",
        "        self.weight_indices = self.weight_indices[:, ~matches]\n",
        "        self.weight_values = nn.Parameter(self.weight_values[~matches])\n",
        "\n",
        "        for i in range(n_neurons):\n",
        "            self.add_edge(child, max_parent + i, n_neurons)\n",
        "\n",
        "        self.weight_size[1] += n_neurons\n",
        "        self.embed_linears[self.current_iteration].replace(child, parent, n_neurons=n_neurons)\n",
        "\n",
        "    def replace_many(self, children, parents, n_neurons: int = 2):\n",
        "        self.current_iteration += (len(children) != 0 and len(parents) != 0)\n",
        "        super().replace_many(children, parents, n_neurons)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for embed_linear in self.embed_linears:\n",
        "            input = embed_linear(input)\n",
        "\n",
        "        sparse_weight = self.create_sparse_tensor()\n",
        "        sparse_bias = torch.sparse_coo_tensor(self.bias_indices, self.bias_values, self.bias_size,\n",
        "                                              device=self.device).to_dense()\n",
        "\n",
        "        try:\n",
        "            output = torch.sparse.mm(sparse_weight, input.t()).t()\n",
        "            output += sparse_bias.unsqueeze(0)\n",
        "        except:\n",
        "            print(sparse_weight.shape, sparse_bias.shape, input.t().shape)\n",
        "            assert 0 == 1\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "VxOVP4CLcP__"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "x00Sjsa5csX4"
      },
      "outputs": [],
      "source": [
        "def edge_replacement_func_new_layer(model, optim, val_loader, metric,\n",
        "                                    threshold: float = 0.05, n_neurons: int = 2):\n",
        "    layer = get_model_last_layer(model)\n",
        "    ef = EdgeFinder(metric, val_loader, device)\n",
        "\n",
        "    vals = ef.calculate_edge_metric_for_dataloader(model)\n",
        "    chosen_edges = ef.choose_edges_threshold(model, threshold)\n",
        "    print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
        "\n",
        "    if len(chosen_edges[0]) == 0:\n",
        "        return {'max': 0, 'sum': 0, 'len': 0, 'len_choose': 0}\n",
        "\n",
        "    layer.replace_many(*chosen_edges, n_neurons=n_neurons)\n",
        "\n",
        "    if layer.embed_linears:\n",
        "        optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
        "    else:\n",
        "        print(\"Empty metric\")\n",
        "        dummy_param = torch.zeros_like(layer.weight_values)\n",
        "        optim.add_param_group({'params': dummy_param})\n",
        "\n",
        "    return {'max': max(vals), 'sum': sum(vals), 'len': len(vals), 'len_choose': len(chosen_edges[0])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "7hQLNdkBcsX4"
      },
      "outputs": [],
      "source": [
        "class SimpleFCN(nn.Module):\n",
        "    def __init__(self, input_size=100):\n",
        "        super(SimpleFCN, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input_size, 50)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class DummyFCN(nn.Module):\n",
        "#     def __init__(self, input_size=100):\n",
        "#         super().__init__()\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc1 = nn.Linear(input_size, 50)\n",
        "#         self.dropout = nn.Dropout(p=0.5)\n",
        "#         # self.fc2 = nn.Linear(50, 50)\n",
        "#         self.fc3 = nn.Linear(50, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.relu(self.fc1(x))\n",
        "#         # x = self.relu(self.fc2(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc3(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "FbZ-qp9G_0nP"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic sublayer size adjustment"
      ],
      "metadata": {
        "id": "EEgNzOkYd1Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expansion_criterion(loss_history, n_prev_epochs: int = 3,\n",
        "                            delta_threshold: float = 0.25) -> bool:\n",
        "    \"\"\"\n",
        "    Idea: extend layer if mean of [|∆loss_i|] over n previous epochs\n",
        "    is smaller than delta_threshold\n",
        "    \"\"\"\n",
        "    # TODO: derivation from mean\n",
        "    arr = np.array(loss_history[-n_prev_epochs:])\n",
        "    deltas = np.array([arr[i + 1] - arr[i] for i in range(len(arr) - 1)])\n",
        "    mean_delta = np.mean(np.abs(deltas))\n",
        "    print(\"Mean delta: \", mean_delta)\n",
        "    return mean_delta < delta_threshold"
      ],
      "metadata": {
        "id": "VIJhPLang7Af"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_n_neurons_by_delta(loss_history, n_prev_epochs: int = 3,\n",
        "                           delta_threshold: float = 0.25, upper_bound: int = 10):\n",
        "    arr = np.array(loss_history[-n_prev_epochs:])\n",
        "    deltas = np.array([arr[i + 1] - arr[i] for i in range(len(arr) - 1)])\n",
        "    mean_delta = np.mean(np.abs(deltas))\n",
        "    n_neurons = min(int(1 / mean_delta), upper_bound)\n",
        "    print(\"Number of new neurons per edge: \", n_neurons)\n",
        "    return n_neurons\n",
        "\n",
        "\n",
        "def get_sqrt_n_neurons_by_delta(loss_history, n_prev_epochs: int = 3,\n",
        "                                delta_threshold: float = 0.25, upper_bound: int = 10):\n",
        "    arr = np.array(loss_history[-n_prev_epochs:])\n",
        "    deltas = np.array([arr[i + 1] - arr[i] for i in range(len(arr) - 1)])\n",
        "    mean_delta = np.mean(np.abs(deltas))\n",
        "    n_neurons = min(int(np.sqrt(1 / mean_delta)), upper_bound)\n",
        "    print(\"Number of new neurons per edge: \", n_neurons)\n",
        "    return n_neurons"
      ],
      "metadata": {
        "id": "Vvc2-7hyQFQk"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "krPGhnTzZ0TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True,\n",
        "                                  download=True, transform=transform)\n",
        "val_dataset = datasets.MNIST(root='./data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "0o6BD4zOcGR7"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "tvu7bPScuIZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From scratch"
      ],
      "metadata": {
        "id": "fuCft0bv9-oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SimpleFCN(input_size=784)\n",
        "# sparse_model = convert_dense_to_sparse_network(model)"
      ],
      "metadata": {
        "id": "QJDZIexfpu2q"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pretrained model"
      ],
      "metadata": {
        "id": "67Ng0q4X1mMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_model = convert_dense_to_sparse_network(SimpleFCN(input_size=784))\n",
        "sparse_model.load_state_dict(torch.load('/content/drive/MyDrive/self_exp_nets/base_mnist.pt', weights_only=True))\n",
        "sparse_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWiPwxiZ16P1",
        "outputId": "ab326e14-8da6-4911-d6a4-787e2f68f738"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleFCN(\n",
              "  (relu): ReLU()\n",
              "  (fc1): ExpandingLinear()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): ExpandingLinear()\n",
              "  (fc3): ExpandingLinear()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freeze all layers except the last one"
      ],
      "metadata": {
        "id": "FDNSy6Z32G49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_model(model, num_trainable_layers: int = 1):\n",
        "    for i in range(len(list(model.children())) - num_trainable_layers):\n",
        "        for param in list(model.children())[i].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "def print_layer_status(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"Layer: {name}, frozen: {not param.requires_grad}\")"
      ],
      "metadata": {
        "id": "FV7erXWH2C2T"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freeze_model(sparse_model, num_trainable_layers=1)"
      ],
      "metadata": {
        "id": "HqMGH0Zk2s38"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_layer_status(sparse_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeB9-PNR3yDA",
        "outputId": "b6773943-915d-4489-cc00-22143cc9ccf5"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: fc1.weight_values, frozen: True\n",
            "Layer: fc1.bias_values, frozen: True\n",
            "Layer: fc2.weight_values, frozen: True\n",
            "Layer: fc2.bias_values, frozen: True\n",
            "Layer: fc3.weight_values, frozen: False\n",
            "Layer: fc3.bias_values, frozen: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check initialization correctness"
      ],
      "metadata": {
        "id": "63Jd0nJaxR6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ext_model = deepcopy(sparse_model)\n",
        "\n",
        "optimizer = optim.Adam(ext_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "metric = GradientMeanEdgeMetric(criterion)\n",
        "metric_threshold = 0.015\n",
        "n_neurons = 2\n",
        "\n",
        "edge_replacement_func_new_layer(ext_model, optimizer, val_loader, metric,\n",
        "                                metric_threshold, n_neurons)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfLVycYvxVNP",
        "outputId": "f7f32b59-7e64-4b61-91c8-339f30a973ec"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen edges: tensor([[ 2,  3,  3,  3,  3,  3,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  7,\n",
            "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9],\n",
            "        [ 3,  3, 11, 18, 21, 39,  0, 30, 31, 46,  6, 11, 15, 18, 20, 21, 39, 42,\n",
            "          2,  3,  6, 21, 31, 36, 37,  0, 27, 30, 31, 35, 42, 46]]) 32\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.4999])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5001])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.5000])\n",
            "new edge value: tensor([0.4999])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max': tensor(0.0249), 'sum': tensor(3.1223), 'len': 500, 'len_choose': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.zeros((1, 784))\n",
        "ext_model(zeros) - sparse_model(zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B6jAIARzvdV",
        "outputId": "baf130d8-6749-450b-fec3-dad0b68d93a9"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0157, 0.3421, 0.1846, 0.0000, 0.0434, 0.3751,\n",
              "         0.6244]], grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.zeros((1, 784))\n",
        "ext_model(zeros) - sparse_model(zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0hBZr3B04cS",
        "outputId": "0daf6253-9ef8-4fa8-c7b1-4a20fe64aebc"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0157, 0.3421, 0.1846, 0.0000, 0.0434, 0.3751,\n",
              "         0.6244]], grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer = get_model_last_layer(ext_model)\n",
        "# ef = EdgeFinder(metric, val_loader, device)\n",
        "\n",
        "# vals = ef.calculate_edge_metric_for_dataloader(ext_model)\n",
        "# chosen_edges = ef.choose_edges_threshold(ext_model, metric_threshold)\n",
        "# print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
        "\n",
        "# if len(chosen_edges[0]) == 0:\n",
        "#     print({'max': 0, 'sum': 0, 'len': 0, 'len_choose': 0})\n",
        "# else:\n",
        "#     layer.replace_many(*chosen_edges, n_neurons=n_neurons)\n",
        "#     if layer.embed_linears:\n",
        "#         optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
        "#     else:\n",
        "#         print(\"Empty metric\")\n",
        "#         dummy_param = torch.zeros_like(layer.weight_values)\n",
        "#         optim.add_param_group({'params': dummy_param})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPA7uHLgyAna",
        "outputId": "95fa6587-6deb-43d2-f63b-1cdd91952221"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen edges: tensor([], size=(2, 0), dtype=torch.int64) 0\n",
            "{'max': 0, 'sum': 0, 'len': 0, 'len_choose': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vals.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReS_ZCVcyusa",
        "outputId": "bd401dca-077f-48f9-ee26-27dbdab57185"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0249)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "P_dxPc_KvO0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "❗️TODO:\n",
        "- adjust train loop code to only extend the head\n",
        "- freeze the backbone\n",
        "- add GPU support"
      ],
      "metadata": {
        "id": "BrchJqD6XrgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "ef = EdgeFinder(GradientMeanEdgeMetric(criterion), val_loader, device)"
      ],
      "metadata": {
        "id": "y2wZHgeYSjrg"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_prev_epochs = 5\n",
        "delta_threshold = 0.08\n",
        "metric_threshold = 0.015\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "Qmg78Zloh7vk"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sparse_recursive(sparse_model,\n",
        "                       train_loader,\n",
        "                       val_loader,\n",
        "                       num_epochs=num_epochs,\n",
        "                       metric=GradientMeanEdgeMetric(criterion),\n",
        "                       edge_replacement_func=edge_replacement_func_new_layer,\n",
        "                       expansion_criterion=get_expansion_criterion,\n",
        "                       logging=True,\n",
        "                       delta_threshold=delta_threshold,\n",
        "                       metric_threshold=metric_threshold,\n",
        "                       n_prev_epochs=n_prev_epochs,\n",
        "                      #  get_n_neurons_func=get_sqrt_n_neurons_by_delta,\n",
        "                       device=device,\n",
        "                       step_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHzSaVyk6j7e",
        "outputId": "83d115c6-bb0d-46b4-ab76-26c7eb0c2fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 59.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(sparse_model.state_dict(), '/content/drive/MyDrive/self_exp_nets/base_mnist.pt')"
      ],
      "metadata": {
        "id": "gUbFp01LvK_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = convert_dense_to_sparse_network(SimpleFCN(784))\n",
        "new_model.load_state_dict(torch.load('/content/drive/MyDrive/self_exp_nets/base_mnist.pt', weights_only=True))\n",
        "new_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfQg7cIwvfbo",
        "outputId": "16a0efcb-f49c-411c-b375-f82312322485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleFCN(\n",
              "  (relu): ReLU()\n",
              "  (fc1): ExpandingLinear()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): ExpandingLinear()\n",
              "  (fc3): ExpandingLinear()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_model = deepcopy(new_model)"
      ],
      "metadata": {
        "id": "olDUiiXy1Jvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "h9XrsDtwdk56"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AT_duuxp0wlF"
      },
      "execution_count": 113,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}