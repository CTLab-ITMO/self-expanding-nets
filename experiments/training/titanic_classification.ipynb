{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.318020Z",
     "start_time": "2025-02-11T21:01:25.170016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851a61bbbac63426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.409711Z",
     "start_time": "2025-02-11T21:02:20.359673Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import convert_dense_to_sparse_network\n",
    "from senmodel.metrics.edge_finder import EdgeFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed98f9250fb9b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.443325Z",
     "start_time": "2025-02-11T21:02:20.428808Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1c4b0b6e33d5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:23.590919Z",
     "start_time": "2025-02-11T21:02:20.480333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "data = pd.read_csv(url, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = data['occupation']\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X = data.drop(['occupation'], axis=1)\n",
    "\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d9e408b16a7a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:24.903666Z",
     "start_time": "2025-02-11T21:02:24.884257Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183cc85a9b097f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.139816Z",
     "start_time": "2025-02-11T21:02:25.010546Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7e5ab74bd84c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.182863Z",
     "start_time": "2025-02-11T21:02:25.161851Z"
    }
   },
   "outputs": [],
   "source": [
    "class MulticlassFCN(nn.Module):\n",
    "    def __init__(self, input_size=14, hidden_sizes=None, output_size=15, dropout_rate=0.3):\n",
    "        super(MulticlassFCN, self).__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [128, 64]\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.output = nn.Linear(hidden_sizes[1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        # x = self.dropout2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe92b9bfdd4d7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.288977Z",
     "start_time": "2025-02-11T21:02:25.260801Z"
    }
   },
   "outputs": [],
   "source": [
    "def edge_replacement_func_new_layer(model, optim, val_loader, metric, choose_threshold, aggregation_mode='mean', len_choose=None):\n",
    "    layer = get_model_last_layer(model)\n",
    "    ef = EdgeFinder(metric, val_loader, device, aggregation_mode)\n",
    "    vals = ef.calculate_edge_metric_for_dataloader(model, len_choose, False)\n",
    "    print(\"Edge metrics:\", vals, max(vals), sum(vals))\n",
    "    chosen_edges = ef.choose_edges_threshold(model, choose_threshold, len_choose)\n",
    "    print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
    "    layer.replace_many(*chosen_edges)\n",
    "\n",
    "    if layer.embed_linears:\n",
    "        optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
    "    else:\n",
    "        print(\"Empty metric\")\n",
    "        dummy_param = torch.zeros_like(layer.weight_values)\n",
    "        optim.add_param_group({'params': dummy_param})\n",
    "\n",
    "    return {'max': max(vals), 'sum': sum(vals), 'len': len(vals), 'len_choose': len(chosen_edges[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e4d086a0b3b29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.591134Z",
     "start_time": "2025-02-11T21:02:25.433629Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import freeze_all_but_last, freeze_only_last\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_sparse_recursive(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func=None,\n",
    "                           window_size=3, threshold=0.1, lr=5e-4, choose_threshold=0.3, aggregation_mode='mean'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_losses = []\n",
    "\n",
    "    len_choose=0\n",
    "    is_repl = False\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if is_repl:\n",
    "                freeze_all_but_last(model, len_choose)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(all_targets, all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        new_l = dict()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        if edge_replacement_func and len(val_losses) > window_size:\n",
    "            recent_changes = [abs(val_losses[i] - val_losses[i - 1]) for i in range(-window_size, 0)]\n",
    "            avg_change = sum(recent_changes) / window_size\n",
    "            if avg_change < threshold:\n",
    "                new_l = edge_replacement_func(model, optimizer, val_loader, metric, choose_threshold, aggregation_mode)\n",
    "                # Замораживаем все слои кроме последнего\n",
    "                is_repl = True\n",
    "                val_losses = []\n",
    "                len_choose = new_l['len_choose']\n",
    "\n",
    "        wandb.log({'val_loss': val_loss, 'val_accuracy': val_accuracy, 'train_loss': train_loss} | new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44615f5a401dd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.628149Z",
     "start_time": "2025-02-11T21:02:25.613167Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.metrics.nonlinearity_metrics import *\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [\n",
    "    AbsGradientEdgeMetric(criterion),\n",
    "    SNIPMetric(criterion),\n",
    "    MagnitudeL2Metric(criterion),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c813771bca507d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.557848Z",
     "start_time": "2025-02-11T21:02:25.697276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: fedornigretuk. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10df7d34ecdcb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.805781Z",
     "start_time": "2025-02-11T21:02:39.779777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.5, window_size: 3, threshold: 0.05, lr: 0.0005'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"num_epochs\": 50,\n",
    "               \"metric\": metrics[0],\n",
    "               \"aggregation_mode\": \"mean\",\n",
    "               \"choose_threshold\": 0.5,\n",
    "               \"window_size\": 3,\n",
    "               \"threshold\": 0.05,\n",
    "               \"lr\": 5e-4\n",
    "               }\n",
    "\n",
    "name = \", \".join(\n",
    "    f\"{key}: {value.__class__.__name__ if key == 'metric' else value}\"\n",
    "    for key, value in hyperparams.items()\n",
    ")\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fe9040816bb570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.151538Z",
     "start_time": "2025-02-11T21:02:40.108023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Coding\\PY\\self-expanding-nets\\experiments\\training\\wandb\\run-20250212_013421-jb4tlhgp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jb4tlhgp' target=\"_blank\">titanic-mul, num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.5, window_size: 3, threshold: 0.05, lr: 0.0005</a></strong> to <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jb4tlhgp' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jb4tlhgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.5694 | Val Loss: 31.1613 | Val Accuracy: 0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.3308 | Val Loss: 28.3144 | Val Accuracy: 0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 2.1891 | Val Loss: 26.5613 | Val Accuracy: 0.3341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 2.1033 | Val Loss: 25.7134 | Val Accuracy: 0.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 2.0544 | Val Loss: 25.3495 | Val Accuracy: 0.3413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 2.0340 | Val Loss: 25.1329 | Val Accuracy: 0.3445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 2.0152 | Val Loss: 24.9794 | Val Accuracy: 0.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 1.9940 | Val Loss: 24.8556 | Val Accuracy: 0.3515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 1.9933 | Val Loss: 24.7750 | Val Accuracy: 0.3518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 1.9805 | Val Loss: 24.6915 | Val Accuracy: 0.3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 1.9709 | Val Loss: 24.6393 | Val Accuracy: 0.3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 1.9683 | Val Loss: 24.5541 | Val Accuracy: 0.3567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 1.9559 | Val Loss: 24.4787 | Val Accuracy: 0.3565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 1.9494 | Val Loss: 24.4204 | Val Accuracy: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 1.9494 | Val Loss: 24.3812 | Val Accuracy: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Train Loss: 1.9406 | Val Loss: 24.3193 | Val Accuracy: 0.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Train Loss: 1.9370 | Val Loss: 24.2700 | Val Accuracy: 0.3576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Train Loss: 1.9351 | Val Loss: 24.2473 | Val Accuracy: 0.3581\n",
      "Edge metrics: tensor([9.4894e-04, 2.0444e-03, 2.3937e-03, 2.3087e-03, 1.6914e-03, 1.6797e-03,\n",
      "        6.9757e-04, 1.8258e-03, 2.7026e-03, 5.2839e-03, 2.9755e-03, 7.7162e-04,\n",
      "        9.7331e-04, 4.8224e-03, 3.5771e-03, 3.9439e-03, 8.5969e-04, 3.2543e-05,\n",
      "        4.4376e-03, 1.5407e-03, 1.1774e-03, 1.1652e-03, 4.7739e-04, 1.4720e-03,\n",
      "        2.2479e-03, 2.1781e-03, 6.4895e-04, 2.3110e-03, 3.1873e-03, 3.3998e-03,\n",
      "        9.9108e-04, 2.9689e-04, 2.1952e-03, 3.4844e-03, 2.9643e-03, 3.0184e-03,\n",
      "        5.3764e-03, 2.2150e-03, 4.0635e-04, 2.6338e-03, 5.0024e-04, 3.2753e-03,\n",
      "        3.2321e-03, 3.6991e-04, 6.6314e-04, 4.3261e-03, 2.3564e-03, 2.1682e-03,\n",
      "        2.9283e-03, 4.0292e-03, 1.0338e-03, 4.4807e-03, 1.7155e-03, 1.2682e-03,\n",
      "        3.1187e-03, 3.5468e-03, 8.3675e-04, 3.0834e-03, 3.3399e-04, 8.7880e-04,\n",
      "        1.7906e-03, 6.2338e-04, 2.1890e-03, 3.7819e-03, 5.1059e-03, 3.7800e-03,\n",
      "        6.0077e-03, 1.0522e-02, 5.6337e-03, 1.1875e-02, 3.2219e-03, 9.0640e-03,\n",
      "        5.9050e-03, 4.9477e-03, 9.4460e-03, 5.1859e-03, 8.2720e-03, 5.8528e-03,\n",
      "        1.1654e-02, 3.6065e-03, 4.3331e-03, 2.6247e-03, 6.8439e-03, 1.1821e-02,\n",
      "        5.2801e-03, 4.3550e-03, 8.4871e-03, 4.9339e-03, 5.3608e-03, 1.0570e-02,\n",
      "        2.9252e-03, 6.2380e-03, 7.7723e-03, 1.1580e-02, 6.5954e-03, 6.0427e-03,\n",
      "        6.3314e-03, 9.7749e-03, 4.4270e-03, 5.4641e-03, 4.4958e-03, 5.4329e-03,\n",
      "        8.0732e-03, 2.7575e-03, 5.6883e-03, 1.2017e-02, 4.8470e-03, 2.9656e-03,\n",
      "        6.1834e-03, 9.3268e-03, 1.3207e-02, 5.1674e-03, 3.8478e-03, 4.8078e-03,\n",
      "        5.4095e-03, 3.5874e-03, 6.4678e-03, 5.1729e-03, 4.5020e-03, 4.8515e-03,\n",
      "        7.6411e-03, 7.2240e-03, 2.0577e-03, 3.8312e-03, 7.3987e-03, 6.2852e-03,\n",
      "        5.6625e-03, 5.2232e-03, 7.9321e-05, 1.5150e-04, 2.5394e-04, 1.3035e-04,\n",
      "        3.0622e-04, 2.4747e-04, 6.6204e-05, 2.3398e-04, 2.9190e-04, 4.5609e-04,\n",
      "        1.6517e-04, 6.3765e-05, 1.5244e-04, 3.9581e-04, 5.3152e-04, 3.1959e-04,\n",
      "        2.1700e-04, 3.5275e-05, 3.5869e-04, 1.6543e-04, 6.7077e-05, 5.3099e-05,\n",
      "        1.3242e-04, 2.4744e-04, 2.7378e-04, 1.2810e-04, 1.3266e-04, 3.8520e-04,\n",
      "        5.3116e-04, 5.4939e-04, 3.1852e-04, 1.0949e-04, 2.7761e-04, 3.0360e-04,\n",
      "        4.4637e-04, 3.6072e-04, 3.9852e-04, 3.3246e-04, 1.2283e-04, 3.8112e-04,\n",
      "        1.5693e-04, 6.7630e-04, 3.7636e-04, 1.0875e-04, 2.5074e-04, 4.8997e-04,\n",
      "        3.5754e-04, 4.3403e-04, 2.6800e-04, 2.6564e-04, 2.2555e-04, 3.8423e-04,\n",
      "        2.6339e-04, 1.2205e-04, 1.1048e-04, 4.1473e-04, 1.7099e-04, 4.3063e-04,\n",
      "        1.1693e-04, 5.0523e-05, 2.2132e-04, 1.8800e-04, 4.0433e-04, 2.4214e-04,\n",
      "        2.7698e-03, 3.6983e-03, 9.9469e-03, 5.4204e-03, 1.2766e-02, 6.7998e-03,\n",
      "        6.3385e-03, 5.0158e-03, 5.4033e-03, 2.6199e-03, 5.3382e-03, 2.1761e-03,\n",
      "        4.3215e-03, 4.6490e-03, 1.2848e-02, 2.0805e-03, 9.9445e-03, 6.1624e-03,\n",
      "        3.8099e-03, 6.0741e-03, 2.3033e-03, 2.1689e-03, 4.8182e-03, 1.0196e-02,\n",
      "        2.3424e-03, 5.6470e-03, 7.7558e-03, 1.2816e-02, 6.5525e-03, 1.5603e-02,\n",
      "        5.2431e-03, 7.7747e-03, 6.2048e-03, 4.7794e-03, 4.2483e-03, 1.9446e-03,\n",
      "        2.5287e-03, 7.4587e-03, 5.7154e-03, 2.6396e-03, 6.7132e-03, 2.0357e-02,\n",
      "        4.6922e-03, 6.2922e-03, 1.0743e-02, 4.5355e-03, 7.2396e-03, 6.6320e-03,\n",
      "        2.1971e-03, 2.5503e-03, 1.2462e-02, 1.9525e-03, 2.8266e-03, 6.6932e-03,\n",
      "        2.0741e-03, 6.0219e-03, 6.9960e-03, 1.0933e-02, 2.3381e-03, 1.3705e-03,\n",
      "        4.9657e-03, 6.5940e-03, 6.7672e-03, 3.6711e-03, 1.2273e-02, 4.4545e-03,\n",
      "        1.6796e-02, 1.0454e-02, 2.4957e-02, 1.2558e-02, 7.8567e-03, 5.9553e-03,\n",
      "        1.1546e-02, 4.9418e-03, 1.3213e-02, 9.3981e-03, 1.6039e-02, 1.0843e-02,\n",
      "        1.3336e-02, 3.8716e-03, 6.3695e-03, 6.8952e-03, 1.4095e-02, 1.0584e-02,\n",
      "        9.9797e-03, 1.0227e-02, 1.3956e-02, 1.6408e-02, 2.7456e-03, 9.6086e-03,\n",
      "        6.4269e-03, 1.7564e-02, 5.5507e-03, 3.0105e-02, 5.1897e-03, 1.5252e-02,\n",
      "        5.6822e-03, 6.2098e-03, 3.1813e-03, 2.3062e-03, 4.9842e-03, 6.7927e-03,\n",
      "        1.5673e-02, 2.9857e-03, 4.4144e-03, 2.3396e-02, 5.5402e-03, 4.9910e-03,\n",
      "        6.9149e-03, 4.0570e-03, 1.6975e-02, 6.3512e-03, 3.2523e-03, 6.8370e-03,\n",
      "        1.4800e-02, 3.7011e-03, 3.6908e-03, 1.4065e-02, 7.3688e-03, 1.2873e-02,\n",
      "        1.9365e-02, 1.7191e-02, 2.0972e-03, 6.3079e-03, 5.8159e-03, 4.8221e-03,\n",
      "        7.1867e-03, 1.1915e-02, 2.4300e-03, 2.0783e-03, 3.7457e-03, 2.6347e-03,\n",
      "        7.3277e-03, 2.5612e-03, 5.7558e-03, 3.6758e-03, 1.7312e-03, 9.2246e-04,\n",
      "        2.3875e-03, 1.4007e-03, 2.7192e-03, 1.2852e-03, 8.3785e-03, 5.2909e-04,\n",
      "        5.5605e-03, 7.3555e-03, 1.3491e-03, 3.8750e-03, 2.6252e-03, 1.8496e-03,\n",
      "        4.2159e-03, 5.1177e-03, 1.2343e-03, 3.0382e-03, 3.4385e-03, 5.6925e-03,\n",
      "        5.0636e-03, 9.4940e-03, 3.2871e-03, 8.8276e-03, 2.9431e-03, 1.6869e-03,\n",
      "        2.7099e-03, 8.8221e-04, 6.4246e-04, 4.0307e-03, 5.6613e-03, 1.8148e-03,\n",
      "        5.1199e-03, 1.4299e-02, 3.6421e-03, 5.9582e-03, 6.7058e-03, 1.3508e-03,\n",
      "        3.2907e-03, 3.3720e-03, 8.9497e-04, 7.9620e-04, 6.7515e-03, 8.1681e-04,\n",
      "        1.2085e-03, 5.2232e-03, 8.7221e-04, 2.4168e-03, 5.8497e-03, 5.4009e-03,\n",
      "        1.8797e-03, 7.9673e-04, 4.2601e-03, 4.9687e-03, 2.6556e-03, 1.3153e-03,\n",
      "        1.0688e-03, 2.4299e-03, 4.1566e-03, 3.1232e-03, 4.4597e-03, 3.8210e-03,\n",
      "        2.5569e-03, 5.2780e-03, 2.3142e-03, 2.8438e-03, 3.0959e-03, 7.4215e-04,\n",
      "        2.5110e-03, 2.4901e-03, 9.5578e-03, 1.8656e-03, 7.1380e-03, 1.9550e-03,\n",
      "        1.8816e-03, 4.0296e-03, 7.4357e-04, 6.6651e-04, 2.1266e-03, 4.6524e-03,\n",
      "        3.0035e-03, 3.4478e-03, 4.4608e-03, 6.0491e-03, 7.2665e-03, 6.6933e-03,\n",
      "        7.3114e-03, 2.7993e-03, 4.9768e-03, 3.5608e-03, 4.8213e-03, 2.7142e-03,\n",
      "        2.4209e-03, 7.1002e-03, 3.0465e-03, 3.0604e-03, 5.6996e-03, 1.2176e-02,\n",
      "        5.4189e-03, 4.4560e-03, 8.4608e-03, 4.3207e-03, 5.0211e-03, 6.6903e-03,\n",
      "        2.4810e-03, 1.4340e-03, 6.1812e-03, 2.4583e-03, 3.2313e-03, 2.4033e-03,\n",
      "        7.7104e-04, 3.4169e-03, 3.2838e-03, 5.2188e-03, 3.7539e-03, 5.6047e-04,\n",
      "        4.8511e-03, 6.4365e-03, 6.3374e-03, 1.9342e-03, 1.5481e-03, 2.8203e-03,\n",
      "        5.5710e-03, 6.2219e-03, 8.2977e-03, 5.2153e-03, 2.9671e-03, 7.2391e-03,\n",
      "        2.2696e-03, 2.2061e-03, 5.4994e-03, 1.2958e-03, 2.4235e-03, 2.9642e-03,\n",
      "        1.1373e-02, 1.3739e-03, 5.9424e-03, 2.7245e-03, 2.3055e-03, 5.8545e-03,\n",
      "        1.0969e-03, 8.7203e-04, 2.9257e-03, 7.0484e-03, 2.9575e-03, 7.6328e-03,\n",
      "        4.5860e-03, 9.7535e-03, 6.2944e-03, 1.0958e-02, 4.3327e-03, 4.0566e-03,\n",
      "        6.7807e-03, 4.5071e-03, 3.2543e-03, 2.4318e-03, 1.7610e-03, 6.7316e-03,\n",
      "        3.7431e-03, 1.9881e-03, 5.4903e-03, 1.4865e-02, 5.9076e-03, 4.6115e-03,\n",
      "        6.6176e-03, 3.9014e-03, 6.1921e-03, 5.0872e-03, 1.8274e-03, 1.1555e-03,\n",
      "        7.7813e-03, 1.8906e-03, 2.5981e-03, 3.8657e-03, 6.3816e-04, 4.8723e-03,\n",
      "        5.0973e-03, 7.9868e-03, 2.1324e-03, 1.1475e-03, 5.2079e-03, 6.3819e-03,\n",
      "        5.2694e-03, 2.5661e-03, 1.9793e-03, 2.6171e-03, 4.2675e-03, 7.9288e-03,\n",
      "        6.8702e-03, 8.1183e-03, 3.4647e-03, 9.3883e-03, 2.9710e-03, 2.4529e-03,\n",
      "        7.6070e-03, 2.7293e-03, 4.5438e-03, 3.9357e-03, 1.3383e-02, 1.6125e-03,\n",
      "        5.7768e-03, 2.5164e-03, 3.9778e-03, 8.0037e-03, 2.0039e-03, 1.4874e-03,\n",
      "        4.3144e-03, 5.0313e-03, 5.7996e-03, 7.8196e-03, 3.2955e-03, 6.6826e-03,\n",
      "        9.9657e-03, 1.0802e-02, 7.6765e-03, 4.7455e-03, 5.9510e-03, 8.6933e-03,\n",
      "        5.5382e-03, 4.7048e-03, 2.0955e-03, 7.7332e-03, 6.3495e-03, 3.2669e-03,\n",
      "        7.6440e-03, 1.5295e-02, 6.8746e-03, 3.9226e-03, 7.9758e-03, 8.1445e-03,\n",
      "        8.1710e-03, 6.3416e-03, 2.2668e-03, 1.9899e-03, 5.1317e-03, 2.1425e-03,\n",
      "        5.1835e-03, 3.1039e-03, 1.0436e-03, 4.0600e-03, 7.0716e-03, 7.3279e-03,\n",
      "        3.5296e-03, 2.2691e-03, 7.8532e-03, 8.5409e-03, 7.3952e-03, 3.7805e-03,\n",
      "        4.1059e-04, 6.5910e-04, 5.5622e-04, 2.9497e-03, 1.0145e-03, 2.0535e-03,\n",
      "        7.2308e-04, 4.1877e-03, 5.1741e-04, 3.8328e-04, 2.8958e-03, 4.7670e-04,\n",
      "        1.1359e-03, 1.0548e-03, 3.1406e-03, 2.3908e-04, 8.4033e-04, 3.4348e-04,\n",
      "        9.5442e-04, 3.3425e-03, 4.8845e-04, 2.7500e-04, 8.6309e-04, 7.6943e-04,\n",
      "        1.7553e-03, 3.4899e-03, 3.2176e-04, 9.9031e-04, 3.5441e-03, 1.9158e-03,\n",
      "        2.4438e-03, 1.0731e-03, 1.9260e-03, 3.1525e-03, 1.9297e-03, 2.4072e-03,\n",
      "        3.2721e-04, 2.0275e-03, 9.7788e-04, 3.2068e-04, 3.5237e-03, 4.1272e-03,\n",
      "        3.4558e-03, 6.7252e-04, 1.7740e-03, 3.9894e-03, 2.3019e-03, 1.1639e-03,\n",
      "        1.0275e-03, 3.0548e-04, 6.9888e-04, 3.0326e-04, 1.2373e-03, 5.1426e-04,\n",
      "        1.3544e-04, 1.1420e-03, 1.1256e-03, 1.2293e-03, 1.0796e-03, 4.5075e-04,\n",
      "        3.1236e-03, 3.3159e-03, 1.5315e-03, 9.6412e-04, 1.5413e-02, 4.3920e-03,\n",
      "        1.6629e-02, 1.0212e-02, 2.6512e-02, 1.2546e-02, 6.5352e-03, 4.7366e-03,\n",
      "        1.0674e-02, 6.1200e-03, 1.1862e-02, 1.2997e-02, 1.7616e-02, 1.0842e-02,\n",
      "        9.9885e-03, 4.6487e-03, 3.6741e-03, 7.6114e-03, 1.5648e-02, 8.6267e-03,\n",
      "        1.4095e-02, 1.3313e-02, 1.7365e-02, 1.6580e-02, 2.8664e-03, 7.7404e-03,\n",
      "        5.4741e-03, 1.7890e-02, 4.2207e-03, 2.9082e-02, 5.0402e-03, 1.7368e-02,\n",
      "        3.8057e-03, 5.6706e-03, 2.9064e-03, 2.0483e-03, 6.1925e-03, 4.2757e-03,\n",
      "        1.4672e-02, 2.9924e-03, 2.7777e-03, 1.7774e-02, 3.3633e-03, 2.8468e-03,\n",
      "        4.3123e-03, 4.1520e-03, 1.7183e-02, 3.7839e-03, 3.0540e-03, 7.8201e-03,\n",
      "        1.3012e-02, 4.4686e-03, 4.6581e-03, 1.5810e-02, 7.6220e-03, 1.3486e-02,\n",
      "        1.7638e-02, 1.5043e-02, 1.3565e-03, 8.7674e-03, 4.3863e-03, 3.2535e-03,\n",
      "        5.5586e-03, 1.2617e-02, 2.5651e-03, 3.1892e-03, 5.2244e-03, 2.2093e-03,\n",
      "        7.3933e-03, 2.5524e-03, 2.0891e-03, 2.6726e-03, 3.9463e-03, 5.7111e-03,\n",
      "        2.7541e-03, 1.6245e-03, 2.6863e-03, 3.7818e-03, 6.1980e-03, 2.7700e-03,\n",
      "        4.3704e-03, 3.0456e-03, 3.3433e-03, 3.6787e-03, 1.9497e-03, 1.6129e-03,\n",
      "        2.9488e-03, 5.4691e-03, 1.1519e-03, 2.0233e-03, 2.9972e-03, 6.9927e-03,\n",
      "        4.0113e-03, 7.8889e-03, 1.9672e-03, 3.9179e-03, 3.3023e-03, 1.5346e-03,\n",
      "        2.5381e-03, 9.8079e-04, 4.4743e-03, 3.9518e-03, 3.7951e-03, 3.1820e-03,\n",
      "        2.1563e-03, 9.4402e-03, 3.7525e-03, 2.4125e-03, 4.3381e-03, 1.5984e-03,\n",
      "        4.0165e-03, 3.8356e-03, 3.0935e-03, 2.9905e-03, 5.1147e-03, 4.6105e-03,\n",
      "        6.9640e-04, 3.6584e-03, 2.6163e-03, 5.1863e-03, 4.8619e-03, 5.7907e-03,\n",
      "        1.0585e-03, 8.9005e-04, 2.5337e-03, 2.2655e-03, 3.5101e-03, 3.1582e-03,\n",
      "        5.3276e-03, 4.4250e-03, 8.9429e-03, 9.2020e-03, 1.2098e-02, 8.2836e-03,\n",
      "        4.2014e-03, 5.1503e-03, 5.2771e-03, 3.4420e-03, 8.9484e-03, 4.6020e-03,\n",
      "        6.9740e-03, 6.4895e-03, 9.5235e-03, 2.3756e-03, 4.3758e-03, 3.6951e-03,\n",
      "        6.3053e-03, 9.3683e-03, 4.1205e-03, 4.2791e-03, 6.1478e-03, 8.9094e-03,\n",
      "        3.0565e-03, 8.5186e-03, 5.1129e-03, 1.0684e-02, 5.3957e-03, 1.3660e-02,\n",
      "        4.8456e-03, 6.5733e-03, 5.8674e-03, 6.4832e-03, 3.4884e-03, 3.7368e-03,\n",
      "        3.2639e-03, 6.0068e-03, 8.4829e-03, 2.1253e-03, 4.2258e-03, 1.5627e-02,\n",
      "        5.8732e-03, 2.5582e-03, 5.6512e-03, 5.4688e-03, 9.6049e-03, 4.5972e-03,\n",
      "        3.0962e-03, 3.7408e-03, 8.4761e-03, 2.6427e-03, 4.3811e-03, 6.9221e-03,\n",
      "        3.3506e-03, 6.5220e-03, 1.1419e-02, 9.4030e-03, 2.0570e-03, 2.7603e-03,\n",
      "        4.3777e-03, 4.3802e-03, 5.4688e-03, 6.6145e-03, 3.6527e-03, 2.3572e-03,\n",
      "        4.5503e-03, 5.5110e-03, 5.7184e-03, 7.2312e-03, 1.4276e-03, 3.8702e-03,\n",
      "        3.6805e-03, 3.2596e-03, 6.1158e-03, 3.7188e-03, 6.3552e-03, 6.4070e-03,\n",
      "        6.7701e-03, 2.0292e-03, 3.4560e-03, 1.6504e-03, 6.0916e-03, 5.8142e-03,\n",
      "        3.4824e-03, 3.1388e-03, 5.3208e-03, 3.7284e-03, 1.8370e-03, 4.9658e-03,\n",
      "        2.3760e-03, 6.0377e-03, 3.8552e-03, 7.2993e-03, 2.9793e-03, 4.0052e-03,\n",
      "        3.2542e-03, 4.2771e-03, 2.0267e-03, 1.5284e-03, 2.8302e-03, 4.2717e-03,\n",
      "        5.5789e-03, 1.8800e-03, 2.1441e-03, 8.3014e-03, 3.7659e-03, 1.9502e-03,\n",
      "        3.5730e-03, 2.7947e-03, 7.4388e-03, 3.5769e-03, 2.2576e-03, 3.0497e-03,\n",
      "        3.2045e-03, 2.5461e-03, 2.2712e-03, 2.8108e-03, 2.6747e-03, 4.4334e-03,\n",
      "        5.8997e-03, 4.9104e-03, 8.4896e-04, 2.5252e-03, 3.9125e-03, 2.2088e-03,\n",
      "        3.5763e-03, 5.2365e-03, 2.1273e-03, 2.1266e-03, 5.0546e-03, 2.8798e-03,\n",
      "        7.8461e-03, 3.4236e-03, 4.4213e-03, 4.2160e-03, 2.0883e-03, 2.7974e-03,\n",
      "        3.0970e-03, 1.5971e-03, 2.8073e-03, 2.8147e-03, 9.0394e-03, 1.6721e-03,\n",
      "        6.7335e-03, 3.3342e-03, 2.7941e-03, 3.6636e-03, 1.5207e-03, 1.5822e-03,\n",
      "        2.9513e-03, 6.4732e-03, 1.9106e-03, 3.0223e-03, 4.8775e-03, 8.0617e-03,\n",
      "        5.9121e-03, 1.0591e-02, 4.6885e-03, 5.1761e-03, 3.5358e-03, 2.3023e-03,\n",
      "        3.6364e-03, 1.6285e-03, 2.1895e-03, 4.9399e-03, 4.0771e-03, 2.4722e-03,\n",
      "        5.1831e-03, 1.4800e-02, 5.6678e-03, 5.5038e-03, 7.1678e-03, 2.1961e-03,\n",
      "        5.0428e-03, 3.9499e-03, 1.8806e-03, 1.8260e-03, 8.1194e-03, 2.3501e-03,\n",
      "        1.3259e-03, 4.4086e-03, 1.5316e-03, 3.6939e-03, 5.5987e-03, 6.9968e-03,\n",
      "        2.1667e-03, 1.1737e-03, 3.8882e-03, 5.2362e-03, 4.3240e-03, 2.6148e-03]) tensor(0.0301) tensor(4.5314)\n",
      "Chosen edges: tensor([[ 3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  8, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12],\n",
      "        [29, 41,  2,  4, 12, 23, 27, 29, 31, 38, 41, 46, 56, 57, 41,  0,  2,  4,\n",
      "         12, 18, 22, 23, 27, 29, 31, 41, 46, 53, 56, 41]]) 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Train Loss: 1.9317 | Val Loss: 24.2492 | Val Accuracy: 0.3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:10<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Train Loss: 1.9313 | Val Loss: 24.2479 | Val Accuracy: 0.3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Train Loss: 1.9291 | Val Loss: 24.2483 | Val Accuracy: 0.3582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Train Loss: 1.9304 | Val Loss: 24.2439 | Val Accuracy: 0.3573\n",
      "Edge metrics: tensor([9.5024e-04, 2.0261e-03, 2.3838e-03, 2.3021e-03, 1.6870e-03, 1.6707e-03,\n",
      "        6.9679e-04, 1.8185e-03, 2.6903e-03, 5.2944e-03, 2.9547e-03, 7.7090e-04,\n",
      "        9.7799e-04, 4.8191e-03, 3.5742e-03, 3.9507e-03, 8.5441e-04, 3.1976e-05,\n",
      "        4.4466e-03, 1.5421e-03, 1.1790e-03, 1.1671e-03, 4.7604e-04, 1.4692e-03,\n",
      "        2.2368e-03, 2.1676e-03, 6.4451e-04, 2.2897e-03, 3.1703e-03, 3.3982e-03,\n",
      "        9.8621e-04, 2.9608e-04, 2.1723e-03, 3.4700e-03, 2.9540e-03, 3.0071e-03,\n",
      "        5.3828e-03, 2.2066e-03, 4.0348e-04, 2.6329e-03, 4.9862e-04, 3.2726e-03,\n",
      "        3.2156e-03, 3.6782e-04, 6.6152e-04, 4.3268e-03, 2.3513e-03, 2.1492e-03,\n",
      "        2.9119e-03, 4.0424e-03, 1.0324e-03, 4.4754e-03, 1.7137e-03, 1.2673e-03,\n",
      "        3.1124e-03, 3.5314e-03, 8.3551e-04, 3.0941e-03, 3.3426e-04, 8.8131e-04,\n",
      "        1.7795e-03, 6.1678e-04, 2.1880e-03, 3.7737e-03, 5.1886e-03, 3.7915e-03,\n",
      "        6.0442e-03, 1.0544e-02, 5.7780e-03, 1.1874e-02, 3.2323e-03, 9.0705e-03,\n",
      "        5.9556e-03, 4.9525e-03, 9.4377e-03, 5.2756e-03, 8.4011e-03, 5.8445e-03,\n",
      "        1.1662e-02, 3.6126e-03, 4.3195e-03, 2.6303e-03, 6.9462e-03, 1.1833e-02,\n",
      "        5.3884e-03, 4.4490e-03, 8.5563e-03, 4.9949e-03, 5.3632e-03, 1.0556e-02,\n",
      "        2.9265e-03, 6.2575e-03, 7.7702e-03, 1.1668e-02, 6.5706e-03, 6.1343e-03,\n",
      "        6.3319e-03, 9.7798e-03, 4.4176e-03, 5.4635e-03, 4.5029e-03, 5.4242e-03,\n",
      "        8.0919e-03, 2.7495e-03, 5.6714e-03, 1.2026e-02, 4.8368e-03, 2.9500e-03,\n",
      "        6.1568e-03, 9.3444e-03, 1.3206e-02, 5.1498e-03, 3.8410e-03, 4.8754e-03,\n",
      "        5.4161e-03, 3.5866e-03, 6.4878e-03, 5.2533e-03, 4.5463e-03, 4.9089e-03,\n",
      "        7.6386e-03, 7.2577e-03, 2.0481e-03, 3.8950e-03, 7.4009e-03, 6.2347e-03,\n",
      "        5.6747e-03, 5.1941e-03, 7.9380e-05, 1.5153e-04, 2.5371e-04, 1.3020e-04,\n",
      "        3.0591e-04, 2.4704e-04, 6.5579e-05, 2.3308e-04, 2.9119e-04, 4.5635e-04,\n",
      "        1.6391e-04, 6.3946e-05, 1.5338e-04, 3.9623e-04, 5.3065e-04, 3.1977e-04,\n",
      "        2.1596e-04, 3.4755e-05, 3.5985e-04, 1.6562e-04, 6.7232e-05, 5.3234e-05,\n",
      "        1.3233e-04, 2.4776e-04, 2.7377e-04, 1.2778e-04, 1.3226e-04, 3.8409e-04,\n",
      "        5.3062e-04, 5.4846e-04, 3.1770e-04, 1.0884e-04, 2.7595e-04, 3.0325e-04,\n",
      "        4.4601e-04, 3.6046e-04, 3.9852e-04, 3.3152e-04, 1.2218e-04, 3.7985e-04,\n",
      "        1.5627e-04, 6.7546e-04, 3.7490e-04, 1.0807e-04, 2.4999e-04, 4.9128e-04,\n",
      "        3.5687e-04, 4.3316e-04, 2.6707e-04, 2.6534e-04, 2.2500e-04, 3.8350e-04,\n",
      "        2.6364e-04, 1.2198e-04, 1.0937e-04, 4.1504e-04, 1.7083e-04, 4.3046e-04,\n",
      "        1.1674e-04, 5.0978e-05, 2.1984e-04, 1.8684e-04, 4.0413e-04, 2.4142e-04,\n",
      "        2.8114e-03, 3.7540e-03, 9.7808e-03, 5.4885e-03, 1.2494e-02, 6.9936e-03,\n",
      "        6.2871e-03, 5.1531e-03, 5.2162e-03, 2.6076e-03, 5.4068e-03, 2.2539e-03,\n",
      "        4.4967e-03, 4.7090e-03, 1.2468e-02, 2.0720e-03, 9.5137e-03, 6.1777e-03,\n",
      "        3.8928e-03, 6.2810e-03, 2.3472e-03, 2.2125e-03, 4.9338e-03, 9.9519e-03,\n",
      "        2.3355e-03, 5.6957e-03, 7.5556e-03, 1.2370e-02, 6.5703e-03, 5.2356e-03,\n",
      "        7.7038e-03, 6.1363e-03, 4.8332e-03, 4.1573e-03, 1.9559e-03, 2.5145e-03,\n",
      "        7.3146e-03, 5.7425e-03, 2.6120e-03, 6.6567e-03, 4.5282e-03, 6.1345e-03,\n",
      "        1.0433e-02, 4.6080e-03, 7.3910e-03, 6.4239e-03, 2.2477e-03, 2.5295e-03,\n",
      "        1.2374e-02, 1.9419e-03, 2.8611e-03, 6.6585e-03, 2.0723e-03, 6.0136e-03,\n",
      "        7.0593e-03, 1.0856e-02, 2.2518e-03, 1.4023e-03, 4.9817e-03, 6.5094e-03,\n",
      "        6.6282e-03, 3.7121e-03, 1.1641e-02, 4.4813e-03, 1.0189e-02, 1.1881e-02,\n",
      "        7.4979e-03, 5.8024e-03, 1.0948e-02, 4.9584e-03, 1.2688e-02, 9.0094e-03,\n",
      "        1.0461e-02, 1.2790e-02, 3.8727e-03, 6.1528e-03, 6.1734e-03, 1.3432e-02,\n",
      "        1.0347e-02, 9.5562e-03, 1.0027e-02, 1.3190e-02, 2.6552e-03, 9.4838e-03,\n",
      "        6.2764e-03, 5.4077e-03, 5.1046e-03, 5.5454e-03, 6.0280e-03, 3.1793e-03,\n",
      "        2.2967e-03, 4.9845e-03, 6.6401e-03, 2.9807e-03, 4.2991e-03, 5.5042e-03,\n",
      "        4.7940e-03, 6.6501e-03, 4.0324e-03, 6.2291e-03, 3.2414e-03, 6.8417e-03,\n",
      "        1.4625e-02, 3.6776e-03, 3.5812e-03, 1.3496e-02, 7.1714e-03, 1.2416e-02,\n",
      "        2.0863e-03, 5.9399e-03, 5.6419e-03, 4.6587e-03, 6.9785e-03, 1.1397e-02,\n",
      "        2.4338e-03, 2.0529e-03, 3.7344e-03, 2.6297e-03, 7.3832e-03, 2.5634e-03,\n",
      "        5.7925e-03, 3.7168e-03, 1.7136e-03, 9.2398e-04, 2.3781e-03, 1.4044e-03,\n",
      "        2.7305e-03, 1.2849e-03, 8.4429e-03, 5.2969e-04, 5.5702e-03, 7.3856e-03,\n",
      "        1.3571e-03, 3.8969e-03, 2.6299e-03, 1.8509e-03, 4.2368e-03, 5.1523e-03,\n",
      "        1.2214e-03, 3.0281e-03, 3.4206e-03, 5.7011e-03, 5.0576e-03, 9.5887e-03,\n",
      "        3.2932e-03, 8.8719e-03, 2.8801e-03, 1.6808e-03, 2.7173e-03, 8.8485e-04,\n",
      "        6.4228e-04, 3.9963e-03, 5.6769e-03, 1.8101e-03, 5.1577e-03, 1.4416e-02,\n",
      "        3.6105e-03, 5.9821e-03, 6.7602e-03, 1.3632e-03, 3.2953e-03, 3.3026e-03,\n",
      "        8.8732e-04, 7.9561e-04, 6.8028e-03, 8.1498e-04, 1.2114e-03, 5.2601e-03,\n",
      "        8.6641e-04, 2.3974e-03, 5.8800e-03, 5.4345e-03, 1.8747e-03, 7.9905e-04,\n",
      "        4.3144e-03, 4.9834e-03, 2.6474e-03, 1.3138e-03, 1.0696e-03, 2.4365e-03,\n",
      "        4.1608e-03, 3.1130e-03, 4.4489e-03, 3.8158e-03, 2.5250e-03, 5.2581e-03,\n",
      "        2.3087e-03, 2.8442e-03, 3.0798e-03, 7.4396e-04, 2.5234e-03, 2.4940e-03,\n",
      "        9.5308e-03, 1.8663e-03, 7.1418e-03, 1.9310e-03, 1.8856e-03, 4.0063e-03,\n",
      "        7.4595e-04, 6.6808e-04, 2.1183e-03, 4.6625e-03, 3.0001e-03, 3.4336e-03,\n",
      "        4.4582e-03, 6.0080e-03, 7.2552e-03, 6.6881e-03, 7.3117e-03, 2.7818e-03,\n",
      "        4.9748e-03, 3.5538e-03, 4.8354e-03, 2.7167e-03, 2.4203e-03, 7.1008e-03,\n",
      "        3.0291e-03, 3.0416e-03, 5.6841e-03, 1.2163e-02, 5.4167e-03, 4.4520e-03,\n",
      "        8.4723e-03, 4.3313e-03, 5.0081e-03, 6.6954e-03, 2.4689e-03, 1.4330e-03,\n",
      "        6.1475e-03, 2.4543e-03, 3.2348e-03, 2.3965e-03, 7.6834e-04, 3.4149e-03,\n",
      "        3.2734e-03, 5.2484e-03, 3.7501e-03, 5.6383e-04, 4.8063e-03, 6.3914e-03,\n",
      "        6.3644e-03, 1.9310e-03, 1.5500e-03, 2.8239e-03, 5.5707e-03, 6.2340e-03,\n",
      "        8.2891e-03, 5.1801e-03, 2.9421e-03, 7.2739e-03, 2.2613e-03, 2.2057e-03,\n",
      "        5.4980e-03, 1.3015e-03, 2.4383e-03, 2.9661e-03, 1.1396e-02, 1.3748e-03,\n",
      "        5.9489e-03, 2.7010e-03, 2.3154e-03, 5.8699e-03, 1.1019e-03, 8.7702e-04,\n",
      "        2.9128e-03, 7.0568e-03, 2.9524e-03, 7.6433e-03, 4.5944e-03, 9.7468e-03,\n",
      "        6.3131e-03, 1.0925e-02, 4.3633e-03, 4.0404e-03, 6.8389e-03, 4.5118e-03,\n",
      "        3.2633e-03, 2.4088e-03, 1.7607e-03, 6.7382e-03, 3.7257e-03, 1.9670e-03,\n",
      "        5.4970e-03, 1.4913e-02, 5.9332e-03, 4.5925e-03, 6.6174e-03, 3.9039e-03,\n",
      "        6.1642e-03, 5.0460e-03, 1.8137e-03, 1.1584e-03, 7.7768e-03, 1.8840e-03,\n",
      "        2.5778e-03, 3.8628e-03, 6.3478e-04, 4.8855e-03, 5.0873e-03, 8.0224e-03,\n",
      "        2.1241e-03, 1.1543e-03, 5.1872e-03, 6.3645e-03, 5.1987e-03, 2.5609e-03,\n",
      "        1.9982e-03, 2.6291e-03, 4.2855e-03, 7.6968e-03, 6.8515e-03, 7.9848e-03,\n",
      "        3.4457e-03, 9.1916e-03, 2.9771e-03, 2.4600e-03, 7.4212e-03, 2.7384e-03,\n",
      "        4.4973e-03, 3.8608e-03, 1.3105e-02, 1.6181e-03, 5.8230e-03, 2.4990e-03,\n",
      "        3.9518e-03, 7.7962e-03, 2.0214e-03, 1.5080e-03, 4.2538e-03, 5.0918e-03,\n",
      "        5.6968e-03, 7.4904e-03, 3.3226e-03, 6.6066e-03, 9.7084e-03, 1.0765e-02,\n",
      "        7.6044e-03, 4.7127e-03, 5.8155e-03, 8.4992e-03, 5.4731e-03, 4.6311e-03,\n",
      "        2.1024e-03, 7.6139e-03, 6.2500e-03, 3.2550e-03, 7.5258e-03, 6.6917e-03,\n",
      "        3.9268e-03, 7.9061e-03, 7.9893e-03, 7.9401e-03, 6.3048e-03, 2.2369e-03,\n",
      "        1.9909e-03, 5.2078e-03, 2.1448e-03, 5.1292e-03, 3.1080e-03, 1.0497e-03,\n",
      "        4.0855e-03, 7.0361e-03, 7.2531e-03, 3.5024e-03, 2.2808e-03, 7.6073e-03,\n",
      "        8.3737e-03, 7.2980e-03, 3.6935e-03, 4.1135e-04, 6.5908e-04, 5.5638e-04,\n",
      "        2.9589e-03, 1.0143e-03, 2.0526e-03, 7.2113e-04, 4.1957e-03, 5.1599e-04,\n",
      "        3.8324e-04, 2.8952e-03, 4.7770e-04, 1.1434e-03, 1.0554e-03, 3.1451e-03,\n",
      "        2.3914e-04, 8.3839e-04, 3.3957e-04, 9.5588e-04, 3.3588e-03, 4.8913e-04,\n",
      "        2.7609e-04, 8.6227e-04, 7.7181e-04, 1.7582e-03, 3.4987e-03, 3.2082e-04,\n",
      "        9.8873e-04, 3.5512e-03, 1.9162e-03, 2.4470e-03, 1.0701e-03, 1.9291e-03,\n",
      "        3.1555e-03, 1.9312e-03, 2.4060e-03, 3.2692e-04, 2.0282e-03, 9.7509e-04,\n",
      "        3.1782e-04, 3.5309e-03, 4.1478e-03, 3.4574e-03, 6.7084e-04, 1.7779e-03,\n",
      "        4.0038e-03, 2.3039e-03, 1.1650e-03, 1.0230e-03, 3.0460e-04, 6.9806e-04,\n",
      "        3.0307e-04, 1.2379e-03, 5.1416e-04, 1.3347e-04, 1.1411e-03, 1.1274e-03,\n",
      "        1.2354e-03, 1.0800e-03, 4.5285e-04, 3.1277e-03, 3.3085e-03, 1.5308e-03,\n",
      "        9.6186e-04, 4.4511e-03, 1.0090e-02, 1.1968e-02, 6.4167e-03, 4.7533e-03,\n",
      "        1.0441e-02, 6.1186e-03, 1.1576e-02, 1.2582e-02, 1.0633e-02, 9.9494e-03,\n",
      "        4.6514e-03, 3.6575e-03, 7.2724e-03, 8.5157e-03, 1.3636e-02, 1.2931e-02,\n",
      "        2.8623e-03, 7.8034e-03, 5.6433e-03, 4.2817e-03, 4.9812e-03, 3.8473e-03,\n",
      "        5.6823e-03, 2.9031e-03, 2.0416e-03, 6.1929e-03, 4.2523e-03, 1.3890e-02,\n",
      "        2.9665e-03, 2.7827e-03, 3.4080e-03, 2.8188e-03, 4.2877e-03, 4.1744e-03,\n",
      "        3.7714e-03, 3.0612e-03, 7.7572e-03, 1.2849e-02, 4.4471e-03, 4.6606e-03,\n",
      "        7.4516e-03, 1.3164e-02, 1.4540e-02, 1.3591e-03, 8.5142e-03, 4.3621e-03,\n",
      "        3.2414e-03, 5.5475e-03, 1.2256e-02, 2.5695e-03, 3.1990e-03, 5.2356e-03,\n",
      "        2.2099e-03, 7.3990e-03, 2.5514e-03, 2.0827e-03, 2.6676e-03, 3.9439e-03,\n",
      "        5.7121e-03, 2.7430e-03, 1.6309e-03, 2.7049e-03, 3.7826e-03, 6.1986e-03,\n",
      "        2.7679e-03, 4.3556e-03, 3.0344e-03, 3.3546e-03, 3.6885e-03, 1.9560e-03,\n",
      "        1.6200e-03, 2.9485e-03, 5.4860e-03, 1.1526e-03, 2.0219e-03, 3.0009e-03,\n",
      "        6.9976e-03, 4.0170e-03, 7.8961e-03, 1.9626e-03, 3.9102e-03, 3.2900e-03,\n",
      "        1.5329e-03, 2.5347e-03, 9.8050e-04, 4.4714e-03, 3.9465e-03, 3.7870e-03,\n",
      "        3.1726e-03, 2.1472e-03, 9.4436e-03, 3.7462e-03, 2.3983e-03, 4.3206e-03,\n",
      "        1.6077e-03, 4.0108e-03, 3.8318e-03, 3.0941e-03, 2.9901e-03, 5.1176e-03,\n",
      "        4.6021e-03, 6.9830e-04, 3.6649e-03, 2.6117e-03, 5.1996e-03, 4.8654e-03,\n",
      "        5.8059e-03, 1.0507e-03, 8.9655e-04, 2.5227e-03, 2.2458e-03, 3.5178e-03,\n",
      "        3.1560e-03, 5.3320e-03, 4.3942e-03, 8.9984e-03, 9.1972e-03, 1.2097e-02,\n",
      "        8.2924e-03, 4.2059e-03, 5.1213e-03, 5.2901e-03, 3.4451e-03, 8.9069e-03,\n",
      "        4.6145e-03, 7.0084e-03, 6.5139e-03, 9.3690e-03, 2.3850e-03, 4.3141e-03,\n",
      "        3.6435e-03, 6.3502e-03, 9.3274e-03, 4.1348e-03, 4.2919e-03, 6.1259e-03,\n",
      "        8.9896e-03, 3.0502e-03, 8.5016e-03, 5.2055e-03, 1.0574e-02, 5.4288e-03,\n",
      "        1.3645e-02, 4.7789e-03, 6.5775e-03, 5.6940e-03, 6.5277e-03, 3.4587e-03,\n",
      "        3.7356e-03, 3.2738e-03, 5.9585e-03, 8.4396e-03, 2.0945e-03, 4.1641e-03,\n",
      "        5.7561e-03, 2.5083e-03, 5.4758e-03, 5.5270e-03, 9.5869e-03, 4.6131e-03,\n",
      "        3.0465e-03, 3.7608e-03, 8.6044e-03, 2.6378e-03, 4.4151e-03, 6.9405e-03,\n",
      "        3.3853e-03, 6.4872e-03, 1.1394e-02, 9.2775e-03, 2.0438e-03, 2.7825e-03,\n",
      "        4.3149e-03, 4.3384e-03, 5.5478e-03, 6.6297e-03, 3.6701e-03, 2.3604e-03,\n",
      "        4.5576e-03, 5.5127e-03, 5.6979e-03, 7.2312e-03, 1.4145e-03, 3.8704e-03,\n",
      "        3.6653e-03, 3.2560e-03, 6.0928e-03, 3.7291e-03, 6.3658e-03, 6.4143e-03,\n",
      "        6.7732e-03, 2.0305e-03, 3.4546e-03, 1.6504e-03, 6.1113e-03, 5.8244e-03,\n",
      "        3.5022e-03, 3.1484e-03, 5.3254e-03, 3.7305e-03, 1.8361e-03, 4.9556e-03,\n",
      "        2.3764e-03, 6.0255e-03, 3.8541e-03, 7.3184e-03, 2.9737e-03, 4.0081e-03,\n",
      "        3.2452e-03, 4.2737e-03, 2.0257e-03, 1.5275e-03, 2.8282e-03, 4.2641e-03,\n",
      "        5.5719e-03, 1.8717e-03, 2.1414e-03, 8.3189e-03, 3.7579e-03, 1.9449e-03,\n",
      "        3.5748e-03, 2.8097e-03, 7.4529e-03, 3.5738e-03, 2.2524e-03, 3.0419e-03,\n",
      "        3.1974e-03, 2.5406e-03, 2.2800e-03, 2.8084e-03, 2.6559e-03, 4.4402e-03,\n",
      "        5.9107e-03, 4.9300e-03, 8.4539e-04, 2.5544e-03, 3.9057e-03, 2.1894e-03,\n",
      "        3.5803e-03, 5.2219e-03, 2.1301e-03, 2.1577e-03, 5.0834e-03, 2.8813e-03,\n",
      "        7.8643e-03, 3.4250e-03, 4.4158e-03, 4.2303e-03, 2.0916e-03, 2.7983e-03,\n",
      "        3.0901e-03, 1.5986e-03, 2.8234e-03, 2.8201e-03, 9.0745e-03, 1.6740e-03,\n",
      "        6.7366e-03, 3.3097e-03, 2.8036e-03, 3.6680e-03, 1.5244e-03, 1.5843e-03,\n",
      "        2.9519e-03, 6.5246e-03, 1.9293e-03, 3.0191e-03, 4.9052e-03, 8.1054e-03,\n",
      "        5.9994e-03, 1.0619e-02, 4.7089e-03, 5.1595e-03, 3.5899e-03, 2.3021e-03,\n",
      "        3.6652e-03, 1.6238e-03, 2.1886e-03, 4.9689e-03, 4.0608e-03, 2.4794e-03,\n",
      "        5.2253e-03, 1.4890e-02, 5.7279e-03, 5.4999e-03, 7.1775e-03, 2.2059e-03,\n",
      "        5.0619e-03, 4.0032e-03, 1.8838e-03, 1.8256e-03, 8.1576e-03, 2.3456e-03,\n",
      "        1.3286e-03, 4.4157e-03, 1.5258e-03, 3.7064e-03, 5.5943e-03, 7.0523e-03,\n",
      "        2.1723e-03, 1.1786e-03, 3.8793e-03, 5.2648e-03, 4.3729e-03, 2.6122e-03,\n",
      "        1.1036e-03, 8.3444e-04, 1.2696e-03, 2.2929e-03, 1.1099e-03, 1.7600e-03,\n",
      "        1.8476e-03, 2.1391e-03, 1.1761e-03, 7.8257e-04, 4.3135e-04, 8.2427e-04,\n",
      "        1.6114e-03, 1.1512e-03, 4.1905e-04, 1.8917e-03, 1.2511e-03, 2.3914e-03,\n",
      "        1.8243e-03, 1.8231e-03, 1.7385e-03, 1.6376e-03, 1.6610e-03, 2.3481e-03,\n",
      "        1.5279e-03, 2.5244e-06, 1.3159e-03, 2.1429e-03, 1.3527e-03, 1.3796e-04]) tensor(0.0149) tensor(3.9948)\n",
      "Chosen edges: tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,\n",
      "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,\n",
      "          7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          8,  8,  8,  8,  8,  8,  8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 13, 14, 14, 14, 14, 14, 14],\n",
      "        [ 3,  5,  7, 10, 12, 14, 19, 22, 25, 28, 29, 33, 38, 41, 45, 46, 56,  2,\n",
      "          4, 14, 16, 23, 26, 27, 31, 44, 50, 57,  0,  3,  5,  6,  8, 10, 11, 13,\n",
      "         14, 18, 19, 20, 21, 22, 25, 50, 53, 55, 63, 14, 29, 31, 41, 14, 41, 44,\n",
      "          4, 14, 25, 27, 29, 41, 50, 57,  3,  5,  7, 14, 19, 25, 28, 29, 30, 33,\n",
      "         37, 40, 44, 45, 46, 60, 61,  3,  5,  8, 10, 11, 13, 14, 19, 20, 21, 25,\n",
      "         38, 49, 50, 55, 57, 59, 63, 29, 41,  2,  3,  4,  5, 10, 14, 19, 23, 25,\n",
      "         27, 29, 38, 46, 50, 56, 57, 41,  4, 14, 27, 29, 41, 50]]) 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/51 [00:02<00:10,  4.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m      4\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m      5\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself-expanding-nets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic-mul, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m],\n\u001b[0;32m      8\u001b[0m     group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew freeze 2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_sparse_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[43medge_replacement_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_replacement_func_new_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mtrain_sparse_recursive\u001b[1;34m(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func, window_size, threshold, lr, choose_threshold, aggregation_mode)\u001b[0m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_repl:\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mfreeze_all_but_last\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_choose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\model\\utils.py:67\u001b[0m, in \u001b[0;36mfreeze_all_but_last\u001b[1;34m(model, len_choose)\u001b[0m\n\u001b[0;32m     64\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_layer_params, ExpandingLinear):\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mlast_layer_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlen_choose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\model\\model.py:117\u001b[0m, in \u001b[0;36mExpandingLinear.freeze_embeds\u001b[1;34m(self, len_choose)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_linears[i]\u001b[38;5;241m.\u001b[39mweight_values\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_values) \u001b[38;5;241m-\u001b[39m len_choose):\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_values\u001b[38;5;241m.\u001b[39mgrad[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dense_model = MulticlassFCN(input_size=X.shape[1])\n",
    "sparse_model = convert_dense_to_sparse_network(dense_model)\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project=\"self-expanding-nets\",\n",
    "    name=f\"titanic-mul, {name}\",\n",
    "    tags=[\"complex model\", \"titanic\", \"multiclass\", hyperparams[\"metric\"].__class__.__name__],\n",
    "    group=\"new freeze 2\"\n",
    ")\n",
    "\n",
    "train_sparse_recursive(sparse_model, train_loader, val_loader,\n",
    "                       edge_replacement_func=edge_replacement_func_new_layer, **hyperparams)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bf1b57447aa62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.697345400Z",
     "start_time": "2025-02-11T11:19:18.201889Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
