{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.318020Z",
     "start_time": "2025-02-11T21:01:25.170016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851a61bbbac63426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.409711Z",
     "start_time": "2025-02-11T21:02:20.359673Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import convert_dense_to_sparse_network, get_model_last_layer\n",
    "from senmodel.metrics.edge_finder import EdgeFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed98f9250fb9b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.443325Z",
     "start_time": "2025-02-11T21:02:20.428808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1c4b0b6e33d5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:23.590919Z",
     "start_time": "2025-02-11T21:02:20.480333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "data = pd.read_csv(url, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = data['occupation']\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X = data.drop(['occupation'], axis=1)\n",
    "\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d9e408b16a7a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:24.903666Z",
     "start_time": "2025-02-11T21:02:24.884257Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183cc85a9b097f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.139816Z",
     "start_time": "2025-02-11T21:02:25.010546Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7e5ab74bd84c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.182863Z",
     "start_time": "2025-02-11T21:02:25.161851Z"
    }
   },
   "outputs": [],
   "source": [
    "class   MulticlassFCN(nn.Module):\n",
    "    def __init__(self, input_size=14, hidden_sizes=None, output_size=15, dropout_rate=0.3):\n",
    "        super(MulticlassFCN, self).__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [128, 64]\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.output = nn.Linear(hidden_sizes[1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        # x = self.dropout2(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe92b9bfdd4d7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.288977Z",
     "start_time": "2025-02-11T21:02:25.260801Z"
    }
   },
   "outputs": [],
   "source": [
    "def edge_replacement_func_new_layer(model, optim, val_loader, metric, choose_threshold, aggregation_mode='mean', len_choose=None):\n",
    "    layer = get_model_last_layer(model)\n",
    "    ef = EdgeFinder(metric, val_loader, device, aggregation_mode)\n",
    "    vals = ef.calculate_edge_metric_for_dataloader(model, len_choose, False)\n",
    "    print(\"Edge metrics:\", vals, max(vals, default=0), sum(vals))\n",
    "    chosen_edges = ef.choose_edges_threshold(model, choose_threshold, len_choose)\n",
    "    print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
    "    layer.replace_many(*chosen_edges)\n",
    "\n",
    "    if len(chosen_edges[0]) > 0:\n",
    "        optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
    "    else:\n",
    "        print(\"Empty metric\")\n",
    "\n",
    "    return {'max': max(vals, default=0), 'sum': sum(vals), 'len': len(vals), 'len_choose': layer.count_replaces[-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e4d086a0b3b29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.591134Z",
     "start_time": "2025-02-11T21:02:25.433629Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import freeze_all_but_last, freeze_only_last\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_sparse_recursive(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func=None,\n",
    "                           window_size=3, threshold=0.1, lr=5e-4, choose_threshold=0.3, aggregation_mode='mean', replace_all_epochs=3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_losses = []\n",
    "\n",
    "    len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if len(len_choose) > replace_all_epochs and i > window_size:\n",
    "                freeze_all_but_last(model)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(all_targets, all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        new_l = dict()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        if edge_replacement_func and len(val_losses) > window_size:\n",
    "            recent_changes = [abs(val_losses[i] - val_losses[i - 1]) for i in range(-window_size, 0)]\n",
    "            avg_change = sum(recent_changes) / window_size\n",
    "            if avg_change < threshold:\n",
    "                print(f\"{len_choose=}\")\n",
    "                len_ch = len_choose[-1] if len(len_choose) > replace_all_epochs else None\n",
    "                new_l = edge_replacement_func(model, optimizer, val_loader, metric, choose_threshold, aggregation_mode, len_ch)\n",
    "                # Замораживаем все слои кроме последнего\n",
    "                val_losses = []\n",
    "                len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "        wandb.log({'val_loss': val_loss, 'val_accuracy': val_accuracy, 'train_loss': train_loss} | new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44615f5a401dd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.628149Z",
     "start_time": "2025-02-11T21:02:25.613167Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.metrics.nonlinearity_metrics import *\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [\n",
    "    AbsGradientEdgeMetric(criterion),\n",
    "    ReversedAbsGradientEdgeMetric(criterion),\n",
    "    SNIPMetric(criterion),\n",
    "    MagnitudeL2Metric(criterion),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c813771bca507d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.557848Z",
     "start_time": "2025-02-11T21:02:25.697276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfedornigretuk\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10df7d34ecdcb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.805781Z",
     "start_time": "2025-02-11T21:02:39.779777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_epochs: 50, metric: MagnitudeL2Metric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"num_epochs\": 50,\n",
    "               \"metric\": metrics[3],\n",
    "               \"aggregation_mode\": \"mean\",\n",
    "               \"choose_threshold\": 0.1,\n",
    "               \"window_size\": 3,\n",
    "               \"threshold\": 0.05,\n",
    "               \"lr\": 5e-4,\n",
    "               \"replace_all_epochs\": 2\n",
    "               }\n",
    "\n",
    "name = \", \".join(\n",
    "    f\"{key}: {value.__class__.__name__ if key == 'metric' else value}\"\n",
    "    for key, value in hyperparams.items()\n",
    ")\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fe9040816bb570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.151538Z",
     "start_time": "2025-02-11T21:02:40.108023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Coding\\PY\\self-expanding-nets\\experiments\\training\\wandb\\run-20250303_173338-jyb2yngr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jyb2yngr' target=\"_blank\">titanic-mul, num_epochs: 50, metric: MagnitudeL2Metric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2</a></strong> to <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jyb2yngr' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/jyb2yngr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 101.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.5203 | Val Loss: 30.2853 | Val Accuracy: 0.2474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 97.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.2026 | Val Loss: 26.9736 | Val Accuracy: 0.3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 85.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 2.0297 | Val Loss: 25.5362 | Val Accuracy: 0.3395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 87.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 1.9677 | Val Loss: 25.0166 | Val Accuracy: 0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 77.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 1.9373 | Val Loss: 24.7714 | Val Accuracy: 0.3458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 85.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 1.9204 | Val Loss: 24.5511 | Val Accuracy: 0.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 82.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 1.9055 | Val Loss: 24.3964 | Val Accuracy: 0.3485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 100.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 1.8943 | Val Loss: 24.2655 | Val Accuracy: 0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 72.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 1.8871 | Val Loss: 24.1881 | Val Accuracy: 0.3542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 72.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 1.8789 | Val Loss: 24.0892 | Val Accuracy: 0.3556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 92.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 1.8720 | Val Loss: 24.0366 | Val Accuracy: 0.3577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 92.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 1.8728 | Val Loss: 23.9967 | Val Accuracy: 0.3564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 77.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 1.8648 | Val Loss: 23.9224 | Val Accuracy: 0.3599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 92.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 1.8577 | Val Loss: 23.8669 | Val Accuracy: 0.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 85.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 1.8540 | Val Loss: 23.8220 | Val Accuracy: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 82.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Train Loss: 1.8488 | Val Loss: 23.7969 | Val Accuracy: 0.3607\n",
      "len_choose=[960]\n",
      "Edge metrics: tensor([6.8479e-03, 6.5744e-02, 2.8886e-02, 2.9458e-02, 3.8313e-02, 4.7368e-03,\n",
      "        4.2148e-02, 1.6906e-03, 5.4783e-02, 7.3160e-02, 7.2886e-02, 9.7355e-02,\n",
      "        1.0855e-02, 8.6562e-02, 4.1004e-02, 9.8299e-02, 5.2507e-02, 2.2878e-02,\n",
      "        5.7040e-02, 1.8114e-02, 4.6554e-03, 6.5523e-02, 4.2630e-02, 2.6155e-02,\n",
      "        7.0665e-02, 2.2973e-02, 6.9456e-03, 2.1084e-03, 1.8328e-02, 3.7800e-04,\n",
      "        3.3185e-03, 2.2529e-02, 9.6627e-04, 2.6697e-02, 3.7704e-03, 1.7831e-02,\n",
      "        1.2519e-04, 6.2173e-02, 3.6183e-03, 8.9173e-04, 6.6477e-02, 2.2494e-03,\n",
      "        4.0203e-02, 1.2762e-05, 8.6924e-02, 8.9079e-03, 1.9736e-04, 1.5607e-02,\n",
      "        5.2140e-02, 4.2796e-03, 8.4221e-02, 2.2891e-02, 1.3315e-04, 1.0996e-02,\n",
      "        1.2899e-02, 2.9505e-02, 5.1663e-02, 9.5430e-04, 4.0031e-02, 5.7288e-03,\n",
      "        7.9637e-02, 2.9841e-02, 3.6015e-02, 5.5688e-02, 1.2088e-03, 2.2918e-03,\n",
      "        2.1846e-03, 5.4763e-03, 8.3276e-04, 1.8336e-02, 5.4646e-04, 4.8081e-05,\n",
      "        4.4949e-03, 9.1279e-04, 7.8548e-04, 1.2699e-02, 2.4373e-05, 1.2610e-02,\n",
      "        7.9355e-03, 6.4242e-03, 1.0543e-02, 7.1898e-02, 3.0642e-02, 1.9557e-02,\n",
      "        2.1308e-02, 7.3601e-03, 8.1095e-06, 3.0156e-03, 9.4048e-03, 1.8416e-03,\n",
      "        4.7466e-03, 4.9058e-03, 4.3038e-03, 1.8881e-02, 3.4311e-04, 3.0007e-06,\n",
      "        8.2917e-03, 1.8959e-02, 4.8607e-04, 2.2554e-02, 5.8513e-03, 5.6757e-03,\n",
      "        1.3243e-03, 2.3577e-02, 1.6688e-02, 6.9380e-04, 1.1293e-02, 1.5600e-02,\n",
      "        7.3337e-03, 2.3821e-02, 2.8984e-03, 1.2260e-02, 3.7168e-05, 2.0281e-02,\n",
      "        1.8877e-04, 4.1757e-03, 1.2465e-05, 1.7376e-02, 1.1681e-02, 3.9733e-03,\n",
      "        1.9040e-02, 6.2632e-03, 1.0744e-02, 1.0269e-05, 3.7602e-03, 2.6900e-04,\n",
      "        1.6676e-02, 8.6299e-05, 4.2728e-02, 2.3682e-02, 2.5416e-02, 2.0060e-02,\n",
      "        8.0001e-05, 9.7875e-04, 2.9337e-02, 7.5857e-05, 1.7960e-02, 3.6547e-02,\n",
      "        2.2433e-02, 6.9192e-02, 2.2987e-02, 3.9074e-02, 7.3562e-04, 7.7167e-02,\n",
      "        4.5803e-02, 6.1954e-03, 9.4344e-04, 5.8222e-04, 2.9331e-02, 1.2717e-02,\n",
      "        2.8791e-03, 8.7828e-05, 6.4819e-02, 3.0504e-03, 1.8226e-02, 1.1013e-03,\n",
      "        2.0706e-03, 7.9965e-04, 5.3744e-03, 4.7247e-04, 9.9483e-03, 3.2180e-03,\n",
      "        7.7356e-03, 2.5909e-04, 6.0113e-03, 3.7962e-03, 1.2387e-02, 2.8016e-02,\n",
      "        1.4227e-03, 3.3591e-03, 7.5317e-03, 2.3524e-02, 4.5111e-02, 1.7129e-04,\n",
      "        1.0293e-03, 1.0259e-03, 4.3440e-06, 1.4031e-02, 8.3939e-02, 1.0059e-04,\n",
      "        2.3798e-02, 3.8483e-03, 4.5778e-02, 6.2631e-02, 1.8648e-02, 2.7052e-04,\n",
      "        1.1905e-02, 2.5549e-03, 6.1196e-05, 5.7204e-03, 8.0605e-04, 3.4504e-04,\n",
      "        8.9950e-03, 1.9209e-04, 2.0037e-02, 1.4804e-03, 5.4189e-02, 7.7971e-04,\n",
      "        1.2936e-03, 7.2672e-05, 6.7242e-04, 1.0528e-02, 1.5684e-02, 6.5013e-03,\n",
      "        2.7172e-03, 7.8327e-04, 1.0228e-05, 8.6735e-03, 8.4780e-03, 4.3049e-02,\n",
      "        7.7791e-05, 3.5024e-06, 2.1401e-02, 2.2257e-02, 3.4534e-02, 8.9116e-04,\n",
      "        1.4848e-02, 7.6329e-03, 1.3585e-02, 5.6242e-03, 1.2661e-02, 2.0060e-02,\n",
      "        1.0484e-03, 2.5335e-02, 6.4735e-03, 2.5422e-03, 1.4304e-03, 5.2576e-06,\n",
      "        3.3199e-03, 4.5184e-03, 5.7242e-03, 1.0290e-02, 2.4410e-02, 1.0818e-02,\n",
      "        3.3503e-03, 8.2515e-03, 1.5257e-03, 1.1527e-02, 2.9900e-02, 1.4180e-02,\n",
      "        8.4513e-03, 1.9887e-02, 1.5598e-04, 9.0650e-03, 7.7619e-03, 7.8158e-03,\n",
      "        1.1596e-02, 2.9881e-07, 1.2074e-02, 1.5702e-02, 6.5035e-04, 2.0619e-03,\n",
      "        3.1132e-03, 1.4783e-02, 1.2707e-02, 7.0361e-03, 1.2448e-03, 3.7919e-03,\n",
      "        3.9425e-03, 1.1146e-02, 2.2687e-03, 1.1991e-02, 5.3620e-03, 1.8423e-02,\n",
      "        5.3297e-03, 3.0950e-04, 9.0452e-03, 3.9394e-03, 5.9630e-03, 5.5254e-03,\n",
      "        4.4169e-03, 8.6579e-03, 1.1174e-02, 9.4603e-06, 1.1060e-02, 8.5216e-03,\n",
      "        1.3839e-04, 1.9823e-03, 1.3819e-03, 1.4484e-02, 1.8576e-03, 1.2309e-02,\n",
      "        1.2339e-02, 9.6583e-03, 1.1912e-03, 1.0282e-02, 1.6877e-04, 1.6518e-02,\n",
      "        5.6318e-04, 3.4006e-04, 2.0802e-02, 4.0699e-03, 6.1113e-04, 2.5724e-03,\n",
      "        1.5772e-02, 4.1866e-03, 4.0601e-03, 1.3999e-03, 9.9256e-03, 5.3445e-03,\n",
      "        9.1463e-03, 2.1673e-03, 1.1405e-02, 2.4143e-02, 6.4193e-03, 1.2062e-03,\n",
      "        3.5425e-06, 4.2965e-03, 3.8241e-03, 4.1595e-03, 4.9055e-02, 3.5584e-03,\n",
      "        3.7421e-03, 5.9944e-02, 1.4578e-03, 5.9905e-04, 1.9786e-03, 1.0512e-05,\n",
      "        5.3094e-03, 1.0185e-02, 4.5572e-06, 2.1887e-02, 2.4644e-02, 1.4242e-03,\n",
      "        2.0588e-03, 4.5164e-02, 3.1052e-03, 2.9410e-04, 3.5449e-04, 1.0867e-04,\n",
      "        1.6977e-03, 1.5764e-03, 8.2237e-03, 2.8701e-04, 6.1479e-07, 2.9596e-04,\n",
      "        1.5764e-02, 1.7453e-02, 4.5319e-02, 7.3843e-02, 1.0681e-02, 1.4637e-02,\n",
      "        1.2500e-02, 2.2327e-02, 2.9093e-03, 8.0733e-02, 1.1454e-03, 2.4464e-03,\n",
      "        4.4738e-02, 9.0453e-03, 5.9491e-03, 1.6842e-03, 1.7718e-03, 1.2142e-02,\n",
      "        3.8205e-02, 6.9543e-02, 2.0236e-03, 2.8257e-02, 6.7704e-03, 5.6351e-03,\n",
      "        4.8786e-02, 1.7579e-02, 6.7442e-02, 2.2375e-03, 1.4128e-03, 1.0885e-02,\n",
      "        7.4302e-04, 1.6298e-03, 4.0923e-02, 7.6440e-03, 9.5234e-06, 1.5525e-03,\n",
      "        1.7958e-02, 5.1955e-03, 6.3796e-04, 2.7838e-02, 2.0538e-03, 2.1193e-02,\n",
      "        8.2558e-03, 1.0419e-03, 1.2900e-04, 2.9027e-02, 1.5307e-02, 7.0882e-03,\n",
      "        2.4405e-03, 2.5656e-05, 1.1477e-02, 3.9832e-02, 1.6072e-03, 2.8222e-03,\n",
      "        9.4434e-03, 4.2616e-02, 4.5875e-03, 4.8248e-05, 8.5909e-04, 9.4095e-03,\n",
      "        1.2931e-02, 1.2449e-02, 2.7896e-02, 2.9419e-03, 2.6977e-04, 3.4002e-02,\n",
      "        1.9767e-02, 1.8655e-03, 3.8250e-03, 1.2854e-02, 3.6052e-03, 6.1336e-02,\n",
      "        2.2005e-02, 4.6922e-03, 7.2147e-07, 1.5969e-03, 2.5887e-02, 1.4012e-02,\n",
      "        1.4275e-02, 1.6132e-02, 1.1692e-05, 1.8837e-02, 2.8061e-04, 5.7760e-03,\n",
      "        2.2847e-04, 1.5555e-02, 1.8345e-02, 3.5510e-02, 4.6676e-03, 2.5619e-03,\n",
      "        4.9439e-03, 8.2018e-05, 3.3001e-03, 1.6442e-02, 1.7432e-02, 3.5394e-04,\n",
      "        3.8882e-03, 4.1217e-06, 2.7155e-02, 6.5476e-05, 1.3990e-02, 2.5159e-03,\n",
      "        2.1840e-02, 4.0051e-03, 8.3444e-03, 1.0631e-02, 2.6253e-02, 3.7078e-04,\n",
      "        5.2698e-04, 1.1241e-05, 4.1402e-06, 7.6046e-03, 1.7728e-02, 1.3649e-04,\n",
      "        2.1148e-03, 2.1133e-02, 1.1325e-02, 2.6907e-02, 8.1476e-04, 1.5748e-02,\n",
      "        5.4075e-02, 1.7003e-02, 1.8609e-04, 1.2200e-02, 3.3980e-02, 7.4762e-03,\n",
      "        1.8654e-02, 2.4067e-03, 1.3385e-02, 3.0421e-02, 1.6846e-02, 1.5137e-03,\n",
      "        2.6696e-02, 8.5645e-03, 2.9542e-02, 1.0667e-01, 3.1098e-03, 1.4910e-02,\n",
      "        2.0812e-03, 2.0115e-04, 7.6047e-03, 6.6876e-03, 2.3222e-02, 7.7058e-04,\n",
      "        1.2014e-04, 1.5321e-02, 1.0970e-06, 3.6436e-06, 5.9737e-07, 3.3327e-02,\n",
      "        5.2023e-04, 1.0119e-02, 8.0515e-03, 3.5339e-04, 2.9972e-03, 1.5237e-02,\n",
      "        3.6649e-04, 2.3922e-02, 4.4263e-03, 1.1593e-05, 1.9384e-02, 8.9185e-03,\n",
      "        3.6102e-03, 1.0341e-02, 5.2614e-03, 2.2834e-02, 2.5014e-02, 4.1779e-03,\n",
      "        2.7359e-02, 5.1371e-03, 5.4578e-02, 9.5756e-03, 1.5456e-03, 1.6659e-02,\n",
      "        1.0389e-03, 4.7843e-03, 5.7588e-03, 1.3210e-02, 9.1028e-03, 3.0177e-02,\n",
      "        3.8744e-03, 1.1888e-03, 4.8644e-04, 8.4142e-03, 1.4075e-02, 5.9186e-04,\n",
      "        9.3110e-03, 1.0630e-03, 9.4466e-03, 8.4674e-03, 3.0905e-02, 4.1016e-03,\n",
      "        1.3939e-04, 1.1199e-02, 2.9051e-03, 2.0744e-02, 4.6631e-03, 3.2299e-02,\n",
      "        2.5266e-03, 1.3726e-03, 6.8311e-04, 4.2795e-02, 3.7271e-02, 2.0353e-03,\n",
      "        8.5423e-03, 7.1054e-06, 1.1674e-02, 2.7684e-03, 2.8933e-06, 1.4674e-02,\n",
      "        2.4479e-02, 4.5421e-04, 1.1605e-02, 1.0768e-03, 3.2586e-03, 3.5389e-02,\n",
      "        3.8871e-03, 3.0055e-03, 3.7823e-04, 1.0408e-02, 1.4532e-02, 2.1421e-02,\n",
      "        2.2709e-02, 4.5335e-03, 6.4559e-03, 2.0856e-04, 4.6911e-03, 2.8178e-02,\n",
      "        1.8815e-02, 1.2130e-02, 1.6829e-02, 1.2196e-02, 5.2972e-04, 5.9127e-03,\n",
      "        1.6030e-03, 3.7952e-04, 1.3491e-02, 8.6165e-03, 1.0929e-02, 9.1107e-07,\n",
      "        2.3183e-02, 2.5091e-04, 5.2094e-03, 6.3737e-02, 1.2580e-02, 9.4337e-03,\n",
      "        1.0206e-02, 3.6195e-03, 1.3356e-03, 1.5573e-02, 4.4316e-02, 2.5999e-03,\n",
      "        1.8120e-03, 1.4601e-02, 3.5062e-02, 7.6273e-02, 8.0366e-02, 1.1794e-03,\n",
      "        7.3812e-05, 2.6814e-03, 1.3949e-02, 1.0419e-02, 2.7302e-02, 1.2729e-02,\n",
      "        3.5621e-02, 1.9625e-02, 4.7319e-04, 2.5239e-03, 2.8560e-02, 1.3590e-02,\n",
      "        7.0461e-04, 1.6610e-02, 4.0266e-02, 7.3412e-03, 2.7908e-03, 5.9422e-04,\n",
      "        2.7195e-03, 1.6542e-02, 4.4148e-02, 1.7517e-02, 2.8230e-02, 3.2357e-02,\n",
      "        1.0317e-03, 1.7202e-02, 3.8326e-03, 9.5533e-03, 4.2830e-05, 1.9997e-06,\n",
      "        2.0351e-02, 2.3792e-02, 6.7046e-02, 9.1514e-03, 2.6358e-03, 2.9511e-04,\n",
      "        4.6010e-02, 1.1566e-01, 1.2436e-02, 1.2420e-02, 2.3358e-02, 1.2816e-02,\n",
      "        2.3022e-03, 3.3982e-02, 2.6000e-02, 2.7971e-02, 2.8403e-02, 2.4248e-02,\n",
      "        2.1193e-03, 3.8520e-03, 1.2260e-03, 2.2884e-02, 1.2832e-02, 2.1928e-02,\n",
      "        3.6603e-02, 8.4141e-03, 1.2506e-02, 2.3150e-05, 3.1671e-02, 1.7708e-04,\n",
      "        1.5668e-02, 4.6390e-04, 2.5301e-02, 1.7050e-04, 5.2772e-04, 1.8109e-02,\n",
      "        2.5460e-03, 4.7136e-04, 7.2183e-05, 3.3953e-02, 5.6622e-03, 2.0180e-02,\n",
      "        7.7205e-05, 2.0426e-03, 9.1019e-03, 9.8176e-03, 2.2902e-02, 2.8124e-03,\n",
      "        4.2868e-03, 1.7295e-02, 2.2190e-02, 2.1437e-02, 1.2303e-02, 3.5863e-04,\n",
      "        1.4977e-03, 1.5659e-02, 2.4271e-03, 4.6651e-02, 2.2567e-03, 1.4136e-05,\n",
      "        1.5837e-02, 6.2650e-04, 1.2076e-02, 2.0337e-02, 5.4104e-02, 4.8194e-03,\n",
      "        1.6297e-02, 1.1797e-04, 5.6671e-03, 3.4846e-03, 6.1000e-04, 1.4060e-02,\n",
      "        1.7009e-02, 2.3500e-06, 7.2251e-04, 9.0465e-03, 1.6590e-05, 2.2579e-03,\n",
      "        1.8182e-02, 3.3068e-03, 2.3900e-03, 2.3906e-03, 5.7355e-03, 5.2043e-04,\n",
      "        3.3773e-02, 4.5196e-03, 3.6546e-05, 1.2772e-04, 6.1567e-04, 4.8767e-03,\n",
      "        6.4028e-03, 2.9517e-03, 2.2336e-04, 7.4989e-04, 3.2365e-02, 1.2965e-02,\n",
      "        6.3399e-03, 3.7435e-05, 1.1291e-02, 1.2362e-03, 8.3006e-03, 1.9837e-03,\n",
      "        7.1330e-02, 4.4395e-02, 1.9192e-02, 2.9198e-02, 3.5814e-03, 6.6928e-04,\n",
      "        5.1888e-03, 1.3127e-03, 1.8636e-02, 2.8921e-03, 1.8838e-04, 1.5050e-03,\n",
      "        1.5604e-03, 2.9315e-03, 1.6914e-03, 1.5350e-03, 8.8149e-05, 7.9916e-03,\n",
      "        4.2361e-04, 5.5120e-03, 2.4679e-02, 1.9167e-03, 4.7178e-04, 3.2815e-03,\n",
      "        8.8558e-04, 7.5281e-03, 7.2245e-03, 5.0910e-04, 1.2631e-02, 2.9938e-02,\n",
      "        4.7740e-03, 3.3133e-04, 2.8563e-04, 7.7781e-04, 1.7913e-03, 2.6778e-02,\n",
      "        5.5860e-02, 1.3970e-03, 8.3813e-05, 3.3210e-03, 4.4684e-03, 1.0103e-03,\n",
      "        3.1345e-03, 3.3629e-03, 3.6214e-03, 1.6842e-04, 1.6944e-02, 1.4662e-02,\n",
      "        1.1114e-03, 4.4641e-05, 2.9549e-02, 2.5554e-04, 9.2039e-04, 1.0060e-02,\n",
      "        5.3433e-03, 1.0532e-02, 7.2287e-03, 2.1101e-03, 2.0931e-03, 4.0506e-02,\n",
      "        1.1624e-03, 2.2407e-03, 2.8791e-04, 1.0991e-02, 1.9389e-02, 7.7247e-02,\n",
      "        6.7703e-03, 2.1067e-02, 1.6982e-02, 2.3701e-02, 1.9962e-04, 4.7032e-06,\n",
      "        8.1812e-03, 1.0806e-03, 9.2235e-03, 1.3056e-03, 1.0293e-02, 1.0440e-02,\n",
      "        1.7503e-04, 2.7089e-02, 3.0780e-05, 5.6541e-03, 1.9211e-03, 9.1607e-03,\n",
      "        1.5755e-02, 4.5672e-03, 4.3828e-03, 9.9168e-03, 1.1275e-04, 1.1789e-03,\n",
      "        1.5326e-02, 1.2495e-02, 6.4346e-03, 1.2070e-02, 1.0636e-02, 1.8025e-03,\n",
      "        2.8993e-03, 3.0509e-03, 1.4082e-02, 3.3002e-02, 1.0527e-01, 2.9670e-03,\n",
      "        1.9804e-02, 2.1026e-02, 1.7970e-02, 2.3712e-03, 2.0447e-04, 1.0973e-02,\n",
      "        1.7707e-03, 1.3869e-03, 3.3237e-03, 4.0449e-03, 4.6733e-03, 3.8933e-03,\n",
      "        2.1575e-03, 6.8708e-03, 7.0900e-05, 7.5362e-04, 3.1853e-03, 8.3150e-05,\n",
      "        2.5826e-03, 4.9831e-03, 1.3448e-02, 3.3172e-02, 4.8169e-04, 3.2343e-03,\n",
      "        9.6173e-03, 8.2726e-04, 1.0375e-03, 1.8894e-04, 2.0285e-02, 2.3366e-02,\n",
      "        3.1561e-03, 5.9805e-03, 1.9244e-03, 8.1304e-07, 2.3921e-02, 6.4306e-03,\n",
      "        4.5635e-04, 2.6219e-03, 2.0826e-03, 1.1740e-02, 6.1895e-03, 1.9626e-03,\n",
      "        2.1683e-03, 6.9419e-04, 1.1199e-04, 1.2606e-03, 6.9734e-03, 5.8176e-04,\n",
      "        1.2404e-03, 1.9514e-03, 2.3055e-04, 8.1144e-03, 3.6178e-04, 9.4830e-03,\n",
      "        2.1589e-03, 2.0083e-02, 6.6990e-03, 5.5374e-03, 3.4174e-02, 8.5324e-03,\n",
      "        5.1820e-04, 6.4376e-04, 4.4396e-03, 1.2589e-02, 1.6214e-05, 1.5692e-03,\n",
      "        6.6720e-03, 6.4888e-03, 5.7887e-05, 1.4953e-02, 2.7012e-03, 9.2587e-05,\n",
      "        2.1005e-02, 6.7292e-03, 2.8415e-03, 1.0331e-03, 1.0183e-02, 3.1572e-03,\n",
      "        4.5155e-03, 1.5559e-02, 1.6379e-02, 1.5020e-03, 3.9445e-03, 4.9002e-03,\n",
      "        2.9018e-03, 2.6160e-02, 4.5730e-03, 3.1171e-02, 1.0175e-02, 3.4121e-03,\n",
      "        1.2480e-02, 1.2994e-03, 2.2827e-02, 7.5134e-03, 1.1951e-03, 1.2334e-02,\n",
      "        1.7817e-03, 1.7386e-02, 4.5838e-03, 1.0689e-02, 6.9761e-03, 6.3677e-03,\n",
      "        1.4657e-02, 8.9583e-03, 9.5677e-03, 1.0172e-02, 1.7909e-03, 3.4635e-05,\n",
      "        9.6413e-06, 3.4445e-02, 4.3221e-02, 1.0934e-02, 5.0521e-04, 2.6238e-02,\n",
      "        4.5502e-02, 2.4227e-04, 1.2114e-03, 1.9285e-03, 1.2565e-02, 8.8949e-03,\n",
      "        5.0606e-04, 3.1505e-03, 5.4238e-03, 2.1354e-02, 1.6293e-03, 8.7189e-04,\n",
      "        2.1494e-04, 2.8939e-03, 1.6144e-03, 1.7850e-02, 5.6420e-04, 2.2493e-03],\n",
      "       grad_fn=<DivBackward0>) tensor(0.1157, grad_fn=<UnbindBackward0>) tensor(12.1702, grad_fn=<AddBackward0>)\n",
      "Chosen edges: tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,\n",
      "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "          3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,\n",
      "          5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "          5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "          6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,\n",
      "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "          8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "          9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "         11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "         14],\n",
      "        [ 1,  2,  3,  4,  6,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 21, 22,\n",
      "         23, 24, 25, 28, 31, 33, 35, 37, 40, 42, 44, 47, 48, 50, 51, 54, 55, 56,\n",
      "         58, 60, 61, 62, 63,  5, 11, 13, 17, 18, 19, 20, 29, 33, 35, 39, 40, 43,\n",
      "         45, 47, 49, 53, 54, 56, 62,  0,  1,  2,  3,  6,  8,  9, 10, 11, 12, 13,\n",
      "         15, 16, 20, 21, 24, 26, 38, 39, 43, 44, 49, 50, 52, 54, 55, 56, 58,  2,\n",
      "          4, 10, 17, 20, 21, 22, 24, 26, 28, 29, 31, 40, 46, 47, 49, 54, 56, 57,\n",
      "         61, 62,  5,  7, 23, 25, 26, 31, 34, 38, 47, 54, 57,  1,  2,  5, 16, 17,\n",
      "         18, 19, 21, 22, 23, 25, 28, 33, 34, 35, 37, 40, 41, 42, 48, 52, 55, 57,\n",
      "         61, 62,  3,  7, 12, 13, 14, 17, 18, 21, 23, 24, 28, 29, 30, 31, 33, 37,\n",
      "         38, 39, 45, 46, 50, 52, 54, 58,  0,  3,  5,  7,  8,  9, 11, 12, 14, 16,\n",
      "         17, 18, 20, 22, 23, 25, 30, 33, 37, 43, 45, 48, 53, 54, 56, 58, 61,  1,\n",
      "          3,  8, 14, 19, 21, 25, 26, 30, 33, 34, 36, 39, 44, 45, 46, 51, 52, 53,\n",
      "         54, 55, 60,  0,  3,  4,  9, 10, 13, 14, 15, 16, 20, 22, 23, 24, 25, 28,\n",
      "         29, 31, 32, 37, 38, 39, 40, 41, 43, 48, 49, 50, 54, 55, 56, 57, 58, 59,\n",
      "         61, 62, 63,  0,  1,  5,  6,  7,  8, 10, 12, 14, 16, 19, 23, 25, 30, 33,\n",
      "         34, 35, 36, 39, 41, 44, 46, 47, 48, 50, 55, 56, 62,  4, 14, 15, 22, 23,\n",
      "         24, 25, 30, 42, 50, 51, 57, 58,  4,  5,  8, 17, 22, 23, 25, 26, 27, 37,\n",
      "         42, 48, 49, 51, 56, 57, 58, 60, 61, 62, 16, 17, 24, 25, 30, 35, 51, 54,\n",
      "         59,  1,  4, 11, 12, 17, 19, 22, 24, 27, 29, 34, 41, 42, 45, 46, 50, 55,\n",
      "         61]]) 343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:04<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Train Loss: 1.8473 | Val Loss: 23.7367 | Val Accuracy: 0.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 14.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Train Loss: 1.8409 | Val Loss: 23.6890 | Val Accuracy: 0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Train Loss: 1.8353 | Val Loss: 23.6511 | Val Accuracy: 0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Train Loss: 1.8295 | Val Loss: 23.5931 | Val Accuracy: 0.3611\n",
      "len_choose=[960, 343]\n",
      "Edge metrics: tensor([6.8479e-03, 4.7368e-03, 1.6906e-03,  ..., 4.4293e-18, 3.6331e-17,\n",
      "        1.7850e-02], grad_fn=<DivBackward0>) tensor(0.1157, grad_fn=<UnbindBackward0>) tensor(12.1702, grad_fn=<AddBackward0>)\n",
      "Chosen edges: tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   4,   4,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,\n",
      "           7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,\n",
      "           7,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
      "          11,  11,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  12,  13,  13,  13,  13,  13,  13,\n",
      "          13,  13,  13,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  14,  14,  14],\n",
      "        [ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "          78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "          92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "         106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "         120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "         134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
      "         148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
      "         162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
      "         176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
      "         190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
      "         204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
      "         218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
      "         232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245,\n",
      "         246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "         260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
      "         274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "         288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
      "         302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "         330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
      "         344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357,\n",
      "         358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371,\n",
      "         372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
      "         386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
      "         400, 401, 402, 403, 404, 405, 406]]) 343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:26<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Train Loss: 1.8330 | Val Loss: 23.6243 | Val Accuracy: 0.3600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:18<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Train Loss: 1.8258 | Val Loss: 23.5347 | Val Accuracy: 0.3631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:19<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Train Loss: 1.8244 | Val Loss: 23.5068 | Val Accuracy: 0.3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:21<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Train Loss: 1.8200 | Val Loss: 23.5732 | Val Accuracy: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:21<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Train Loss: 1.8202 | Val Loss: 23.4729 | Val Accuracy: 0.3693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:20<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Train Loss: 1.8138 | Val Loss: 23.4774 | Val Accuracy: 0.3682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:20<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Train Loss: 1.8125 | Val Loss: 23.3713 | Val Accuracy: 0.3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:19<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Train Loss: 1.8116 | Val Loss: 23.3656 | Val Accuracy: 0.3720\n",
      "len_choose=[960, 343, 343]\n",
      "Edge metrics: tensor([1.9745e-17, 1.2471e-17, 6.5014e-17, 7.8978e-18, 6.8548e-18, 5.6639e-17,\n",
      "        3.9027e-17, 3.8473e-18, 2.0362e-19, 3.8206e-18, 4.8750e-18, 2.3921e-02,\n",
      "        2.5640e-17, 8.6176e-17, 1.1833e-18, 3.3683e-18, 3.8017e-20, 1.0100e-18,\n",
      "        8.9990e-17, 8.8603e-18, 1.6444e-17, 8.0495e-17, 3.8554e-17, 2.1594e-17,\n",
      "        1.7176e-17, 2.8507e-17, 1.1740e-02, 3.4383e-18, 2.5664e-17, 2.0401e-17,\n",
      "        5.0359e-18, 6.1436e-17, 1.7868e-18, 8.4722e-18, 5.3289e-17, 6.4822e-17,\n",
      "        3.6677e-17, 6.9459e-17, 6.8378e-19, 6.3157e-19, 3.3506e-17, 2.0083e-02,\n",
      "        2.8152e-17, 3.8004e-18, 2.0883e-21, 8.6788e-17, 5.4937e-17, 9.3787e-18,\n",
      "        7.7427e-17, 2.2463e-17, 8.7331e-19, 1.5423e-19, 1.5677e-17, 7.8450e-17,\n",
      "        1.7859e-17, 8.2781e-17, 3.4174e-02, 9.9049e-18, 9.7355e-17, 5.0618e-18,\n",
      "        7.0011e-17, 5.3133e-17, 7.1411e-17, 5.3435e-18, 6.7562e-18, 2.1390e-17,\n",
      "        9.7826e-17, 2.8521e-18, 4.1390e-17, 6.7648e-18, 3.1441e-17, 1.2589e-02,\n",
      "        3.1862e-17, 2.3768e-17, 1.0045e-17, 3.5505e-17, 9.1323e-17, 3.5794e-17,\n",
      "        8.8227e-18, 1.2024e-17, 7.5255e-17, 6.2822e-17, 6.4585e-18, 4.8163e-17,\n",
      "        4.3296e-18, 1.3337e-17, 3.6285e-17, 1.4953e-02, 3.2312e-17, 1.6162e-17,\n",
      "        1.0024e-17, 5.3141e-17, 1.4905e-17, 6.1329e-17, 1.3897e-17, 9.7527e-17,\n",
      "        3.3946e-17, 9.7199e-18, 7.0948e-17, 2.1946e-18, 9.7147e-17, 5.1356e-17,\n",
      "        2.1005e-02, 9.8178e-17, 7.9686e-17, 2.2989e-17, 5.0238e-19, 4.2625e-17,\n",
      "        5.2039e-19, 9.8441e-17, 7.9043e-20, 2.7570e-17, 6.3677e-19, 8.3879e-17,\n",
      "        1.6348e-17, 1.4992e-17, 2.0702e-17, 1.5559e-02, 7.3164e-17, 4.5081e-17,\n",
      "        9.0100e-18, 9.2833e-17, 8.9278e-17, 7.6688e-17, 8.3198e-17, 4.6331e-17,\n",
      "        8.8035e-17, 7.9642e-19, 7.2524e-17, 4.4858e-18, 3.4771e-17, 5.1654e-20,\n",
      "        1.6379e-02, 1.9266e-17, 9.0422e-17, 4.5572e-17, 1.1691e-18, 6.7366e-17,\n",
      "        5.1939e-17, 2.5744e-18, 3.3450e-17, 3.5602e-17, 8.4665e-17, 1.4575e-17,\n",
      "        1.0014e-19, 3.6277e-18, 3.7354e-17, 2.6160e-02, 6.3909e-17, 9.1734e-18,\n",
      "        3.4829e-17, 1.2091e-17, 8.9129e-17, 2.6863e-17, 7.8485e-18, 1.6706e-17,\n",
      "        4.0860e-17, 8.1803e-17, 9.7241e-17, 2.6656e-18, 6.2425e-17, 3.2524e-18,\n",
      "        3.1171e-02, 2.9543e-18, 6.0584e-18, 1.8602e-17, 2.3109e-17, 8.3745e-17,\n",
      "        2.9547e-17, 3.7611e-19, 2.9626e-17, 3.6898e-17, 3.7169e-17, 3.3501e-17,\n",
      "        9.4261e-17, 9.5993e-17, 9.5787e-18, 1.2480e-02, 8.4380e-17, 5.2470e-17,\n",
      "        6.0577e-17, 5.0273e-18, 6.2806e-17, 2.0848e-17, 5.0547e-17, 5.1226e-18,\n",
      "        6.8753e-17, 3.0300e-17, 2.3394e-17, 4.4944e-19, 6.0231e-17, 5.4114e-20,\n",
      "        2.2827e-02, 3.6513e-17, 2.7731e-19, 4.6159e-17, 8.3159e-17, 1.3934e-17,\n",
      "        1.3663e-17, 9.6055e-17, 2.9174e-18, 2.9329e-17, 2.1396e-17, 1.7567e-18,\n",
      "        5.2220e-17, 9.6727e-19, 8.9418e-18, 1.2334e-02, 2.0856e-17, 9.6038e-17,\n",
      "        2.9792e-17, 2.8862e-17, 1.1371e-17, 4.7669e-17, 1.0212e-18, 7.6318e-17,\n",
      "        2.5817e-17, 5.2058e-17, 6.2787e-17, 5.2105e-19, 3.4883e-17, 3.6183e-17,\n",
      "        1.7386e-02, 6.8384e-17, 7.0792e-17, 3.1568e-18, 4.6472e-17, 5.5217e-17,\n",
      "        2.7885e-17, 2.8130e-17, 1.9383e-17, 2.3987e-18, 6.7030e-17, 9.3521e-18,\n",
      "        1.7514e-17, 2.6979e-17, 1.3139e-17, 1.4657e-02, 4.3460e-17, 4.2937e-17,\n",
      "        4.9500e-17, 1.5181e-17, 6.3469e-17, 3.7775e-17, 6.3054e-17, 1.8875e-20,\n",
      "        3.7075e-18, 4.7918e-17, 6.6555e-18, 1.1692e-17, 1.5464e-17, 2.8203e-17,\n",
      "        3.4445e-02, 3.7417e-17, 1.4370e-18, 1.2614e-18, 1.3347e-17, 2.5029e-19,\n",
      "        4.5247e-17, 2.1267e-17, 9.4782e-17, 1.7103e-17, 2.5234e-18, 8.6514e-18,\n",
      "        1.1856e-17, 6.0958e-17, 8.0483e-17, 4.3221e-02, 2.4420e-17, 2.0155e-17,\n",
      "        5.9747e-17, 4.4574e-17, 2.2702e-18, 5.2013e-18, 3.9449e-17, 3.2067e-18,\n",
      "        9.2717e-17, 2.1303e-18, 7.0833e-17, 2.1992e-17, 2.7894e-19, 5.0246e-17,\n",
      "        2.6238e-02, 3.2078e-17, 2.9349e-18, 2.0391e-18, 3.5657e-17, 4.5792e-17,\n",
      "        2.1045e-17, 9.7320e-18, 2.7190e-17, 3.1207e-18, 3.4164e-17, 7.9147e-17,\n",
      "        6.7204e-17, 2.0738e-17, 1.5857e-17, 4.5502e-02, 8.1482e-18, 7.4163e-17,\n",
      "        5.7834e-17, 4.2771e-17, 4.8231e-17, 1.3920e-17, 5.4474e-17, 3.6202e-17,\n",
      "        2.3036e-17, 8.1571e-18, 6.0393e-17, 1.9063e-18, 9.2015e-17, 8.5912e-17,\n",
      "        1.2565e-02, 1.0329e-19, 2.5136e-18, 2.3131e-17, 2.8242e-17, 7.2815e-18,\n",
      "        1.5827e-17, 1.4669e-17, 3.3948e-17, 7.6684e-17, 5.2721e-18, 4.8159e-17,\n",
      "        1.3603e-18, 4.3029e-17, 8.1630e-17, 2.1354e-02, 9.8104e-18, 1.0010e-17,\n",
      "        2.1885e-18, 8.7292e-17, 1.1409e-17, 6.4216e-17, 2.3713e-18, 9.9923e-18,\n",
      "        6.8917e-17, 1.4015e-18, 3.2941e-19, 1.7541e-17, 8.0659e-19, 3.5577e-18,\n",
      "        1.7850e-02]) tensor(0.0455) tensor(0.5086)\n",
      "Chosen edges: tensor([[ 0,  1,  1,  1,  2,  2,  2,  3,  3,  4,  4,  4,  4,  5,  5,  5,  6,  6,\n",
      "          7,  7,  7,  8,  8],\n",
      "        [36,  3, 25, 48, 17, 37, 63, 18, 42,  1, 18, 39, 56, 11, 39, 63, 22, 49,\n",
      "         10, 38, 60, 16, 40]]) 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:22<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Train Loss: 1.8128 | Val Loss: 23.3817 | Val Accuracy: 0.3688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:20<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Train Loss: 1.8088 | Val Loss: 23.3150 | Val Accuracy: 0.3729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:19<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Train Loss: 1.8077 | Val Loss: 23.3522 | Val Accuracy: 0.3740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:20<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Train Loss: 1.8071 | Val Loss: 23.3718 | Val Accuracy: 0.3729\n",
      "len_choose=[960, 343, 343, 23]\n",
      "Edge metrics: tensor([9.1804e-17, 1.3939e-04, 6.9565e-17, 1.0532e-17, 2.3735e-18, 5.0636e-18,\n",
      "        1.2927e-17, 1.0418e-18, 1.5293e-17, 9.4403e-17, 6.5893e-17, 1.3426e-17,\n",
      "        1.2470e-17, 8.7597e-17, 4.2483e-17, 8.7442e-17, 3.8871e-03, 6.3075e-17,\n",
      "        9.3694e-17, 2.5102e-17, 1.0039e-19, 8.7217e-19, 8.6775e-17]) tensor(0.0039) tensor(0.0040)\n",
      "Chosen edges: tensor([[ 0],\n",
      "        [46]]) 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:22<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Train Loss: 1.8070 | Val Loss: 23.3022 | Val Accuracy: 0.3769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:21<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Train Loss: 1.8048 | Val Loss: 23.3191 | Val Accuracy: 0.3733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:19<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Train Loss: 1.8030 | Val Loss: 23.2701 | Val Accuracy: 0.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:18<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Train Loss: 1.8021 | Val Loss: 23.3108 | Val Accuracy: 0.3776\n",
      "len_choose=[960, 343, 343, 23, 1]\n",
      "Edge metrics: tensor([9.3631e-18]) tensor(9.3631e-18) tensor(9.3631e-18)\n",
      "Chosen edges: tensor([], size=(2, 0), dtype=torch.int64) 0\n",
      "Empty metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: index out of row bound: 0 not between 1 and 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m wandb.finish()\n\u001b[32m      4\u001b[39m wandb.init(\n\u001b[32m      5\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33mself-expanding-nets\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtitanic-mul, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     tags=[\u001b[33m\"\u001b[39m\u001b[33mcomplex model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtitanic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m, hyperparams[\u001b[33m\"\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m\"\u001b[39m].\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m],\n\u001b[32m      8\u001b[39m     group=\u001b[33m\"\u001b[39m\u001b[33mnew activation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrain_sparse_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                       \u001b[49m\u001b[43medge_replacement_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_replacement_func_new_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m wandb.finish()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_sparse_recursive\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func, window_size, threshold, lr, choose_threshold, aggregation_mode, replace_all_epochs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[32m     22\u001b[39m     inputs, targets = inputs.to(device), targets.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     loss = criterion(outputs, targets)\n\u001b[32m     25\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mMulticlassFCN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.relu2(x)\n\u001b[32m     23\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout2(x)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# x = self.dropout2(x)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\model\\model.py:164\u001b[39m, in \u001b[36mExpandingLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# Применяем все EmbedLinear слои\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m embed_linear \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed_linears:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43membed_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Создаём разреженную матрицу весов с учётом маски\u001b[39;00m\n\u001b[32m    167\u001b[39m     masked_weight_values = \u001b[38;5;28mself\u001b[39m.weight_values * \u001b[38;5;28mself\u001b[39m.weight_mask \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    168\u001b[39m                                                                             \u001b[33m'\u001b[39m\u001b[33mweight_mask\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\model\\model.py:79\u001b[39m, in \u001b[36mEmbedLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m     78\u001b[39m     sparse_embed_weight = \u001b[38;5;28mself\u001b[39m.create_sparse_tensor()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_embed_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.t()\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.activation(output)], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: addmm: index out of row bound: 0 not between 1 and 0"
     ]
    }
   ],
   "source": [
    "dense_model = MulticlassFCN(input_size=X.shape[1])\n",
    "sparse_model = convert_dense_to_sparse_network(dense_model)\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project=\"self-expanding-nets\",\n",
    "    name=f\"titanic-mul, {name}\",\n",
    "    tags=[\"complex model\", \"titanic\", \"multiclass\", hyperparams[\"metric\"].__class__.__name__],\n",
    "    group=\"new activation\"\n",
    ")\n",
    "\n",
    "train_sparse_recursive(sparse_model, train_loader, val_loader,\n",
    "                       edge_replacement_func=edge_replacement_func_new_layer, **hyperparams)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bf1b57447aa62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.697345400Z",
     "start_time": "2025-02-11T11:19:18.201889Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afbc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
