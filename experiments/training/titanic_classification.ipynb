{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.318020Z",
     "start_time": "2025-02-11T21:01:25.170016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851a61bbbac63426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.409711Z",
     "start_time": "2025-02-11T21:02:20.359673Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import convert_dense_to_sparse_network, get_model_last_layer\n",
    "from senmodel.metrics.edge_finder import EdgeFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed98f9250fb9b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:20.443325Z",
     "start_time": "2025-02-11T21:02:20.428808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1c4b0b6e33d5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:23.590919Z",
     "start_time": "2025-02-11T21:02:20.480333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "data = pd.read_csv(url, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "y = data['occupation']\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X = data.drop(['occupation'], axis=1)\n",
    "\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d9e408b16a7a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:24.903666Z",
     "start_time": "2025-02-11T21:02:24.884257Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183cc85a9b097f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.139816Z",
     "start_time": "2025-02-11T21:02:25.010546Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7e5ab74bd84c4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.182863Z",
     "start_time": "2025-02-11T21:02:25.161851Z"
    }
   },
   "outputs": [],
   "source": [
    "class MulticlassFCN(nn.Module):\n",
    "    def __init__(self, input_size=14, hidden_sizes=None, output_size=15, dropout_rate=0.3):\n",
    "        super(MulticlassFCN, self).__init__()\n",
    "        if hidden_sizes is None:\n",
    "            hidden_sizes = [128, 64]\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.output = nn.Linear(hidden_sizes[1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        # x = self.dropout2(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe92b9bfdd4d7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.288977Z",
     "start_time": "2025-02-11T21:02:25.260801Z"
    }
   },
   "outputs": [],
   "source": [
    "def edge_replacement_func_new_layer(model, optim, val_loader, metric, choose_threshold, aggregation_mode='mean', len_choose=None):\n",
    "    layer = get_model_last_layer(model)\n",
    "    ef = EdgeFinder(metric, val_loader, device, aggregation_mode)\n",
    "    vals = ef.calculate_edge_metric_for_dataloader(model, len_choose, False)\n",
    "    print(\"Edge metrics:\", vals, max(vals, default=0), sum(vals))\n",
    "    chosen_edges = ef.choose_edges_threshold(model, choose_threshold, len_choose)\n",
    "    print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
    "    layer.replace_many(*chosen_edges)\n",
    "\n",
    "    if len(chosen_edges[0]) > 0:\n",
    "        optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
    "    else:\n",
    "        print(\"Empty metric\")\n",
    "\n",
    "    return {'max': max(vals, default=0), 'sum': sum(vals), 'len': len(vals), 'len_choose': layer.count_replaces[-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e4d086a0b3b29f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.591134Z",
     "start_time": "2025-02-11T21:02:25.433629Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import freeze_all_but_last, freeze_only_last\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_sparse_recursive(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func=None,\n",
    "                           window_size=3, threshold=0.1, lr=5e-4, choose_threshold=0.3, aggregation_mode='mean', replace_all_epochs=3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_losses = []\n",
    "\n",
    "    len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if len(len_choose) > replace_all_epochs and i > window_size:\n",
    "                freeze_all_but_last(model)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(all_targets, all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        new_l = dict()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        if edge_replacement_func and len(val_losses) > window_size:\n",
    "            recent_changes = [abs(val_losses[i] - val_losses[i - 1]) for i in range(-window_size, 0)]\n",
    "            avg_change = sum(recent_changes) / window_size\n",
    "            if avg_change < threshold:\n",
    "                print(f\"{len_choose=}\")\n",
    "                len_ch = len_choose[-1] if len(len_choose) > replace_all_epochs else None\n",
    "                new_l = edge_replacement_func(model, optimizer, val_loader, metric, choose_threshold, aggregation_mode, len_ch)\n",
    "                # Замораживаем все слои кроме последнего\n",
    "                val_losses = []\n",
    "                len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "        wandb.log({'val_loss': val_loss, 'val_accuracy': val_accuracy, 'train_loss': train_loss} | new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44615f5a401dd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:25.628149Z",
     "start_time": "2025-02-11T21:02:25.613167Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.metrics.nonlinearity_metrics import *\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [\n",
    "    AbsGradientEdgeMetric(criterion),\n",
    "    ReversedAbsGradientEdgeMetric(criterion),\n",
    "    SNIPMetric(criterion),\n",
    "    MagnitudeL2Metric(criterion),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c813771bca507d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.557848Z",
     "start_time": "2025-02-11T21:02:25.697276Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: fedornigretuk. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10df7d34ecdcb18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:02:39.805781Z",
     "start_time": "2025-02-11T21:02:39.779777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"num_epochs\": 50,\n",
    "               \"metric\": metrics[0],\n",
    "               \"aggregation_mode\": \"mean\",\n",
    "               \"choose_threshold\": 0.1,\n",
    "               \"window_size\": 3,\n",
    "               \"threshold\": 0.05,\n",
    "               \"lr\": 5e-4,\n",
    "               \"replace_all_epochs\": 2\n",
    "               }\n",
    "\n",
    "name = \", \".join(\n",
    "    f\"{key}: {value.__class__.__name__ if key == 'metric' else value}\"\n",
    "    for key, value in hyperparams.items()\n",
    ")\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fe9040816bb570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.151538Z",
     "start_time": "2025-02-11T21:02:40.108023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Coding\\PY\\self-expanding-nets\\experiments\\training\\wandb\\run-20250217_204936-ayc0578c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/ayc0578c' target=\"_blank\">titanic-mul, num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2</a></strong> to <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/ayc0578c' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/ayc0578c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 102.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.5898 | Val Loss: 31.3412 | Val Accuracy: 0.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 91.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.3290 | Val Loss: 27.9515 | Val Accuracy: 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 82.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 2.1677 | Val Loss: 26.2915 | Val Accuracy: 0.3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 65.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 2.0907 | Val Loss: 25.5653 | Val Accuracy: 0.3421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 108.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 2.0504 | Val Loss: 25.2382 | Val Accuracy: 0.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 104.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 2.0269 | Val Loss: 25.0457 | Val Accuracy: 0.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 107.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 2.0070 | Val Loss: 24.9003 | Val Accuracy: 0.3515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 99.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 1.9891 | Val Loss: 24.7735 | Val Accuracy: 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 84.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 1.9836 | Val Loss: 24.7035 | Val Accuracy: 0.3510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 104.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 1.9743 | Val Loss: 24.6210 | Val Accuracy: 0.3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 104.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 1.9617 | Val Loss: 24.5418 | Val Accuracy: 0.3513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 113.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 1.9585 | Val Loss: 24.5066 | Val Accuracy: 0.3547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 105.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 1.9527 | Val Loss: 24.4498 | Val Accuracy: 0.3562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 88.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 1.9533 | Val Loss: 24.4236 | Val Accuracy: 0.3564\n",
      "len_choose=[960]\n",
      "Edge metrics: tensor([2.1832e-03, 1.7168e-03, 1.6962e-03, 2.4361e-03, 1.2990e-03, 3.4370e-03,\n",
      "        3.7553e-03, 3.2096e-03, 4.1456e-03, 1.2484e-03, 1.8506e-03, 3.8651e-03,\n",
      "        1.8922e-03, 7.9294e-04, 1.2308e-03, 1.3074e-03, 5.9653e-04, 3.8191e-03,\n",
      "        4.1986e-03, 2.9507e-03, 5.1419e-03, 1.8864e-03, 9.4029e-04, 5.5347e-03,\n",
      "        2.8281e-03, 3.9857e-03, 4.1108e-03, 4.5621e-03, 1.6913e-03, 4.5774e-03,\n",
      "        2.2085e-03, 2.8353e-03, 4.4594e-03, 8.7726e-05, 1.9147e-03, 7.6381e-04,\n",
      "        2.6647e-03, 3.1062e-04, 3.5528e-03, 5.8953e-03, 3.6279e-03, 3.3805e-03,\n",
      "        1.5322e-03, 3.0239e-03, 6.4215e-03, 2.7720e-03, 1.6117e-03, 1.9075e-03,\n",
      "        4.6834e-03, 2.7111e-03, 1.5790e-03, 2.0312e-03, 1.7066e-03, 5.8721e-03,\n",
      "        3.9045e-03, 1.3319e-03, 1.2573e-03, 2.4578e-03, 1.2967e-03, 2.1365e-03,\n",
      "        1.1610e-03, 2.7331e-04, 2.4104e-03, 2.1264e-03, 8.4733e-03, 5.2388e-03,\n",
      "        7.4498e-03, 1.0917e-02, 1.1448e-02, 2.7823e-03, 7.4151e-03, 4.3240e-03,\n",
      "        3.6443e-03, 5.3931e-03, 4.7181e-03, 5.8683e-03, 1.0129e-02, 4.3573e-03,\n",
      "        7.3721e-03, 1.2201e-02, 2.6928e-03, 1.1745e-02, 8.3380e-03, 4.3786e-03,\n",
      "        4.3598e-03, 4.3028e-03, 9.5800e-03, 4.7082e-03, 7.8563e-03, 3.2271e-03,\n",
      "        6.2591e-03, 2.7879e-03, 1.2759e-02, 3.0792e-03, 7.9729e-03, 3.7339e-03,\n",
      "        7.1482e-03, 1.1044e-03, 3.7775e-03, 4.8625e-03, 8.1679e-03, 5.4417e-03,\n",
      "        5.7208e-03, 4.2684e-03, 1.0326e-02, 9.7725e-03, 6.7533e-03, 5.3824e-03,\n",
      "        9.0271e-03, 2.9948e-03, 1.0341e-02, 8.1347e-03, 7.0320e-03, 4.7803e-03,\n",
      "        9.0254e-03, 4.1674e-03, 5.5073e-03, 6.2867e-03, 2.4105e-03, 4.8429e-03,\n",
      "        7.5916e-03, 8.0741e-03, 4.2242e-03, 8.5327e-03, 5.0584e-03, 2.9930e-03,\n",
      "        5.4443e-03, 6.3166e-03, 2.1465e-04, 1.3709e-04, 2.1762e-04, 2.3704e-04,\n",
      "        3.1212e-04, 3.0598e-04, 3.4188e-04, 3.0214e-04, 3.4940e-04, 3.9374e-04,\n",
      "        3.1625e-04, 2.9235e-04, 3.4783e-04, 1.7617e-04, 1.7745e-04, 4.8793e-04,\n",
      "        9.6650e-05, 8.4625e-04, 4.9208e-04, 3.8020e-04, 4.6569e-04, 3.2057e-04,\n",
      "        2.2167e-04, 5.4184e-04, 5.6052e-04, 3.9775e-04, 3.5538e-04, 4.1302e-04,\n",
      "        4.4438e-04, 3.7360e-04, 3.6378e-04, 2.9472e-04, 4.0704e-04, 1.4299e-05,\n",
      "        2.5084e-04, 1.9997e-04, 6.2293e-04, 1.0872e-04, 4.5318e-04, 5.2166e-04,\n",
      "        5.8993e-04, 7.1094e-04, 1.6429e-04, 2.2428e-04, 6.9174e-04, 3.0627e-04,\n",
      "        4.4930e-04, 3.0933e-04, 4.8562e-04, 3.6374e-04, 3.1965e-04, 2.9875e-04,\n",
      "        3.1060e-04, 6.7681e-04, 3.9754e-04, 3.1027e-04, 2.0258e-04, 1.8201e-04,\n",
      "        2.5944e-04, 2.6721e-04, 1.3883e-04, 1.2764e-04, 3.8273e-04, 2.5226e-04,\n",
      "        4.5825e-03, 2.4412e-03, 3.9016e-03, 5.8064e-03, 5.3708e-03, 1.8908e-03,\n",
      "        5.2698e-03, 2.6938e-03, 2.2827e-03, 1.2372e-02, 7.4133e-03, 2.7149e-03,\n",
      "        5.9843e-03, 2.4342e-03, 3.4745e-03, 1.0026e-02, 1.6514e-03, 1.7306e-02,\n",
      "        4.7347e-03, 9.9202e-03, 8.3240e-03, 7.8165e-03, 4.4351e-03, 4.2477e-03,\n",
      "        1.0768e-02, 2.7652e-03, 3.6689e-03, 5.4673e-03, 7.6118e-03, 2.4552e-03,\n",
      "        6.2336e-03, 1.0595e-03, 4.7073e-03, 7.2449e-04, 9.3235e-03, 5.9480e-03,\n",
      "        1.6641e-02, 7.0050e-03, 3.1121e-03, 3.1506e-03, 6.4518e-03, 2.0330e-02,\n",
      "        4.3352e-03, 3.3114e-03, 5.0450e-03, 4.1441e-03, 1.3610e-02, 6.0448e-03,\n",
      "        4.9091e-03, 1.1104e-02, 5.6364e-03, 8.0234e-03, 4.7712e-03, 5.1995e-03,\n",
      "        2.3771e-03, 1.0641e-02, 4.9279e-03, 4.1960e-03, 6.2005e-03, 5.0738e-03,\n",
      "        2.8104e-03, 8.4601e-03, 5.7325e-03, 9.2124e-03, 5.8514e-03, 8.4434e-03,\n",
      "        1.3760e-02, 9.5194e-03, 1.5868e-02, 3.7240e-03, 1.3816e-02, 4.4953e-03,\n",
      "        3.5360e-03, 8.9112e-03, 5.0495e-03, 9.3098e-03, 1.6756e-02, 6.5809e-03,\n",
      "        9.4118e-03, 2.4303e-02, 1.0331e-03, 2.8828e-02, 3.5546e-03, 6.1680e-03,\n",
      "        7.8916e-03, 5.2513e-03, 1.4176e-02, 4.7024e-03, 1.9569e-02, 3.2490e-03,\n",
      "        5.7976e-03, 5.9474e-03, 2.0810e-02, 4.1652e-03, 6.1340e-03, 1.3940e-03,\n",
      "        1.1453e-02, 6.5428e-04, 8.5002e-03, 7.5428e-03, 1.7652e-02, 1.1886e-02,\n",
      "        2.4443e-03, 4.6911e-03, 9.0496e-03, 2.8064e-02, 1.1067e-02, 5.1656e-03,\n",
      "        4.5625e-03, 3.8228e-03, 2.5033e-02, 5.8362e-03, 4.2118e-03, 1.1047e-02,\n",
      "        1.4100e-02, 5.1750e-03, 4.0158e-03, 4.3305e-03, 2.9712e-03, 6.8697e-03,\n",
      "        1.5213e-02, 6.3010e-03, 4.8150e-03, 5.8200e-03, 1.9983e-03, 5.2788e-03,\n",
      "        2.9417e-03, 1.4751e-02, 2.2354e-03, 1.0802e-03, 1.8798e-03, 2.7135e-03,\n",
      "        2.6504e-03, 4.0536e-04, 2.6157e-03, 1.1916e-03, 7.9371e-04, 8.4396e-03,\n",
      "        4.1210e-03, 1.0586e-03, 3.8545e-03, 1.6248e-03, 1.4833e-03, 6.5315e-03,\n",
      "        6.7977e-04, 1.1417e-02, 9.6172e-04, 4.9686e-03, 4.1604e-03, 4.2560e-03,\n",
      "        3.3425e-03, 2.2034e-03, 5.1905e-03, 9.7021e-04, 1.2802e-03, 3.0017e-03,\n",
      "        4.8334e-03, 6.2163e-04, 4.4489e-03, 5.6314e-04, 1.6853e-03, 5.6965e-04,\n",
      "        4.4105e-03, 5.5289e-03, 1.1067e-02, 8.4120e-03, 1.0078e-03, 1.2093e-03,\n",
      "        3.1115e-03, 1.0913e-02, 1.7419e-03, 1.4052e-03, 1.7215e-03, 2.3383e-03,\n",
      "        8.9402e-03, 6.2014e-03, 2.4988e-03, 4.4661e-03, 3.7715e-03, 4.1330e-03,\n",
      "        3.0141e-03, 2.7097e-03, 6.7879e-04, 6.9802e-03, 2.3001e-03, 2.4357e-03,\n",
      "        4.1869e-03, 2.4379e-03, 7.3953e-04, 8.3423e-03, 3.9362e-03, 5.8594e-03,\n",
      "        2.4388e-03, 6.9575e-04, 1.8083e-03, 3.0996e-03, 2.7610e-03, 1.6672e-03,\n",
      "        2.2726e-03, 1.5293e-03, 2.2699e-03, 8.3825e-03, 5.9778e-03, 1.3644e-03,\n",
      "        3.4608e-03, 1.1946e-03, 1.9728e-03, 4.0249e-03, 2.2774e-03, 6.6489e-03,\n",
      "        4.0816e-03, 7.2232e-03, 4.6287e-03, 6.6505e-03, 1.7912e-03, 4.4430e-03,\n",
      "        4.8569e-03, 2.4960e-03, 2.5444e-03, 2.8588e-03, 3.7433e-03, 2.1490e-03,\n",
      "        4.7027e-03, 1.5322e-03, 2.1517e-03, 7.1045e-04, 5.0289e-03, 3.0383e-03,\n",
      "        9.8000e-03, 2.8530e-03, 3.7361e-03, 3.1226e-03, 6.4865e-03, 8.5839e-03,\n",
      "        1.4924e-03, 1.6990e-03, 5.6104e-03, 2.9480e-03, 5.1000e-03, 5.2406e-03,\n",
      "        5.6200e-03, 5.1480e-03, 2.8088e-03, 6.9641e-03, 5.7260e-03, 6.5004e-03,\n",
      "        1.8913e-03, 6.6661e-03, 1.7694e-03, 2.6495e-03, 6.2160e-03, 3.8644e-03,\n",
      "        2.3789e-03, 3.8572e-03, 5.7670e-03, 4.6388e-03, 4.9720e-03, 1.1857e-03,\n",
      "        1.9796e-03, 5.7182e-03, 2.5693e-03, 1.0957e-03, 2.8544e-03, 1.6932e-03,\n",
      "        1.7746e-03, 7.2404e-03, 6.1566e-03, 1.2965e-03, 4.2673e-03, 1.8071e-03,\n",
      "        2.3995e-03, 6.2123e-03, 1.4210e-03, 1.1404e-02, 3.5163e-03, 5.5874e-03,\n",
      "        4.8435e-03, 4.4280e-03, 1.8330e-03, 2.2458e-03, 7.1790e-03, 1.1880e-03,\n",
      "        4.1750e-03, 3.9127e-03, 5.5663e-03, 1.5257e-03, 6.4367e-03, 1.5186e-03,\n",
      "        1.8302e-03, 1.0911e-03, 6.2312e-03, 4.3049e-03, 9.7489e-03, 3.6014e-03,\n",
      "        2.8345e-03, 2.6982e-03, 6.1166e-03, 1.3313e-02, 1.9595e-03, 3.9028e-03,\n",
      "        5.5496e-03, 1.9464e-03, 8.0585e-03, 7.6593e-03, 5.5930e-03, 5.7723e-03,\n",
      "        4.0266e-03, 5.4797e-03, 4.0574e-03, 3.8292e-03, 1.4440e-03, 5.6757e-03,\n",
      "        3.1162e-03, 6.3828e-03, 4.7296e-03, 6.0947e-03, 2.1143e-03, 3.6584e-03,\n",
      "        4.5611e-03, 4.9309e-03, 5.9854e-03, 2.3381e-03, 3.9138e-03, 7.9267e-03,\n",
      "        5.7583e-03, 1.4226e-03, 3.6940e-03, 3.7228e-03, 1.7676e-03, 7.3246e-03,\n",
      "        7.2864e-03, 3.1469e-03, 6.3601e-03, 2.2981e-03, 4.4077e-03, 7.4360e-03,\n",
      "        3.1400e-03, 1.0819e-02, 8.2169e-03, 5.7947e-03, 3.7718e-03, 6.2909e-03,\n",
      "        4.6798e-03, 3.7431e-03, 5.6474e-03, 2.3178e-03, 5.1245e-03, 2.7302e-03,\n",
      "        8.8125e-03, 1.8278e-03, 7.4470e-03, 2.6202e-03, 2.6996e-03, 1.8527e-03,\n",
      "        3.8398e-03, 4.8211e-03, 9.8758e-03, 3.2670e-03, 6.0953e-03, 2.6899e-03,\n",
      "        1.0134e-02, 9.0492e-03, 3.5139e-03, 4.4375e-03, 9.9702e-03, 2.7600e-03,\n",
      "        8.5992e-03, 9.8228e-03, 8.1378e-03, 4.7398e-03, 5.9527e-03, 5.3307e-03,\n",
      "        8.3112e-03, 8.2220e-03, 1.7378e-03, 6.0307e-03, 5.6374e-03, 5.7672e-03,\n",
      "        6.3960e-03, 7.4860e-03, 4.7134e-03, 5.5295e-03, 8.7830e-03, 4.8249e-03,\n",
      "        3.0242e-03, 4.0050e-04, 8.2210e-04, 2.4368e-03, 1.2680e-03, 2.0012e-04,\n",
      "        6.7113e-04, 1.2114e-03, 2.9945e-04, 1.5150e-03, 1.9603e-03, 5.6397e-04,\n",
      "        1.4977e-03, 6.2727e-04, 9.7226e-04, 1.4928e-03, 1.5749e-03, 1.6293e-03,\n",
      "        3.2239e-03, 9.7781e-04, 6.6811e-04, 1.1135e-03, 1.0469e-03, 5.2973e-04,\n",
      "        1.2104e-03, 2.9505e-04, 2.1061e-03, 6.5754e-04, 1.7658e-03, 2.9654e-04,\n",
      "        3.2884e-03, 1.3088e-03, 4.7436e-04, 4.8888e-04, 4.9678e-04, 6.9697e-04,\n",
      "        1.5339e-03, 7.1441e-04, 2.3284e-03, 4.1994e-04, 2.0495e-03, 1.3679e-03,\n",
      "        5.2182e-04, 1.4410e-03, 3.3307e-03, 3.0170e-04, 1.4700e-03, 4.0141e-03,\n",
      "        3.7060e-03, 7.1140e-04, 1.5121e-03, 7.3355e-04, 2.6998e-03, 2.0075e-03,\n",
      "        2.8375e-04, 1.1823e-03, 1.1867e-03, 3.1246e-03, 2.0717e-03, 3.3766e-03,\n",
      "        2.2764e-03, 9.7415e-04, 3.6806e-03, 8.2052e-04, 4.4813e-03, 1.2352e-02,\n",
      "        1.7526e-02, 8.3576e-03, 2.1409e-02, 4.3997e-03, 1.6220e-02, 4.2057e-03,\n",
      "        4.3364e-03, 4.0721e-03, 2.8767e-03, 1.1565e-02, 1.8949e-02, 1.0311e-02,\n",
      "        1.0828e-02, 2.7055e-02, 1.1990e-03, 2.9599e-02, 3.9479e-03, 3.6185e-03,\n",
      "        7.4872e-03, 3.1718e-03, 1.8381e-02, 3.8495e-03, 2.1645e-02, 3.3415e-03,\n",
      "        5.0231e-03, 5.3801e-03, 2.2606e-02, 5.0914e-03, 4.4452e-03, 1.7704e-03,\n",
      "        1.4404e-02, 8.2388e-04, 6.9252e-03, 6.0349e-03, 1.3354e-02, 1.2887e-02,\n",
      "        2.8909e-03, 5.6035e-03, 9.1188e-03, 2.8930e-02, 1.6610e-02, 3.8215e-03,\n",
      "        5.8076e-03, 3.9081e-03, 2.8845e-02, 4.1082e-03, 3.5482e-03, 8.0783e-03,\n",
      "        1.6156e-02, 3.0232e-03, 2.9567e-03, 4.1469e-03, 4.1488e-03, 3.3033e-03,\n",
      "        1.7384e-02, 4.8620e-03, 3.0328e-03, 4.3572e-03, 1.6313e-03, 2.0033e-03,\n",
      "        2.3986e-03, 1.3539e-02, 1.3466e-03, 2.3569e-03, 2.8883e-03, 1.9632e-03,\n",
      "        2.8102e-03, 2.3844e-03, 5.2492e-03, 2.0881e-03, 3.9882e-03, 5.0393e-03,\n",
      "        3.0800e-03, 3.0768e-03, 3.9541e-03, 9.0291e-04, 1.4986e-03, 6.1090e-03,\n",
      "        4.5343e-04, 8.8484e-03, 1.3587e-03, 4.8965e-03, 5.7422e-03, 3.2338e-03,\n",
      "        2.5824e-03, 5.2110e-03, 5.4824e-03, 4.2855e-03, 2.8726e-03, 3.8950e-03,\n",
      "        5.4521e-03, 3.6289e-03, 3.0952e-03, 9.2712e-04, 5.2717e-03, 3.1553e-04,\n",
      "        4.0207e-03, 3.3757e-03, 7.0609e-03, 2.8269e-03, 1.2074e-03, 5.9022e-03,\n",
      "        3.3233e-03, 9.1921e-03, 2.7717e-03, 2.0825e-03, 2.5160e-03, 3.2616e-03,\n",
      "        6.9936e-03, 3.0658e-03, 2.3541e-03, 4.6332e-03, 2.8660e-03, 3.4562e-03,\n",
      "        1.5832e-03, 2.9484e-03, 1.9661e-03, 3.8514e-03, 2.6551e-03, 1.5093e-03,\n",
      "        1.9983e-03, 1.8876e-03, 6.3520e-04, 3.3455e-03, 1.8066e-03, 4.5456e-03,\n",
      "        4.9049e-03, 4.5623e-03, 6.9724e-03, 8.6804e-03, 8.5758e-03, 2.1259e-03,\n",
      "        6.8080e-03, 3.3417e-03, 2.4714e-03, 5.8282e-03, 4.2452e-03, 4.7529e-03,\n",
      "        1.0388e-02, 3.4645e-03, 5.0503e-03, 1.1983e-02, 2.6290e-03, 1.5020e-02,\n",
      "        4.8534e-03, 5.3716e-03, 4.2002e-03, 4.2863e-03, 8.0233e-03, 2.9513e-03,\n",
      "        9.9668e-03, 2.2205e-03, 5.2260e-03, 5.0202e-03, 1.3419e-02, 2.6855e-03,\n",
      "        5.7412e-03, 2.8263e-03, 5.7753e-03, 1.0388e-03, 5.5701e-03, 5.2875e-03,\n",
      "        8.7306e-03, 5.0765e-03, 4.1939e-03, 3.4871e-03, 6.7764e-03, 1.4487e-02,\n",
      "        6.6507e-03, 5.0960e-03, 5.5124e-03, 1.5644e-03, 1.2986e-02, 6.3448e-03,\n",
      "        5.1393e-03, 6.1110e-03, 8.4353e-03, 4.6263e-03, 3.5877e-03, 4.7099e-03,\n",
      "        2.0547e-03, 4.8354e-03, 9.1386e-03, 5.8251e-03, 4.4541e-03, 5.1254e-03,\n",
      "        3.2201e-03, 3.8944e-03, 4.6945e-03, 6.6504e-03, 4.1858e-03, 3.4114e-03,\n",
      "        5.6925e-03, 5.4332e-03, 7.4307e-03, 1.8798e-03, 5.0665e-03, 3.1945e-03,\n",
      "        2.4970e-03, 3.9484e-03, 3.1473e-03, 3.9051e-03, 7.2877e-03, 2.9180e-03,\n",
      "        4.6777e-03, 8.0278e-03, 8.0928e-04, 7.4644e-03, 3.0157e-03, 3.2790e-03,\n",
      "        3.5015e-03, 2.8706e-03, 5.8237e-03, 2.8099e-03, 5.2613e-03, 2.4504e-03,\n",
      "        4.0245e-03, 3.1955e-03, 8.9232e-03, 2.3953e-03, 3.4101e-03, 8.6535e-04,\n",
      "        4.5081e-03, 4.6282e-04, 2.7345e-03, 2.9247e-03, 5.5473e-03, 2.1536e-03,\n",
      "        1.8597e-03, 2.9558e-03, 4.9365e-03, 6.1385e-03, 4.2797e-03, 3.4324e-03,\n",
      "        3.8790e-03, 2.2490e-03, 6.8550e-03, 3.9584e-03, 3.1393e-03, 3.0643e-03,\n",
      "        6.8183e-03, 2.8357e-03, 2.2306e-03, 2.7935e-03, 1.5540e-03, 3.2418e-03,\n",
      "        5.9522e-03, 3.7475e-03, 2.1172e-03, 3.4268e-03, 1.4087e-03, 2.5822e-03,\n",
      "        1.7100e-03, 3.8727e-03, 2.5325e-03, 1.9051e-03, 2.8538e-03, 3.1260e-03,\n",
      "        3.2750e-03, 1.3575e-03, 3.6715e-03, 1.8687e-03, 1.8411e-03, 7.5392e-03,\n",
      "        4.8592e-03, 2.0195e-03, 4.2765e-03, 1.6218e-03, 1.6469e-03, 6.6412e-03,\n",
      "        8.6974e-04, 1.1534e-02, 2.3580e-03, 4.2722e-03, 4.0541e-03, 3.5449e-03,\n",
      "        2.7517e-03, 2.8972e-03, 6.7951e-03, 2.4749e-03, 2.3396e-03, 2.3595e-03,\n",
      "        5.8822e-03, 1.9433e-03, 4.7227e-03, 6.1324e-04, 2.4307e-03, 7.2234e-04,\n",
      "        6.1553e-03, 4.3453e-03, 1.0250e-02, 4.8968e-03, 1.7060e-03, 2.6485e-03,\n",
      "        5.1629e-03, 1.2970e-02, 2.4196e-03, 1.9883e-03, 3.6559e-03, 3.2915e-03,\n",
      "        8.8507e-03, 6.8461e-03, 3.5989e-03, 5.3112e-03, 3.2034e-03, 3.6503e-03,\n",
      "        3.4355e-03, 4.3537e-03, 1.5194e-03, 7.3864e-03, 2.8734e-03, 2.4088e-03,\n",
      "        3.9075e-03, 3.5868e-03, 1.0748e-03, 6.4675e-03, 4.1136e-03, 5.6129e-03]) tensor(0.0296) tensor(4.3945)\n",
      "Chosen edges: tensor([[ 0,  0,  0,  ..., 14, 14, 14],\n",
      "        [ 5,  6,  7,  ..., 61, 62, 63]]) 573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 72.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 1.9454 | Val Loss: 24.3818 | Val Accuracy: 0.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 53.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Train Loss: 1.9431 | Val Loss: 24.3662 | Val Accuracy: 0.3565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 65.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Train Loss: 1.9409 | Val Loss: 24.3291 | Val Accuracy: 0.3559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Train Loss: 1.9364 | Val Loss: 24.2958 | Val Accuracy: 0.3570\n",
      "len_choose=[960, 573]\n",
      "Edge metrics: tensor([2.3991e-03, 1.8013e-03, 2.2935e-03, 3.2650e-03, 1.3082e-03, 1.3218e-03,\n",
      "        1.7522e-03, 2.5955e-03, 1.1807e-03, 1.6787e-03, 1.2722e-03, 1.0054e-03,\n",
      "        3.1778e-03, 1.6472e-03, 6.3173e-04, 2.6329e-03, 2.1894e-03, 2.6316e-03,\n",
      "        3.7623e-03, 1.0522e-04, 1.9644e-03, 1.0260e-03, 3.1994e-03, 2.7721e-04,\n",
      "        1.4615e-03, 5.3441e-03, 1.3659e-03, 1.6259e-03, 2.5448e-03, 2.0136e-03,\n",
      "        1.9519e-03, 1.5611e-03, 1.4902e-03, 1.5736e-03, 2.9301e-03, 1.5010e-03,\n",
      "        2.5302e-03, 1.0886e-03, 1.1720e-04, 3.7827e-03, 3.0611e-03, 2.8645e-03,\n",
      "        4.2156e-03, 3.5212e-03, 1.2156e-03, 2.7006e-03, 1.9279e-04, 1.1320e-04,\n",
      "        1.6831e-04, 2.1576e-04, 2.5787e-04, 2.9266e-04, 2.7392e-04, 2.8000e-04,\n",
      "        3.6759e-04, 3.4932e-04, 2.6668e-04, 2.6218e-04, 2.7275e-04, 1.9901e-04,\n",
      "        1.7198e-04, 3.7733e-04, 1.4849e-04, 7.5884e-04, 4.9847e-04, 3.1541e-04,\n",
      "        4.2544e-04, 2.7242e-04, 1.7016e-04, 5.6955e-04, 4.1656e-04, 3.5354e-04,\n",
      "        2.9891e-04, 3.8952e-04, 3.7865e-04, 3.6347e-04, 3.4320e-04, 3.1434e-04,\n",
      "        3.7912e-04, 1.4598e-05, 1.9925e-04, 2.0282e-04, 5.6647e-04, 7.8064e-05,\n",
      "        4.8543e-04, 5.3042e-04, 5.0244e-04, 5.6511e-04, 9.9250e-05, 2.3391e-04,\n",
      "        6.0040e-04, 3.0944e-04, 3.2710e-04, 2.3961e-04, 4.7571e-04, 2.8763e-04,\n",
      "        2.9077e-04, 2.5069e-04, 2.7203e-04, 6.4335e-04, 3.5657e-04, 2.7173e-04,\n",
      "        1.8142e-04, 1.6069e-04, 2.3614e-04, 2.3324e-04, 1.2015e-04, 6.0889e-05,\n",
      "        4.0116e-04, 2.1453e-04, 2.6725e-03, 2.0029e-03, 2.8635e-03, 2.4168e-03,\n",
      "        3.0085e-03, 2.8910e-03, 2.1907e-03, 2.2130e-03, 2.4410e-03, 1.1442e-03,\n",
      "        6.3402e-04, 2.3082e-03, 2.6400e-03, 1.5592e-03, 1.5884e-03, 6.8092e-04,\n",
      "        2.5622e-03, 3.2038e-03, 1.7219e-03, 2.8521e-03, 2.1520e-03, 1.2831e-03,\n",
      "        1.7817e-03, 2.8964e-03, 2.6422e-03, 4.2679e-04, 3.0461e-03, 1.0696e-03,\n",
      "        8.7309e-04, 1.5801e-03, 1.9939e-03, 1.7023e-03, 8.4805e-04, 1.0885e-03,\n",
      "        2.0874e-03, 9.5164e-04, 1.2925e-03, 6.0261e-04, 4.9434e-04, 1.8033e-03,\n",
      "        4.7202e-04, 1.0432e-03, 1.2577e-03, 1.5329e-03, 1.3976e-03, 1.2297e-03,\n",
      "        2.2372e-03, 2.4802e-03, 2.4475e-03, 5.9450e-04, 2.4433e-03, 2.3134e-03,\n",
      "        2.2530e-03, 7.8402e-04, 2.4014e-03, 7.4285e-04, 1.6821e-03, 2.7891e-03,\n",
      "        1.6621e-03, 2.1945e-03, 1.4894e-03, 2.5148e-03, 1.3472e-03, 1.5920e-03,\n",
      "        2.0568e-03, 3.0548e-03, 1.6130e-03, 2.2233e-03, 2.4123e-03, 3.1614e-03,\n",
      "        2.1386e-03, 1.6188e-03, 2.2121e-03, 6.8350e-04, 2.9148e-03, 1.1878e-03,\n",
      "        1.8823e-03, 3.3155e-03, 2.8204e-03, 1.7880e-03, 1.7196e-03, 2.3632e-03,\n",
      "        2.4129e-03, 1.1752e-03, 1.7765e-03, 2.6580e-03, 1.1346e-03, 2.8570e-03,\n",
      "        1.6640e-03, 2.1012e-03, 1.2516e-03, 2.2430e-03, 2.4141e-03, 1.8976e-03,\n",
      "        1.8401e-03, 2.4761e-03, 1.5564e-03, 1.5478e-03, 1.6410e-03, 1.8681e-03,\n",
      "        1.2815e-03, 3.0333e-03, 2.9989e-03, 1.7929e-03, 2.0938e-03, 1.3667e-03,\n",
      "        2.2573e-03, 2.3667e-03, 1.4806e-03, 1.9879e-03, 2.8328e-03, 2.1891e-03,\n",
      "        3.3185e-03, 1.8937e-03, 2.4528e-03, 2.9253e-03, 1.9804e-03, 2.8400e-03,\n",
      "        3.0862e-03, 1.7495e-03, 3.7849e-04, 8.0405e-04, 3.0828e-03, 1.2975e-03,\n",
      "        1.9396e-04, 6.4603e-04, 1.2138e-03, 3.0269e-04, 1.5532e-03, 2.0441e-03,\n",
      "        6.3217e-04, 1.4538e-03, 6.7212e-04, 9.8164e-04, 1.4031e-03, 2.1047e-03,\n",
      "        1.8386e-03, 8.8652e-04, 5.7182e-04, 1.0098e-03, 1.0486e-03, 5.2723e-04,\n",
      "        1.0588e-03, 2.4264e-04, 2.4327e-03, 7.3379e-04, 1.7965e-03, 2.5258e-04,\n",
      "        1.2115e-03, 4.6239e-04, 6.0137e-04, 4.4621e-04, 9.5347e-04, 1.6625e-03,\n",
      "        7.5786e-04, 2.3831e-03, 4.2839e-04, 1.8957e-03, 1.3658e-03, 3.6067e-04,\n",
      "        1.7420e-03, 3.3898e-04, 1.4799e-03, 6.0212e-04, 1.7603e-03, 7.3430e-04,\n",
      "        2.5223e-03, 1.9101e-03, 2.4662e-04, 1.2194e-03, 1.3217e-03, 2.5288e-03,\n",
      "        2.4402e-03, 7.6692e-04, 8.1364e-04, 3.0792e-03, 1.7837e-03, 2.0774e-03,\n",
      "        8.8962e-04, 3.5733e-03, 2.7836e-03, 1.4951e-03, 2.3634e-03, 2.5003e-03,\n",
      "        1.3911e-03, 2.4987e-03, 2.7236e-03, 2.0126e-03, 2.7210e-03, 2.4674e-03,\n",
      "        2.1583e-03, 1.3699e-03, 1.6150e-03, 6.6119e-04, 1.6156e-03, 2.4223e-03,\n",
      "        2.6276e-03, 1.0129e-03, 3.3367e-04, 2.8428e-03, 1.4195e-03, 2.2820e-03,\n",
      "        2.3867e-03, 2.3411e-03, 2.4063e-03, 3.2687e-03, 1.4707e-03, 3.0334e-03,\n",
      "        2.0011e-03, 2.7894e-03, 1.4433e-03, 1.9017e-03, 1.7120e-03, 6.3785e-04,\n",
      "        1.9941e-03, 2.1451e-03, 2.6485e-03, 3.6298e-03, 2.9669e-03, 2.1209e-03,\n",
      "        2.5819e-03, 3.0765e-03, 1.1175e-03, 1.7089e-03, 1.8826e-03, 2.0546e-03,\n",
      "        2.9146e-03, 3.5398e-03, 1.1546e-03, 2.9740e-03, 3.2696e-03, 2.6077e-03,\n",
      "        2.5322e-03, 9.3434e-04, 5.1869e-04, 3.1542e-03, 3.7145e-03, 2.4555e-03,\n",
      "        2.1893e-03, 3.3814e-03, 2.5928e-03, 2.6526e-03, 1.9059e-03, 2.7424e-03,\n",
      "        1.5737e-03, 2.1003e-03, 1.3615e-03, 1.9691e-03, 1.7623e-03, 2.5930e-03,\n",
      "        2.0436e-03, 2.7475e-03, 1.4973e-03, 1.9916e-03, 2.1071e-03, 2.3478e-03,\n",
      "        2.3671e-03, 1.8485e-03, 1.0546e-03, 2.2697e-03, 2.6822e-03, 3.3739e-03,\n",
      "        2.1971e-03, 2.3838e-03, 2.5927e-03, 2.0129e-03, 5.4172e-04, 2.6032e-03,\n",
      "        5.0600e-04, 1.8302e-03, 2.7791e-03, 2.1675e-03, 2.2945e-03, 1.5074e-03,\n",
      "        2.9946e-03, 2.4174e-03, 1.1146e-03, 1.0420e-03, 9.5054e-04, 1.0469e-03,\n",
      "        1.4054e-03, 7.8989e-04, 1.1898e-03, 5.4720e-04, 6.7602e-04, 1.0137e-03,\n",
      "        1.2100e-03, 1.3222e-03, 3.3657e-04, 1.1722e-03, 1.2262e-03, 9.4495e-04,\n",
      "        1.0202e-03, 9.0869e-04, 9.3991e-04, 8.1164e-04, 1.0753e-03, 4.0428e-04,\n",
      "        1.0336e-03, 1.2061e-03, 1.3884e-03, 1.1197e-04, 3.9883e-04, 1.5577e-03,\n",
      "        5.7529e-04, 2.3073e-04, 3.7873e-04, 3.0516e-04, 4.3556e-04, 2.8177e-04,\n",
      "        5.6408e-06, 9.2776e-04, 3.8810e-04, 5.8040e-04, 6.3168e-04, 7.7621e-04,\n",
      "        2.4027e-04, 5.5897e-05, 2.0667e-04, 2.3935e-04, 5.2063e-04, 1.7952e-04,\n",
      "        4.1091e-04, 2.8295e-04, 1.0004e-03, 3.4776e-04, 6.0039e-05, 7.3447e-04,\n",
      "        7.0732e-06, 6.9035e-05, 2.4901e-04, 5.7941e-04, 3.8893e-04, 5.6509e-04,\n",
      "        4.5551e-05, 8.5992e-05, 5.0954e-04, 1.5896e-04, 2.2414e-06, 7.3132e-04,\n",
      "        5.2432e-04, 4.7509e-05, 1.1733e-04, 5.5241e-04, 5.2959e-04, 2.3566e-04,\n",
      "        8.8892e-04, 1.6196e-04, 9.5964e-05, 2.8761e-04, 3.5201e-04, 6.7620e-04,\n",
      "        1.2755e-03, 5.7804e-04, 9.7073e-04, 8.2656e-06, 2.6808e-04, 6.0033e-04,\n",
      "        5.4875e-04, 1.4683e-04, 2.8332e-04, 6.8531e-04, 3.2170e-04, 1.4741e-04,\n",
      "        9.1615e-04, 8.2035e-04, 1.0327e-04, 1.9343e-04, 6.1653e-04, 1.6104e-03,\n",
      "        4.3661e-04, 1.0077e-03, 6.9453e-04, 7.9721e-04, 2.7391e-04, 1.5785e-04,\n",
      "        1.0983e-03, 2.0128e-04, 5.5929e-04, 4.8336e-04, 3.7022e-04, 5.8837e-05,\n",
      "        6.3108e-04, 6.9642e-04, 1.3889e-03, 1.4629e-05, 5.4145e-04, 3.1463e-04,\n",
      "        2.6740e-04, 6.7600e-04, 9.5747e-05, 1.3758e-04, 1.7072e-04, 1.3458e-04,\n",
      "        1.8429e-04, 3.1896e-04, 4.5339e-04, 1.2549e-03, 2.1240e-04, 1.0586e-03,\n",
      "        1.9001e-04, 5.0241e-04, 1.0513e-03, 5.6774e-04, 3.9630e-04, 7.4418e-04,\n",
      "        2.8337e-05, 2.3583e-04, 1.7687e-04, 2.1377e-04, 2.1970e-04, 8.2334e-04,\n",
      "        1.3873e-03, 6.8748e-04, 1.2629e-03, 5.1689e-05, 1.4936e-03, 2.7991e-04,\n",
      "        2.1481e-05, 3.3764e-04, 1.0989e-04, 8.8790e-04, 1.7855e-03, 1.4712e-04,\n",
      "        4.2386e-04, 2.2375e-03, 2.3975e-03, 4.2561e-04, 1.8895e-04, 4.1374e-04,\n",
      "        1.3118e-04, 1.3768e-03, 2.5975e-04, 2.0178e-03, 2.6360e-04, 3.0259e-04,\n",
      "        1.9783e-04, 5.3441e-04, 6.9671e-05, 2.1014e-04, 1.2843e-03, 4.6832e-04,\n",
      "        5.0552e-04, 1.3006e-03, 1.2765e-03, 3.1400e-04, 3.9500e-04, 1.0982e-03,\n",
      "        1.5639e-03, 3.5009e-04, 2.7055e-04, 1.2301e-04, 5.9248e-04, 2.9304e-04,\n",
      "        3.3556e-04, 1.1868e-03, 1.1779e-03, 4.4288e-05, 4.1129e-04, 4.5956e-04,\n",
      "        2.7293e-04, 1.5821e-03, 5.7649e-04, 3.4915e-04, 8.7831e-05, 1.1197e-04,\n",
      "        8.7232e-04, 8.4372e-04, 2.2753e-04, 2.8546e-04, 1.5435e-04, 7.1080e-04,\n",
      "        2.4567e-04, 7.4452e-04, 5.2915e-05, 2.7596e-06, 1.1324e-04, 7.2463e-04,\n",
      "        2.8303e-04, 3.2942e-04, 1.0239e-05, 8.3444e-04, 8.6929e-04, 1.5263e-03,\n",
      "        1.7251e-04, 1.7019e-04, 1.7115e-04, 7.3656e-04, 6.0902e-05, 2.7366e-04,\n",
      "        1.0571e-04, 1.8074e-04, 8.8899e-04, 5.5765e-04, 1.2154e-03, 3.4970e-04,\n",
      "        2.4551e-04, 1.1563e-04, 1.7224e-04, 6.1333e-04, 2.0857e-04, 1.1421e-04,\n",
      "        1.0287e-04, 2.6482e-04, 7.2295e-04, 8.3971e-06, 5.6132e-04, 1.7949e-05,\n",
      "        3.4230e-04, 4.6899e-04, 4.2823e-04, 5.1619e-04, 2.7637e-04, 5.3738e-04,\n",
      "        2.4700e-04, 5.4533e-04, 4.5578e-04, 4.3365e-04, 5.3181e-04, 1.0634e-03,\n",
      "        5.2556e-05, 5.0261e-04, 3.3350e-05, 8.8114e-04, 4.9911e-04, 7.4823e-04,\n",
      "        2.9165e-04, 3.3218e-04, 4.9978e-04, 1.8109e-04, 4.2783e-04, 5.3638e-04,\n",
      "        5.2545e-04, 5.7137e-04, 3.3584e-04, 7.9236e-04, 6.3488e-06, 3.1917e-04,\n",
      "        3.7664e-04, 2.3194e-04, 3.4298e-04, 3.6244e-04, 1.3043e-04, 2.2160e-04,\n",
      "        2.3277e-04, 1.9446e-04, 3.7880e-04, 6.6365e-04, 4.0073e-04, 2.4285e-04,\n",
      "        5.1108e-04, 3.3470e-04, 2.1172e-04, 2.0079e-04, 3.5695e-04, 3.7153e-04,\n",
      "        5.2062e-04, 6.8223e-04, 5.2928e-04, 4.9271e-04, 3.4336e-05, 7.1655e-04,\n",
      "        3.0514e-04, 2.1759e-04, 2.4647e-04, 1.1427e-04, 1.0293e-03, 2.8209e-04,\n",
      "        7.3878e-04, 2.1136e-04, 4.3034e-04, 9.8526e-05, 7.9914e-04, 5.4309e-05,\n",
      "        9.6129e-04, 4.1293e-04, 3.4018e-04, 5.4028e-04, 2.3261e-04, 2.8415e-04,\n",
      "        2.1667e-04, 4.7914e-04, 1.4551e-04, 3.0009e-04, 1.7876e-04, 5.3296e-04,\n",
      "        6.5020e-04, 1.9982e-04, 2.9646e-04, 2.7005e-04, 3.2761e-04, 3.8812e-05,\n",
      "        7.0003e-05, 5.0524e-04, 1.3663e-04, 5.7332e-04, 7.1565e-04, 1.9356e-04,\n",
      "        1.5832e-05, 1.7281e-04, 5.8218e-04, 7.4144e-04, 4.6818e-04, 1.5542e-04,\n",
      "        2.8395e-04, 1.1395e-03, 4.2730e-04, 8.1449e-04, 8.7505e-04, 5.8379e-05,\n",
      "        4.2417e-04, 5.1240e-04, 8.4214e-04, 8.1714e-04, 2.4539e-04, 3.0059e-04,\n",
      "        7.4540e-04, 6.9784e-04, 7.8085e-04, 7.4065e-04, 9.8405e-05, 8.6579e-04,\n",
      "        6.6534e-04, 1.3591e-04, 3.7125e-04, 8.6581e-05, 2.2741e-04, 1.5743e-04,\n",
      "        2.0733e-04, 5.8721e-04, 1.4733e-04, 4.8781e-04, 7.1073e-05, 1.7399e-03,\n",
      "        1.8285e-03, 7.4051e-04, 1.9496e-03, 5.9692e-04, 1.6815e-03, 3.6501e-04,\n",
      "        1.0329e-04, 2.8066e-04, 1.3397e-03, 1.8024e-03, 1.4838e-03, 1.2954e-03,\n",
      "        2.1710e-03, 2.0588e-03, 3.7184e-04, 4.5739e-04, 3.1695e-04, 5.2821e-04,\n",
      "        1.8131e-03, 2.0559e-04, 1.8880e-03, 1.4298e-04, 3.1102e-05, 2.4187e-04,\n",
      "        8.2697e-04, 5.2918e-04, 7.0840e-04, 1.6741e-03, 4.1375e-04, 3.2219e-04,\n",
      "        6.7019e-04, 1.1286e-03, 4.5505e-05, 1.8439e-04, 1.1188e-03, 1.9379e-03,\n",
      "        1.9374e-04, 2.0711e-05, 7.0272e-04, 7.0967e-04, 1.9906e-04, 5.6972e-04,\n",
      "        3.7984e-04, 1.5750e-03, 4.1385e-04, 2.9229e-04, 4.3887e-04, 3.1542e-04,\n",
      "        1.7762e-03, 2.2568e-04, 3.0442e-04, 6.0997e-04, 4.8496e-04, 2.0366e-04,\n",
      "        3.0872e-04, 2.2718e-04, 3.7527e-04, 5.9425e-07, 3.8199e-04, 6.3350e-05,\n",
      "        1.0056e-04, 8.2716e-05, 1.7454e-04, 1.0460e-04, 5.4954e-04, 1.0952e-05,\n",
      "        5.7998e-04, 1.4122e-04, 2.5607e-04, 1.4912e-04, 1.8116e-04, 6.6110e-04,\n",
      "        2.0740e-04, 3.5333e-04, 1.1447e-04, 5.6251e-04, 3.8532e-04, 5.1252e-06,\n",
      "        1.3903e-04, 1.8878e-04, 2.5744e-04, 9.6887e-05, 6.0739e-05, 1.0946e-04,\n",
      "        3.9006e-05, 2.2447e-04, 4.1172e-04, 7.7476e-05, 5.7906e-04, 8.6300e-04,\n",
      "        4.8665e-04, 5.4704e-04, 1.3781e-04, 1.0123e-04, 2.8523e-04, 1.1174e-05,\n",
      "        9.7786e-04, 1.4519e-05, 5.6752e-04, 1.1457e-03, 1.1512e-03, 4.0097e-04,\n",
      "        2.4290e-04, 2.6773e-05, 8.5127e-05, 1.7653e-04, 9.1405e-04, 3.4379e-04,\n",
      "        3.3407e-04, 7.6023e-05, 9.5146e-05, 3.0017e-04, 1.4496e-04, 4.5213e-04,\n",
      "        3.6457e-04, 6.1102e-04, 2.2276e-04, 1.0081e-03, 5.6201e-06, 6.0013e-04,\n",
      "        5.1774e-04, 4.3567e-05, 4.8191e-04, 1.1402e-04, 3.2948e-04, 7.5621e-05,\n",
      "        6.9652e-04, 8.6327e-04, 7.4358e-05, 1.8076e-04, 1.8884e-04, 3.1496e-04,\n",
      "        4.3410e-04, 5.1309e-04, 2.3703e-04, 4.1910e-04, 6.0543e-04, 6.1671e-05,\n",
      "        1.3418e-04, 4.4185e-04, 1.9228e-04, 4.9516e-05, 5.4726e-04, 3.9176e-04,\n",
      "        4.7915e-04, 2.6928e-04, 3.0182e-04, 1.2542e-04, 3.9794e-04, 1.7436e-04,\n",
      "        4.4506e-04, 4.3888e-04, 4.4358e-04, 4.5396e-04, 2.7636e-04, 1.0037e-04,\n",
      "        1.7317e-04, 7.8388e-05, 4.9337e-04, 8.8821e-05, 1.4425e-04, 2.0222e-04,\n",
      "        3.8089e-04, 4.3847e-04, 2.6888e-04, 1.0771e-04, 3.5586e-05, 1.3325e-06,\n",
      "        1.2122e-04, 9.5163e-06, 2.5438e-04, 4.1287e-04, 9.2127e-05, 1.1785e-04,\n",
      "        1.7126e-04, 3.4449e-04, 2.3046e-04, 2.4540e-04, 3.9201e-05, 1.4808e-06,\n",
      "        1.8746e-04, 5.6732e-04, 7.3531e-05, 6.3618e-04, 4.7639e-04, 4.6866e-04,\n",
      "        3.2712e-04, 6.1515e-04, 3.1265e-04, 1.4899e-04, 1.0980e-04, 2.5869e-04,\n",
      "        8.2474e-04, 2.5604e-04, 1.4957e-04, 4.1919e-05, 7.9670e-04, 3.4282e-04,\n",
      "        2.2696e-04, 1.7434e-04, 5.8392e-05, 4.0987e-04, 3.2763e-04, 5.9109e-04,\n",
      "        1.6196e-05, 2.3872e-04, 2.8440e-04, 5.3671e-04, 8.7712e-05, 3.3049e-04,\n",
      "        7.5559e-04, 3.0294e-04, 4.6759e-05, 2.5874e-04, 1.6864e-04, 2.3721e-04]) tensor(0.0053) tensor(0.9061)\n",
      "Chosen edges: tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,\n",
      "           1,   2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   3,   3,   3,\n",
      "           3,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,   4,   4,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,\n",
      "           7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,  10,\n",
      "          10,  10,  10,  10,  10,  10,  10,  10,  11,  11,  11,  11,  11,  11,\n",
      "          11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
      "          11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  13,  13,  13,  13,  13,  13,  13,  13,\n",
      "          13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,\n",
      "          13,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n",
      "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "           3,   3,   3,   3,   3,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   6,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   7,   7,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   9,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  11,  11,  11,  11,  12,  12,  12,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  13,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14],\n",
      "        [  0,   1,   2,   3,   4,   9,  10,  12,  13,  14,  15,  16,  19,  21,\n",
      "          22,  24,  28,  30,  31,  34,  35,  36,  42,  45,  46,  47,  49,  50,\n",
      "          51,  52,  55,  56,  57,  58,  59,  60,  62,  63,   5,  16,  27,  33,\n",
      "          54,  17,  23,  36,  41,  44,  53,   1,   5,   7,   8,  11,  13,  16,\n",
      "          25,  29,  31,  33,  54,  60,  16,  31,  33,  38,  54,  60,  62,   0,\n",
      "           1,   2,   3,   4,   6,   7,   8,  11,  13,  14,  16,  18,  23,  25,\n",
      "          26,  29,  32,  38,  39,  42,  43,  44,  45,  48,  53,  54,  56,  57,\n",
      "          59,  60,   0,   1,   2,   4,   5,   6,   7,   8,  11,  13,  14,  16,\n",
      "          22,  25,  26,  27,  29,  31,  32,  33,  37,  42,  43,  45,  50,  54,\n",
      "          56,  57,  60,   1,   2,   4,   5,   6,   7,   8,  11,  13,  14,  16,\n",
      "          22,  23,  25,  29,  31,  32,  33,  38,  39,  42,  45,  54,  60,   1,\n",
      "           5,   8,  13,  25,  27,  29,  31,  32,  33,  39,  45,  54,   2,   3,\n",
      "           4,   6,   7,   9,  10,  11,  12,  13,  14,  15,  16,  17,  19,  20,\n",
      "          21,  22,  24,  26,  27,  28,  31,  33,  35,  36,  37,  38,  40,  41,\n",
      "          43,  46,  49,  50,  51,  52,  53,  55,  56,  58,  60,  61,  63,  10,\n",
      "          16,  31,  33,  38,  52,  60,  61,  62,   0,   1,   2,   3,   4,   5,\n",
      "           7,  13,  14,  16,  18,  22,  26,  31,  37,  38,  42,  43,  44,  48,\n",
      "          50,  52,  53,  54,  56,  57,  58,  59,  60,  62,   5,   8,  16,  23,\n",
      "          25,  29,  31,  33,  45,  54,   5,   8,  13,  16,  21,  23,  25,  29,\n",
      "          31,  34,  35,  37,  38,  39,  45,  51,  52,  53,  54,  58,  60,  61,\n",
      "          62,   0,   1,   2,   5,   7,   8,  11,  13,  14,  16,  18,  22,  23,\n",
      "          25,  26,  27,  29,  31,  32,  38,  39,  42,  43,  54,  56,  57,  60,\n",
      "          64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  76,  77,  78,\n",
      "          79,  80,  81,  82,  83,  85,  86,  87,  90,  91,  98, 100, 101, 102,\n",
      "         111, 114, 118, 120, 126, 130, 133, 138, 139, 140, 141, 144, 145, 148,\n",
      "         151, 152, 155, 156, 158, 159, 160, 163, 165, 169, 170, 171, 173, 176,\n",
      "         184, 186, 189, 190, 192, 198, 199, 200, 201, 203, 208, 209, 212, 213,\n",
      "         218, 220, 227, 230, 231, 234, 235, 239, 242, 243, 248, 249, 253, 254,\n",
      "         258, 260, 264, 268, 269, 270, 274, 279, 280, 281, 286, 291, 293, 300,\n",
      "         302, 306, 310, 312, 318, 320, 322, 334, 344, 348, 353, 355, 359, 361,\n",
      "         364, 373, 382, 383, 387, 388, 392, 394, 395, 399, 400, 403, 404, 405,\n",
      "         406, 408, 409, 416, 420, 421, 422, 423, 424, 425, 429, 430, 431, 432,\n",
      "         433, 434, 439, 441, 445, 447, 448, 451, 452, 455, 456, 459, 460, 462,\n",
      "         464, 469, 472, 485, 487, 492, 496, 509, 510, 512, 517, 519, 520, 521,\n",
      "         527, 536, 538, 540, 547, 548, 557, 563, 602, 604, 608, 613, 617, 624,\n",
      "         628, 631]]) 492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 40.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Train Loss: 1.9333 | Val Loss: 24.2758 | Val Accuracy: 0.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Train Loss: 1.9314 | Val Loss: 24.2412 | Val Accuracy: 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 41.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Train Loss: 1.9300 | Val Loss: 24.2259 | Val Accuracy: 0.3577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 34.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Train Loss: 1.9273 | Val Loss: 24.1837 | Val Accuracy: 0.3582\n",
      "len_choose=[960, 573, 492]\n",
      "Edge metrics: tensor([3.6022e-04, 1.2145e-06, 2.2145e-04, 7.1305e-04, 1.7397e-04, 2.6078e-04,\n",
      "        4.1071e-04, 1.3411e-04, 6.7830e-05, 5.7254e-05, 2.1704e-04, 1.4596e-04,\n",
      "        4.5712e-04, 6.1487e-04, 8.6210e-05, 5.8863e-04, 6.4395e-04, 7.2830e-04,\n",
      "        8.2924e-04, 5.1927e-04, 2.1901e-04, 7.0289e-04, 9.0538e-06, 1.0625e-03,\n",
      "        2.5074e-04, 2.8317e-04, 7.5409e-04, 3.4120e-04, 1.6598e-04, 1.8305e-04,\n",
      "        1.8327e-04, 8.5224e-05, 3.1174e-04, 2.3027e-05, 3.2663e-04, 6.7139e-05,\n",
      "        6.4504e-04, 4.3011e-04, 9.1294e-05, 6.1551e-04, 2.8891e-04, 1.2240e-04,\n",
      "        1.3489e-04, 1.4925e-04, 6.1333e-05, 1.2060e-04, 1.0921e-04, 6.4871e-05,\n",
      "        4.3141e-05, 8.2689e-05, 2.7749e-04, 7.9314e-05, 1.1796e-04, 4.3022e-04,\n",
      "        1.6342e-04, 1.4858e-04, 1.3979e-04, 3.9954e-04, 1.7070e-04, 3.9611e-05,\n",
      "        3.4515e-04, 3.1607e-05, 3.0976e-04, 9.4099e-05, 5.0239e-05, 4.7912e-04,\n",
      "        1.5180e-04, 1.9852e-04, 3.6293e-04, 5.0686e-06, 2.0061e-04, 1.1530e-04,\n",
      "        3.3275e-04, 3.8347e-04, 3.6619e-05, 9.7336e-05, 1.5893e-04, 8.7752e-05,\n",
      "        3.1077e-04, 9.1708e-05, 1.6937e-05, 5.9300e-05, 2.4862e-04, 1.4262e-04,\n",
      "        2.4312e-04, 9.5108e-05, 2.8913e-04, 1.8884e-05, 8.0401e-05, 1.9873e-04,\n",
      "        2.2968e-04, 8.1031e-05, 1.1551e-05, 7.2786e-05, 8.1609e-05, 2.0309e-06,\n",
      "        1.3723e-04, 3.4361e-04, 4.3678e-05, 2.2838e-05, 2.3813e-05, 1.0214e-04,\n",
      "        1.1377e-04, 2.4847e-05, 3.6794e-04, 3.7147e-04, 1.3914e-04, 2.6803e-04,\n",
      "        7.1060e-05, 3.3595e-05, 1.1728e-04, 6.9215e-05, 1.8441e-04, 3.8280e-05,\n",
      "        1.9836e-04, 1.0343e-04, 1.6621e-04, 1.0977e-04, 8.5535e-05, 4.5740e-05,\n",
      "        8.9098e-05, 1.8948e-04, 1.1339e-04, 8.5911e-05, 1.6481e-04, 7.2366e-05,\n",
      "        2.5462e-04, 1.0324e-04, 6.8066e-05, 1.0223e-04, 2.5185e-04, 1.1279e-04,\n",
      "        2.2986e-04, 3.6533e-04, 1.2819e-05, 4.1735e-04, 1.9202e-04, 2.8465e-04,\n",
      "        3.7164e-04, 1.4177e-04, 1.9224e-04, 8.3530e-06, 6.3729e-05, 2.0705e-04,\n",
      "        1.0711e-04, 1.6717e-04, 3.0440e-04, 1.8441e-04, 5.2642e-04, 4.9985e-05,\n",
      "        1.5119e-05, 2.0880e-04, 1.1756e-04, 2.2685e-04, 2.6276e-04, 1.7795e-04,\n",
      "        2.6004e-04, 3.4375e-05, 5.4445e-05, 2.2581e-04, 1.1077e-05, 2.4295e-04,\n",
      "        4.4624e-04, 7.0613e-05, 1.9885e-04, 5.2800e-05, 4.4687e-05, 2.7977e-04,\n",
      "        8.1472e-05, 1.3779e-04, 7.5764e-05, 4.4854e-04, 2.4639e-04, 4.7525e-05,\n",
      "        1.6346e-04, 1.0401e-04, 6.1842e-06, 1.6508e-04, 1.9189e-04, 4.2277e-04,\n",
      "        1.2207e-04, 6.0780e-05, 2.2051e-04, 3.5902e-05, 1.1839e-04, 2.8065e-04,\n",
      "        1.1810e-04, 3.6833e-04, 8.3325e-05, 1.8572e-05, 5.6938e-05, 3.7020e-04,\n",
      "        1.5357e-04, 3.8982e-05, 3.2827e-04, 3.3729e-04, 1.8787e-04, 3.2713e-04,\n",
      "        1.5311e-04, 8.8654e-05, 1.0748e-04, 1.4578e-04, 2.3834e-04, 4.2155e-04,\n",
      "        6.4225e-05, 1.4373e-04, 2.8611e-04, 1.6331e-04, 2.1909e-04, 4.3622e-04,\n",
      "        6.5811e-05, 9.1657e-05, 1.5452e-04, 2.5403e-04, 4.0360e-04, 5.1553e-05,\n",
      "        3.0524e-04, 2.2817e-04, 1.7506e-04, 1.9912e-04, 2.8553e-04, 3.1045e-04,\n",
      "        1.3606e-04, 2.0808e-06, 8.9477e-05, 1.5876e-04, 2.8759e-05, 1.0281e-04,\n",
      "        7.5779e-05, 1.6384e-04, 4.2379e-05, 6.9698e-05, 2.0952e-04, 8.3620e-05,\n",
      "        7.6770e-05, 2.2780e-04, 1.5090e-04, 3.2508e-04, 5.3147e-04, 9.8185e-05,\n",
      "        8.3842e-05, 1.1284e-04, 2.7533e-04, 1.3378e-04, 2.0774e-04, 3.0208e-04,\n",
      "        1.5767e-05, 1.1346e-04, 5.0625e-04, 7.0105e-04, 5.5354e-04, 3.7580e-04,\n",
      "        1.9351e-04, 4.7163e-04, 2.2771e-04, 2.2203e-04, 9.3047e-05, 3.3014e-04,\n",
      "        1.1337e-04, 1.8447e-04, 1.2600e-04, 3.6473e-05, 2.5455e-04, 1.9500e-04,\n",
      "        8.5255e-05, 1.0420e-04, 7.7051e-05, 5.9361e-05, 2.0215e-04, 1.5471e-04,\n",
      "        2.2752e-04, 2.4176e-04, 5.8692e-05, 3.0214e-04, 1.8058e-04, 3.3134e-05,\n",
      "        8.2601e-05, 3.5338e-04, 7.6262e-05, 1.7108e-04, 3.0162e-04, 1.5886e-04,\n",
      "        1.3875e-04, 5.7629e-04, 1.4574e-04, 1.2688e-05, 1.7913e-04, 3.5560e-04,\n",
      "        2.4879e-04, 4.5340e-04, 7.5998e-05, 2.6636e-04, 2.4026e-04, 1.0523e-04,\n",
      "        1.6101e-04, 6.8912e-05, 2.9421e-05, 2.4403e-04, 2.9662e-05, 3.5010e-05,\n",
      "        2.0770e-04, 1.4402e-04, 1.1551e-04, 1.5650e-04, 6.6152e-06, 4.9324e-04,\n",
      "        3.2189e-04, 1.0382e-04, 1.1910e-03, 9.9561e-04, 1.0775e-03, 1.4541e-03,\n",
      "        8.6363e-04, 1.2981e-03, 7.6055e-04, 9.1189e-04, 1.1694e-03, 1.4114e-03,\n",
      "        1.2894e-03, 1.1087e-03, 1.1704e-03, 9.7587e-04, 1.2218e-03, 8.8785e-04,\n",
      "        9.9458e-04, 9.2306e-04, 1.1509e-03, 1.0294e-03, 1.2020e-03, 1.4547e-03,\n",
      "        1.8642e-03, 6.5669e-04, 1.0356e-03, 6.6031e-04, 8.0768e-04, 8.9689e-04,\n",
      "        1.0078e-03, 7.8874e-04, 6.1441e-04, 6.9150e-04, 6.1131e-04, 5.9311e-04,\n",
      "        1.0087e-03, 7.1334e-04, 1.5072e-03, 6.5748e-04, 1.0777e-03, 6.2056e-04,\n",
      "        5.7320e-04, 6.2680e-04, 8.4644e-04, 7.5594e-04, 5.8858e-04, 1.5920e-03,\n",
      "        9.1613e-04, 6.8583e-04, 9.4264e-04, 1.2044e-03, 5.3329e-04, 5.8073e-04,\n",
      "        7.9788e-04, 1.4647e-03, 4.7624e-04, 8.4966e-04, 1.3682e-03, 1.0259e-03,\n",
      "        1.0328e-03, 5.6523e-04, 6.8347e-04, 7.9004e-04, 1.5628e-03, 7.9007e-04,\n",
      "        1.0653e-03, 1.4788e-03, 1.0140e-03, 1.7105e-03, 2.5475e-03, 2.8416e-03,\n",
      "        1.2119e-03, 2.0308e-03, 1.3349e-03, 1.3944e-03, 1.6710e-03, 1.4259e-03,\n",
      "        1.5414e-03, 6.1423e-04, 1.4385e-03, 1.4514e-03, 1.5872e-03, 6.1232e-04,\n",
      "        8.9537e-04, 7.3091e-04, 7.7489e-04, 7.4127e-04, 7.0234e-04, 1.0659e-03,\n",
      "        8.2403e-04, 1.5516e-03, 7.4539e-04, 8.1154e-04, 5.3834e-04, 1.3074e-03,\n",
      "        5.9891e-04, 5.7003e-04, 6.3610e-04, 4.8416e-04, 5.9389e-04, 9.6252e-04,\n",
      "        9.3502e-04, 8.3723e-04, 5.4803e-04, 7.1166e-04, 9.1584e-04, 7.2723e-04,\n",
      "        6.9355e-04, 7.5560e-04, 1.1353e-03, 6.7252e-04, 8.2469e-04, 1.0897e-03,\n",
      "        5.2776e-04, 6.5483e-04, 7.3752e-04, 7.3538e-04, 4.8124e-04, 6.6991e-04,\n",
      "        1.1543e-03, 8.3652e-04, 7.9583e-04, 8.4071e-04, 8.1965e-04, 7.8946e-04,\n",
      "        6.2600e-04, 8.4001e-04, 8.0710e-04, 8.5875e-04, 7.7670e-04, 6.3976e-04,\n",
      "        1.3001e-03, 1.7345e-03, 8.4178e-04, 1.6001e-03, 6.1917e-04, 1.4936e-03,\n",
      "        1.2485e-03, 1.4582e-03, 1.0358e-03, 1.0888e-03, 1.7014e-03, 1.6358e-03,\n",
      "        1.3883e-03, 1.4861e-03, 8.5321e-04, 7.4327e-04, 1.3753e-03, 7.1368e-04,\n",
      "        1.1600e-03, 1.1311e-03, 1.7380e-03, 7.9057e-04, 6.4447e-04, 5.7625e-04,\n",
      "        1.4519e-03, 1.5441e-03, 5.2064e-04, 5.7094e-04, 6.8939e-04, 5.9667e-04,\n",
      "        7.8138e-04, 6.3492e-04, 8.6302e-04, 5.1794e-04, 8.6188e-04, 5.4468e-04,\n",
      "        1.1305e-03, 1.1957e-03, 1.0188e-03, 7.6579e-04, 9.7759e-04, 5.9087e-04,\n",
      "        7.6211e-04, 7.7395e-04, 6.1602e-04, 5.8127e-04, 5.9683e-04, 6.3995e-04,\n",
      "        8.4196e-04, 9.2904e-04, 8.2075e-04, 5.6159e-04, 5.2060e-04, 7.0849e-04]) tensor(0.0028) tensor(0.2394)\n",
      "Chosen edges: tensor([[  0,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   5,   9,   9,   9,   9,\n",
      "          14,   1,   1,   1,   1,   3,   3,   3,   3,   3,   4,   4,   4,   5,\n",
      "           5,   5,   5,   5,   5,   6,   6,   6,   6,   6,   6,   7,   7,   7,\n",
      "           7,   7,   7,   7,   7,   7,   8,   8,   8,   8,   8,   8,  10,  10,\n",
      "          10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  10,  10,  10,  10,  10,  10,  10,  11,  11,  11,  11,\n",
      "          11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
      "          11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  12,  12,  13,  13,  13,  13,  13,\n",
      "          13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,\n",
      "          13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,\n",
      "          13,  13,  13,  13,  13,  13,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  14,  14,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0],\n",
      "        [ 33,   0,   3,   9,  10,  12,  13,  14,  15,  16,  19,  21,  25,  26,\n",
      "          31,  33,  35,  37,  39,  40,  56,  60,  63,  31,   5,  25,  39,  42,\n",
      "          33,  95, 110, 121, 122, 172, 175, 179, 191, 194, 217, 229, 244, 256,\n",
      "         262, 266, 267, 272, 282, 285, 289, 296, 298, 304, 305, 327, 328, 336,\n",
      "         339, 340, 341, 342, 345, 350, 371, 376, 379, 384, 390, 393, 426, 427,\n",
      "         435, 436, 437, 438, 440, 442, 443, 444, 446, 449, 450, 453, 454, 457,\n",
      "         458, 461, 463, 465, 466, 467, 468, 470, 471, 473, 474, 475, 476, 477,\n",
      "         478, 479, 480, 481, 482, 483, 484, 486, 488, 489, 490, 491, 493, 494,\n",
      "         495, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 511,\n",
      "         513, 514, 515, 516, 518, 522, 523, 524, 525, 526, 528, 529, 530, 531,\n",
      "         532, 533, 534, 535, 537, 539, 541, 542, 543, 544, 545, 546, 549, 550,\n",
      "         551, 552, 553, 554, 555, 556, 558, 559, 560, 561, 562, 564, 565, 566,\n",
      "         567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580,\n",
      "         581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
      "         595, 596, 597, 598, 599, 600, 601, 603, 605, 606, 607, 609, 610, 611,\n",
      "         612, 614, 615, 616, 618, 619, 620, 621, 622, 623, 625, 626, 627, 629,\n",
      "         630, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644,\n",
      "         645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
      "         659, 660]]) 254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Train Loss: 1.9254 | Val Loss: 24.1650 | Val Accuracy: 0.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 33.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Train Loss: 1.9211 | Val Loss: 24.1292 | Val Accuracy: 0.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Train Loss: 1.9185 | Val Loss: 24.1113 | Val Accuracy: 0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 32.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Train Loss: 1.9190 | Val Loss: 24.0630 | Val Accuracy: 0.3610\n",
      "len_choose=[960, 573, 492, 254]\n",
      "Edge metrics: tensor([3.5202e-05, 1.8988e-05, 1.9705e-05, 5.4259e-05, 2.1863e-05, 2.2319e-05,\n",
      "        1.1049e-05, 5.9474e-06, 2.4944e-05, 1.4071e-05, 2.6427e-05, 3.6592e-05,\n",
      "        2.9845e-05, 1.5145e-05, 8.2140e-06, 1.7069e-06, 4.1889e-05, 1.5311e-06,\n",
      "        4.2543e-05, 6.2619e-05, 4.0656e-06, 1.1148e-05, 2.7260e-05, 4.8411e-05,\n",
      "        2.0425e-05, 2.4496e-05, 6.8315e-05, 5.4139e-05, 4.8213e-05, 4.3046e-04,\n",
      "        3.3581e-04, 1.3572e-04, 7.8174e-05, 4.6122e-05, 2.9769e-04, 1.3139e-04,\n",
      "        3.6409e-04, 1.3823e-04, 1.1454e-04, 1.0027e-03, 2.4335e-05, 2.5773e-04,\n",
      "        1.1917e-06, 3.9268e-04, 6.2590e-05, 2.9611e-04, 4.3788e-04, 3.3254e-04,\n",
      "        2.9845e-04, 4.4599e-04, 4.8597e-04, 4.2575e-04, 5.6202e-04, 4.7380e-04,\n",
      "        3.1707e-04, 2.8251e-04, 1.7776e-04, 4.8225e-05, 2.9159e-04, 2.2779e-04,\n",
      "        6.2623e-04, 2.6993e-04, 2.9706e-04, 2.8598e-04, 1.4880e-04, 2.8858e-04,\n",
      "        2.2999e-04, 4.8771e-04, 4.4560e-04, 1.5225e-04, 3.1185e-04, 4.9713e-04,\n",
      "        4.7874e-04, 5.3569e-04, 2.6088e-04, 3.0902e-04, 1.5439e-06, 3.7636e-04,\n",
      "        5.9483e-04, 4.5104e-04, 6.6817e-04, 5.8338e-06, 2.5253e-04, 1.8766e-04,\n",
      "        3.8926e-05, 2.9319e-04, 6.9439e-04, 3.0224e-04, 3.2508e-04, 4.0133e-04,\n",
      "        2.9135e-04, 1.9576e-04, 3.3269e-04, 7.0049e-04, 1.8719e-04, 4.0036e-04,\n",
      "        2.4510e-04, 3.7829e-04, 6.5076e-05, 3.2081e-04, 6.6695e-05, 3.2874e-04,\n",
      "        4.1510e-05, 1.5845e-04, 1.5415e-04, 8.8279e-05, 6.0207e-05, 2.6989e-04,\n",
      "        1.0386e-04, 2.1909e-04, 2.4961e-04, 4.2275e-04, 1.2395e-04, 2.9891e-04,\n",
      "        2.2518e-05, 1.1137e-04, 1.9325e-04, 2.7703e-04, 1.2969e-04, 4.3078e-05,\n",
      "        6.1093e-05, 7.2757e-05, 1.5791e-04, 5.3284e-04, 1.0555e-04, 4.9497e-04,\n",
      "        1.4073e-04, 2.0178e-04, 3.1628e-04, 5.5680e-06, 2.9875e-05, 4.0071e-04,\n",
      "        3.2230e-04, 4.3636e-05, 1.1598e-04, 2.2107e-04, 2.7373e-04, 3.0650e-04,\n",
      "        3.3250e-05, 1.4286e-04, 2.8976e-04, 2.6805e-04, 5.3213e-04, 6.2729e-04,\n",
      "        2.8445e-04, 3.5562e-06, 5.4269e-04, 9.2169e-05, 4.7530e-04, 1.2740e-04,\n",
      "        4.3833e-04, 8.6245e-05, 5.4781e-05, 1.7803e-04, 1.3199e-04, 3.3070e-04,\n",
      "        5.5615e-04, 6.8698e-04, 2.3926e-04, 4.1028e-04, 3.1628e-05, 2.2827e-04,\n",
      "        4.1988e-04, 7.0826e-06, 5.8702e-05, 4.7970e-04, 4.8812e-04, 1.8134e-04,\n",
      "        2.0071e-04, 1.3244e-04, 3.5033e-04, 1.2803e-04, 5.0209e-04, 4.4218e-04,\n",
      "        4.9155e-04, 5.0965e-04, 2.0276e-04, 1.6940e-04, 1.0079e-04, 1.1647e-04,\n",
      "        5.4290e-04, 4.4962e-05, 2.5907e-05, 5.0896e-06, 3.7291e-04, 3.0110e-04,\n",
      "        6.8143e-05, 3.8840e-05, 1.4000e-05, 2.5052e-05, 9.4014e-05, 1.5437e-05,\n",
      "        1.6172e-04, 4.5218e-04, 2.1182e-04, 1.6869e-04, 2.8301e-04, 3.1286e-04,\n",
      "        1.5391e-04, 2.8008e-04, 7.8713e-05, 7.4921e-05, 2.2894e-04, 2.5343e-05,\n",
      "        4.8871e-04, 4.7948e-04, 3.0046e-04, 2.7217e-04, 1.6632e-04, 1.8170e-04,\n",
      "        2.8108e-04, 3.3883e-04, 2.1377e-04, 7.4704e-05, 4.2125e-04, 2.0543e-04,\n",
      "        2.9203e-04, 2.7812e-05, 3.4331e-04, 3.4028e-04, 5.7070e-05, 3.9392e-04,\n",
      "        2.9318e-04, 9.1750e-05, 3.0800e-04, 3.6689e-04, 6.2126e-05, 2.4178e-04,\n",
      "        1.9438e-04, 2.7150e-04, 4.6222e-04, 4.1520e-05, 2.5205e-04, 6.2940e-04,\n",
      "        1.4622e-04, 2.0328e-04, 3.4018e-04, 1.0032e-04, 6.2589e-05, 5.3609e-05,\n",
      "        1.9570e-04, 1.3655e-04, 5.2947e-04, 4.5949e-04, 8.3333e-05, 5.8008e-04,\n",
      "        5.1959e-04, 8.2567e-04, 7.5643e-04, 3.6879e-04, 1.9329e-04, 7.5801e-04,\n",
      "        1.0639e-05, 7.5782e-04]) tensor(0.0010) tensor(0.0611)\n",
      "Chosen edges: tensor([[  2,   2,   2,   2,   2,   2,   2,   5,   5,   9,   9,   9,   9,   9,\n",
      "          11,  13,   0,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   3,   3,   3,   3,   3,   3,   3,\n",
      "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   7,\n",
      "           7,   7,   7,   7,   7,   7,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   9,   9,\n",
      "           9,   9,   9,   9,   9,  10,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   1],\n",
      "        [ 51,  52,  54,  58,  59,  61,  62,   5,  33,   8,  29,  34,  45,  54,\n",
      "          33,  33,  75,  84,  88,  89,  92,  93,  94,  97,  99, 103, 104, 105,\n",
      "         106, 107, 108, 109, 112, 113, 115, 116, 117, 119, 123, 124, 125, 128,\n",
      "         129, 131, 132, 135, 136, 142, 143, 146, 147, 149, 150, 153, 154, 157,\n",
      "         161, 162, 164, 166, 168, 177, 180, 181, 185, 187, 188, 193, 195, 196,\n",
      "         197, 204, 205, 206, 207, 215, 216, 219, 221, 222, 223, 224, 228, 232,\n",
      "         236, 237, 238, 240, 245, 246, 247, 250, 251, 252, 257, 261, 263, 265,\n",
      "         275, 276, 277, 278, 283, 284, 287, 290, 292, 297, 299, 301, 303, 307,\n",
      "         308, 309, 311, 313, 314, 315, 316, 317, 321, 323, 329, 330, 343, 346,\n",
      "         347, 349, 351, 352, 354, 356, 360, 363, 365, 366, 367, 368, 369, 370,\n",
      "         372, 374, 377, 378, 380, 385, 386, 391, 396, 398, 401, 407, 410, 411,\n",
      "         412, 414, 415, 417, 418, 419, 663, 664, 665, 666, 668, 669, 670, 671,\n",
      "         672, 673, 674, 676]]) 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 27.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Train Loss: 1.9145 | Val Loss: 24.0558 | Val Accuracy: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 26.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Train Loss: 1.9139 | Val Loss: 24.0232 | Val Accuracy: 0.3617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Train Loss: 1.9127 | Val Loss: 24.0095 | Val Accuracy: 0.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Train Loss: 1.9012 | Val Loss: 23.9817 | Val Accuracy: 0.3622\n",
      "len_choose=[960, 573, 492, 254, 172]\n",
      "Edge metrics: tensor([8.1262e-06, 2.8315e-05, 2.1491e-05, 1.6671e-05, 1.7046e-05, 2.5122e-06,\n",
      "        6.9686e-05, 1.1763e-05, 9.3561e-05, 3.6368e-05, 3.1598e-05, 2.6555e-05,\n",
      "        2.9267e-05, 1.5825e-05, 5.3743e-05, 2.0520e-05, 7.9603e-04, 6.7357e-04,\n",
      "        7.9789e-06, 3.7684e-04, 8.9009e-05, 2.9353e-04, 3.1473e-04, 1.0793e-04,\n",
      "        5.1424e-04, 1.0671e-04, 3.4701e-05, 2.0327e-04, 2.5970e-04, 6.9046e-04,\n",
      "        2.1213e-04, 5.1545e-04, 5.7040e-04, 4.6568e-05, 1.1315e-04, 9.6732e-06,\n",
      "        2.3095e-04, 2.4659e-04, 6.2982e-04, 1.7477e-04, 1.0817e-04, 4.5354e-05,\n",
      "        1.3796e-04, 3.9017e-04, 1.3014e-04, 1.5643e-04, 2.2822e-04, 3.6804e-05,\n",
      "        3.1684e-04, 1.2461e-04, 3.8797e-04, 3.4604e-04, 1.5744e-04, 1.0321e-04,\n",
      "        2.9406e-04, 3.9326e-04, 2.4556e-04, 1.2708e-04, 2.7000e-04, 3.3840e-04,\n",
      "        6.1842e-05, 7.4012e-05, 6.0326e-05, 8.3828e-05, 1.9508e-04, 3.2770e-04,\n",
      "        4.5798e-04, 5.8215e-05, 2.7220e-04, 2.3411e-05, 3.3000e-04, 2.4879e-04,\n",
      "        4.3021e-05, 4.5688e-04, 2.9629e-06, 3.2292e-04, 7.6972e-04, 3.8376e-04,\n",
      "        2.2665e-04, 3.2708e-04, 4.9378e-04, 4.6025e-04, 5.5297e-04, 2.3698e-04,\n",
      "        2.2463e-04, 3.4365e-04, 1.0547e-04, 3.7659e-04, 5.3714e-04, 6.3659e-04,\n",
      "        4.1797e-04, 2.8684e-04, 5.3390e-05, 1.8854e-04, 3.9056e-04, 1.3076e-04,\n",
      "        7.8792e-05, 2.3762e-04, 8.4193e-05, 3.5117e-04, 1.1645e-04, 2.6395e-04,\n",
      "        5.5460e-04, 9.6924e-05, 1.3431e-04, 1.7986e-04, 1.3970e-04, 4.8908e-04,\n",
      "        3.0887e-04, 2.0301e-04, 4.3637e-04, 1.8381e-06, 5.1501e-04, 9.8738e-06,\n",
      "        7.3409e-04, 4.0528e-04, 3.7092e-04, 5.4194e-04, 1.9471e-04, 4.4378e-04,\n",
      "        3.2843e-04, 2.2305e-05, 2.9730e-04, 4.2192e-04, 7.1949e-04, 7.0376e-04,\n",
      "        4.3824e-05, 4.6520e-04, 3.9765e-04, 4.4769e-05, 3.5079e-04, 3.0190e-04,\n",
      "        1.9626e-05, 2.9211e-04, 1.5572e-04, 3.6676e-04, 2.5188e-04, 3.9400e-04,\n",
      "        2.5105e-04, 4.4634e-04, 7.4103e-04, 1.1867e-04, 2.4804e-04, 3.7246e-05,\n",
      "        4.1357e-04, 1.2810e-04, 2.5862e-04, 2.7094e-04, 2.2070e-04, 4.7263e-04,\n",
      "        1.5789e-04, 6.3362e-05, 2.6258e-04, 2.7804e-04, 4.1612e-05, 2.6758e-04,\n",
      "        2.1643e-04, 2.1122e-04, 5.2871e-04, 1.0931e-04, 5.9052e-04, 2.1975e-04,\n",
      "        1.0160e-04, 1.6625e-04, 5.7435e-05, 3.5725e-04, 9.6643e-07, 3.6162e-04,\n",
      "        7.5223e-05, 6.5984e-04, 3.3589e-04, 5.4240e-04]) tensor(0.0008) tensor(0.0439)\n",
      "Chosen edges: tensor([[  2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "           9,   9,   1,   1,   1,   3,   3,   3,   3,   4,   4,   4,   4,   4,\n",
      "           4,   4,   5,   5,   5,   5,   6,   6,   6,   7,   7,   7,   7,   7,\n",
      "           7,   7,   8,   8,   8,   8,   9,  10,   0,   0,   0,   1,   1,   1,\n",
      "           1,   2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   3,   3,   3,\n",
      "           3,   3,   3,   3,   4,   4,   4,   4,   4,   4,   4,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,   6,\n",
      "           6,   6,   6,   7],\n",
      "        [  8,  29,  30,  34,  38,  42,  43,  45,  46,  47,  49,  50,  55,  57,\n",
      "           1,  23,  96, 134, 137, 167, 174, 178, 183, 202, 210, 211, 214, 226,\n",
      "         233, 241, 255, 259, 271, 273, 288, 294, 295, 319, 324, 332, 333, 335,\n",
      "         337, 357, 362, 375, 389, 402, 413, 428, 661, 662, 667, 675, 677, 678,\n",
      "         679, 680, 681, 682, 683, 684, 685, 686, 688, 689, 690, 692, 693, 694,\n",
      "         695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 707, 709, 710,\n",
      "         711, 712, 713, 714, 715, 717, 718, 719, 720, 722, 723, 725, 726, 728,\n",
      "         729, 730, 731, 732, 733, 734, 735, 736, 737, 739, 740, 741, 742, 743,\n",
      "         744, 745, 747, 748, 750, 751, 752, 753, 754, 755, 756, 757, 758, 760,\n",
      "         762, 764, 765, 766]]) 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Train Loss: 1.9013 | Val Loss: 23.9484 | Val Accuracy: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Train Loss: 1.9016 | Val Loss: 23.9463 | Val Accuracy: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 25.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Train Loss: 1.9000 | Val Loss: 23.9215 | Val Accuracy: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Train Loss: 1.8963 | Val Loss: 23.9120 | Val Accuracy: 0.3610\n",
      "len_choose=[960, 573, 492, 254, 172, 130]\n",
      "Edge metrics: tensor([3.9797e-05, 2.8566e-05, 2.7879e-05, 3.3300e-06, 5.7056e-05, 1.0893e-06,\n",
      "        3.8847e-06, 8.1489e-06, 1.3748e-05, 1.3799e-05, 1.1180e-05, 1.5015e-05,\n",
      "        2.0279e-05, 8.2522e-06, 3.1412e-05, 1.0760e-04, 3.5492e-04, 1.1105e-04,\n",
      "        3.2154e-04, 5.3515e-04, 3.4399e-04, 2.1518e-04, 3.7800e-04, 1.3171e-05,\n",
      "        1.4289e-04, 4.5831e-04, 4.1681e-04, 2.1192e-04, 2.4135e-04, 2.8559e-04,\n",
      "        1.8219e-04, 2.4393e-04, 1.2966e-04, 4.8963e-04, 1.0749e-04, 4.8713e-06,\n",
      "        4.1489e-04, 6.7886e-04, 2.8827e-04, 1.9125e-04, 3.5584e-04, 4.0172e-04,\n",
      "        5.3646e-04, 3.8515e-04, 3.6063e-04, 3.2403e-04, 4.3158e-04, 2.9416e-04,\n",
      "        1.1259e-04, 3.1940e-04, 1.3570e-04, 2.4828e-04, 1.5511e-04, 6.5815e-05,\n",
      "        2.9243e-04, 2.2917e-04, 8.3308e-05, 5.5598e-05, 4.7313e-05, 4.5838e-05,\n",
      "        4.0261e-05, 5.1936e-05, 3.4267e-05, 1.7584e-04, 3.0727e-05, 1.8076e-04,\n",
      "        5.0958e-04, 2.5370e-04, 9.7358e-05, 3.9072e-04, 1.8805e-04, 1.1604e-04,\n",
      "        3.5673e-04, 6.6259e-05, 3.6568e-04, 9.1936e-05, 1.0463e-04, 4.7153e-04,\n",
      "        1.7431e-04, 2.6899e-04, 3.7962e-04, 2.8702e-04, 3.1634e-04, 3.7691e-04,\n",
      "        1.2626e-04, 1.3706e-04, 1.4994e-04, 1.2417e-04, 3.1429e-04, 1.0106e-05,\n",
      "        3.5543e-05, 2.6076e-04, 1.3852e-04, 9.2605e-05, 3.6352e-04, 3.6212e-05,\n",
      "        2.3752e-04, 5.4613e-05, 1.7239e-05, 1.3459e-04, 6.5266e-05, 2.6184e-06,\n",
      "        1.6603e-04, 3.4951e-04, 3.1950e-06, 2.2892e-05, 5.3427e-05, 1.0443e-04,\n",
      "        5.7926e-05, 2.7336e-04, 3.4312e-04, 1.6903e-04, 4.1335e-04, 7.5139e-05,\n",
      "        7.4716e-05, 1.3644e-04, 1.2777e-04, 2.3300e-04, 1.2241e-04, 2.5023e-04,\n",
      "        1.3340e-04, 1.0556e-04, 1.2358e-04, 1.9940e-04, 1.6084e-04, 1.9850e-06,\n",
      "        1.4900e-04, 1.0070e-04, 6.3465e-05, 1.0374e-04]) tensor(0.0007) tensor(0.0238)\n",
      "Chosen edges: tensor([[  2,   2,   9,   1,   3,   4,   7,   7,   7,   7,   8,   8,   3,   3,\n",
      "           5,   5,   5,   5,   5,   6,   6,   6,   6,   6,   6,   7,   7,   7,\n",
      "           7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "          10,  10,  10,  10,  10,  10,  10],\n",
      "        [ 32,  48,  32, 127, 182, 225, 325, 326, 338, 358, 381, 397, 687, 691,\n",
      "         706, 708, 716, 721, 724, 738, 746, 749, 759, 761, 763, 767, 768, 769,\n",
      "         770, 771, 772, 773, 774, 775, 776, 777, 779, 780, 781, 788, 790, 791,\n",
      "         792, 793, 794, 795, 796, 797, 799, 800, 801, 802, 803, 804, 805, 806,\n",
      "         807, 808, 809, 810, 811, 812, 813, 816, 817, 818, 819, 821, 824, 827,\n",
      "         828, 832, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845,\n",
      "         846, 847, 848, 849, 851, 852, 854]]) 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 22.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Train Loss: 1.9002 | Val Loss: 23.8958 | Val Accuracy: 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 23.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Train Loss: 1.8988 | Val Loss: 23.8610 | Val Accuracy: 0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Train Loss: 1.8958 | Val Loss: 23.8485 | Val Accuracy: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Train Loss: 1.8963 | Val Loss: 23.8319 | Val Accuracy: 0.3639\n",
      "len_choose=[960, 573, 492, 254, 172, 130, 91]\n",
      "Edge metrics: tensor([2.4528e-05, 7.3366e-05, 2.2363e-05, 3.6314e-04, 4.0503e-04, 3.3347e-05,\n",
      "        6.5185e-04, 2.2891e-04, 4.9379e-04, 3.5358e-04, 1.0135e-04, 4.2260e-04,\n",
      "        3.1771e-04, 6.6659e-05, 9.9541e-05, 1.6510e-04, 1.4879e-04, 3.1943e-04,\n",
      "        2.9132e-05, 9.7911e-05, 1.8060e-05, 1.9220e-04, 1.8362e-04, 1.2907e-04,\n",
      "        2.4284e-04, 2.4091e-04, 1.0242e-04, 1.0683e-04, 4.4172e-04, 2.8196e-05,\n",
      "        5.1172e-04, 1.8746e-04, 3.5523e-04, 3.8298e-04, 2.3339e-04, 1.9564e-04,\n",
      "        1.6549e-04, 2.2837e-04, 1.0758e-04, 2.2576e-04, 2.8462e-04, 2.2205e-04,\n",
      "        1.8238e-04, 2.8244e-04, 5.1833e-05, 9.0645e-05, 2.3929e-04, 1.9428e-05,\n",
      "        6.0477e-04, 1.3928e-04, 9.0915e-05, 1.0643e-04, 4.8715e-05, 3.3526e-04,\n",
      "        9.5433e-05, 1.4327e-04, 1.5488e-05, 3.7020e-04, 2.9114e-04, 3.4505e-05,\n",
      "        1.3755e-04, 1.1081e-04, 3.6101e-05, 5.0242e-04, 1.2944e-04, 5.1656e-05,\n",
      "        2.7497e-04, 2.3917e-04, 3.2219e-04, 7.7271e-05, 4.9964e-04, 3.2802e-04,\n",
      "        3.5652e-04, 1.5348e-04, 5.3091e-05, 1.6139e-04, 6.7959e-05, 2.3146e-04,\n",
      "        4.0982e-04, 5.5669e-05, 3.4212e-05, 4.2616e-04, 1.1811e-04, 2.2080e-04,\n",
      "        5.0900e-04, 4.6034e-06, 1.1108e-04, 1.4035e-04, 3.6200e-04, 1.0816e-05,\n",
      "        3.0755e-04]) tensor(0.0007) tensor(0.0188)\n",
      "Chosen edges: tensor([[  0,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   7,   5,   7,\n",
      "           7,   7,   7,   7,   7,   8,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,  10,  10,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,\n",
      "          11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  11,  12,\n",
      "          12,  12,  12,  12,  12,  12,  12,  13,  13,  13,  13,  13,  13,  13,\n",
      "          13],\n",
      "        [ 61,   2,   4,   6,   7,  11,  18,  20,  22,  24,  28, 331, 727, 778,\n",
      "         783, 785, 786, 787, 789, 798, 814, 815, 820, 823, 825, 826, 829, 830,\n",
      "         831, 833, 850, 853, 855, 856, 857, 858, 859, 861, 862, 864, 865, 866,\n",
      "         867, 869, 870, 871, 873, 874, 876, 877, 879, 880, 882, 883, 884, 885,\n",
      "         886, 887, 888, 889, 891, 893, 894, 897, 898, 899, 900, 902, 903, 904,\n",
      "         906]]) 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Train Loss: 1.8928 | Val Loss: 23.8147 | Val Accuracy: 0.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Train Loss: 1.8904 | Val Loss: 23.8124 | Val Accuracy: 0.3620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 21.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Train Loss: 1.8857 | Val Loss: 23.7843 | Val Accuracy: 0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Train Loss: 1.8881 | Val Loss: 23.7588 | Val Accuracy: 0.3648\n",
      "len_choose=[960, 573, 492, 254, 172, 130, 91, 71]\n",
      "Edge metrics: tensor([8.3767e-07, 1.2444e-06, 2.7235e-06, 1.0347e-05, 3.0582e-05, 2.4053e-05,\n",
      "        6.2721e-05, 4.9817e-05, 1.1440e-06, 2.4261e-05, 9.2509e-06, 5.3413e-04,\n",
      "        2.6440e-04, 2.1178e-05, 6.1147e-04, 6.0032e-04, 2.9703e-06, 2.1882e-05,\n",
      "        2.0254e-04, 2.2847e-04, 2.5589e-04, 2.7017e-04, 2.6539e-05, 1.2835e-04,\n",
      "        7.4121e-05, 1.3332e-04, 1.7110e-04, 2.7528e-05, 3.6659e-04, 1.2847e-04,\n",
      "        1.8205e-04, 4.0646e-04, 1.4833e-04, 1.6575e-04, 2.5908e-04, 2.7392e-04,\n",
      "        1.3091e-04, 2.1101e-04, 1.6908e-04, 1.3452e-04, 8.1059e-05, 1.7575e-04,\n",
      "        2.1377e-04, 9.1732e-05, 1.0424e-04, 3.3080e-05, 8.4487e-05, 2.5308e-04,\n",
      "        1.2060e-04, 4.7842e-05, 3.0635e-04, 1.1433e-04, 2.4734e-04, 3.8089e-05,\n",
      "        1.8389e-04, 3.7674e-04, 8.1958e-04, 5.3921e-04, 4.6457e-04, 4.1915e-04,\n",
      "        2.5521e-04, 2.0080e-04, 4.8041e-04, 1.4602e-04, 1.9846e-04, 1.8439e-04,\n",
      "        1.1798e-04, 3.1612e-05, 4.0068e-05, 7.8095e-05, 2.0157e-04]) tensor(0.0008) tensor(0.0130)\n",
      "Chosen edges: tensor([[ 11,  11,  12,  12,  13,  13,  13,  13,  13,  13,  13,  13,  13,  13,\n",
      "          14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  14,  14,  14,  14,  14,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0],\n",
      "        [875, 878, 890, 892, 901, 905, 907, 908, 910, 912, 913, 915, 916, 917,\n",
      "         918, 919, 920, 921, 922, 923, 924, 925, 926, 928, 929, 930, 931, 933,\n",
      "         934, 935, 937, 938, 939, 941, 942, 943, 944, 945, 946, 947, 948, 949,\n",
      "         950, 951, 952, 953, 957]]) 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Train Loss: 1.8850 | Val Loss: 23.7749 | Val Accuracy: 0.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:02<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Train Loss: 1.8852 | Val Loss: 23.7490 | Val Accuracy: 0.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Train Loss: 1.8835 | Val Loss: 23.7424 | Val Accuracy: 0.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Train Loss: 1.8855 | Val Loss: 23.7269 | Val Accuracy: 0.3653\n",
      "len_choose=[960, 573, 492, 254, 172, 130, 91, 71, 47]\n",
      "Edge metrics: tensor([4.1471e-04, 2.3654e-04, 5.9868e-04, 2.8385e-04, 1.5821e-04, 1.3481e-04,\n",
      "        1.5294e-04, 1.7606e-04, 2.8654e-04, 1.7089e-04, 7.0817e-05, 2.1656e-04,\n",
      "        2.5183e-04, 3.5737e-04, 2.4082e-04, 3.2315e-04, 5.2666e-04, 2.2521e-04,\n",
      "        5.5840e-05, 2.0446e-04, 3.8620e-04, 3.1759e-04, 4.8136e-04, 2.0723e-04,\n",
      "        2.4007e-04, 9.8674e-06, 1.4436e-04, 4.2843e-05, 3.3603e-04, 3.4078e-05,\n",
      "        1.7654e-04, 1.9581e-04, 1.5859e-04, 9.3508e-05, 4.2653e-04, 2.6137e-04,\n",
      "        1.6416e-04, 6.9957e-04, 5.4855e-04, 7.1893e-04, 8.2512e-04, 4.7635e-04,\n",
      "        1.2465e-03, 8.8398e-04, 6.8250e-04, 7.4502e-04, 6.9958e-04]) tensor(0.0012) tensor(0.0163)\n",
      "Chosen edges: tensor([[  0,   2,   2,   2,   7,   7,   9,  11,  11,  11,  11,  13,  13,  13,\n",
      "          13,  13,  14,  14,  14,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
      "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [ 37,   1,   5,  27, 782, 784, 822, 860, 863, 868, 881, 895, 896, 909,\n",
      "         911, 914, 927, 936, 940, 954, 955, 956, 958, 960, 962, 964, 965, 966,\n",
      "         968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980]]) 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Train Loss: 1.8805 | Val Loss: 23.7154 | Val Accuracy: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Train Loss: 1.8847 | Val Loss: 23.7137 | Val Accuracy: 0.3642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Train Loss: 1.8795 | Val Loss: 23.6877 | Val Accuracy: 0.3651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:03<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Train Loss: 1.8768 | Val Loss: 23.6777 | Val Accuracy: 0.3657\n",
      "len_choose=[960, 573, 492, 254, 172, 130, 91, 71, 47, 41]\n",
      "Edge metrics: tensor([1.0816e-05, 8.4392e-07, 9.9055e-06, 5.6326e-05, 1.4416e-04, 3.4392e-04,\n",
      "        4.6316e-04, 1.7039e-04, 5.3279e-05, 1.4673e-04, 1.4021e-04, 1.5424e-05,\n",
      "        4.0265e-06, 4.6402e-05, 3.2194e-04, 3.7175e-04, 1.1834e-04, 1.4843e-04,\n",
      "        2.5698e-04, 9.1976e-04, 8.7215e-04, 6.6625e-04, 7.4320e-04, 1.2239e-03,\n",
      "        7.6747e-04, 6.5541e-04, 6.8785e-04, 1.7751e-03, 7.7348e-04, 1.2019e-03,\n",
      "        6.8733e-04, 1.3324e-03, 1.3566e-03, 1.1247e-03, 5.9247e-04, 6.8824e-04,\n",
      "        7.0524e-04, 6.4758e-04, 5.5691e-04, 1.0893e-03, 7.7061e-04]) tensor(0.0018) tensor(0.0227)\n",
      "Chosen edges: tensor([[   1,    1,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4],\n",
      "        [ 967,  981,  989,  990,  993,  994,  995,  996,  997,  998,  999, 1000,\n",
      "         1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
      "         1013, 1014, 1015]]) 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>len</td><td>██▄▃▂▂▁▁▁▁</td></tr><tr><td>len_choose</td><td>█▇▄▃▂▂▂▁▁▁</td></tr><tr><td>max</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>sum</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>len</td><td>41</td></tr><tr><td>len_choose</td><td>27</td></tr><tr><td>max</td><td>0.00178</td></tr><tr><td>sum</td><td>0.02266</td></tr><tr><td>train_loss</td><td>1.87684</td></tr><tr><td>val_accuracy</td><td>0.36573</td></tr><tr><td>val_loss</td><td>23.67769</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">titanic-mul, num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2</strong> at: <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/ayc0578c' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/ayc0578c</a><br/> View project at: <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250217_204936-ayc0578c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense_model = MulticlassFCN(input_size=X.shape[1])\n",
    "sparse_model = convert_dense_to_sparse_network(dense_model)\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project=\"self-expanding-nets\",\n",
    "    name=f\"titanic-mul, {name}\",\n",
    "    tags=[\"complex model\", \"titanic\", \"multiclass\", hyperparams[\"metric\"].__class__.__name__],\n",
    "    group=\"new activation\"\n",
    ")\n",
    "\n",
    "train_sparse_recursive(sparse_model, train_loader, val_loader,\n",
    "                       edge_replacement_func=edge_replacement_func_new_layer, **hyperparams)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bf1b57447aa62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:06:17.697345400Z",
     "start_time": "2025-02-11T11:19:18.201889Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afbc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
