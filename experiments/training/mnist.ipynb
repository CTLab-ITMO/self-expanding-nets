{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932f4bdded4b6539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:25.334745Z",
     "start_time": "2025-01-21T14:31:20.096324Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a089a23845b1066b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:25.350457Z",
     "start_time": "2025-01-21T14:31:25.338930Z"
    }
   },
   "outputs": [],
   "source": [
    "from senmodel.model.utils import *\n",
    "from senmodel.metrics.nonlinearity_metrics import *\n",
    "from senmodel.metrics.edge_finder import *\n",
    "from senmodel.metrics.train_metrics import *\n",
    "from senmodel.train.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888ee10c0aa4d1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:25.520919Z",
     "start_time": "2025-01-21T14:31:25.513942Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aff98b408abbf8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:25.547124Z",
     "start_time": "2025-01-21T14:31:25.543666Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self, input_size=28 * 28, hidden_size=16):\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.fc0 = nn.Linear(input_size, 10)\n",
    "        # self.fc1 = nn.Linear(hidden_size, 10)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90353a5a49cd1022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:25.590348Z",
     "start_time": "2025-01-21T14:31:25.554956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and Dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "# Load dataset and split into train/validation sets\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb8129e6befdf70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:26.821483Z",
     "start_time": "2025-01-21T14:31:26.807650Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleFCN()\n",
    "sparse_model = convert_dense_to_sparse_network(model, layers=[model.fc0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33ccfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"num_epochs: 64, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_thresholds: {'fc0': 0.7}, replace_layers: ['fc0'], threshold: 0.1, min_delta_epoch_replace: 1, window_size: 1, lr: 0.0001, delete_after: 1\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"num_epochs\": 64,\n",
    "    \"metric\": AbsGradientEdgeMetric(nn.CrossEntropyLoss()),\n",
    "    \"aggregation_mode\": \"mean\",\n",
    "    \"choose_thresholds\": {\"fc0\": 0.7},\n",
    "    \"replace_layers\": [\"fc0\"],\n",
    "    \"threshold\": 0.1,\n",
    "    \"min_delta_epoch_replace\": 1,\n",
    "    \"window_size\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"delete_after\": 1,    \n",
    "}\n",
    "\n",
    "name = \", \".join(\n",
    "    f\"{key}: {value.__class__.__name__ if key == 'metric' else value}\"\n",
    "    for key, value in hyperparams.items()\n",
    ")\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e8399a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:31:30.645772Z",
     "start_time": "2025-01-21T14:31:27.560914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfedornigretuk\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4cee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Coding\\PY\\self-expanding-nets\\experiments\\training\\wandb\\run-20250331_170944-50lrz6ov</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/50lrz6ov' target=\"_blank\">MNIST: num_epochs: 64, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_thresholds: {'fc0': 0.7}, replace_layers: ['fc0'], threshold: 0.1, min_delta_epoch_replace: 1, window_size: 1, lr: 0.0001, delete_after: 1</a></strong> to <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/50lrz6ov' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/50lrz6ov</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"self-expanding-nets\",\n",
    "    name=f\"MNIST: {name}\",\n",
    "    config=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3534e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:09<00:00, 79.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64, Train Loss: 1.4805, Val Loss: 0.9977, Val Accuracy: 0.8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:08<00:00, 83.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/64, Train Loss: 0.8088, Val Loss: 0.6843, Val Accuracy: 0.8528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:08<00:00, 84.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/64, Train Loss: 0.6078, Val Loss: 0.5551, Val Accuracy: 0.8699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:09<00:00, 82.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/64, Train Loss: 0.5129, Val Loss: 0.4851, Val Accuracy: 0.8792\n",
      "Chosen edges: tensor([[  1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   2,   3,   3,\n",
      "           3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   4,   4,   4,   4,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "           5,   5,   7,   7,   7,   7,   7,   7,   7,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
      "           8,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "           9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9],\n",
      "        [323, 350, 351, 378, 379, 406, 407, 434, 462, 379, 407, 434, 182, 183,\n",
      "         184, 208, 351, 352, 378, 379, 380, 406, 407, 408, 374, 380, 381, 401,\n",
      "         402, 407, 408, 409, 434, 435, 436, 463, 464, 181, 182, 183, 184, 185,\n",
      "         207, 208, 209, 211, 212, 235, 264, 265, 266, 267, 291, 292, 293, 294,\n",
      "         318, 319, 320, 321, 322, 346, 347, 348, 349, 350, 374, 375, 376, 377,\n",
      "         378, 379, 402, 403, 404, 405, 406, 407, 408, 431, 432, 576, 577, 599,\n",
      "         600, 601, 602, 603, 604, 605, 626, 627, 628, 629, 630, 631, 632, 656,\n",
      "         657, 658, 240, 241, 268, 269, 435, 436, 463, 155, 181, 182, 183, 184,\n",
      "         185, 207, 208, 209, 210, 211, 212, 213, 214, 234, 235, 236, 237, 241,\n",
      "         242, 262, 263, 269, 270, 296, 297, 298, 323, 324, 325, 326, 349, 350,\n",
      "         351, 352, 353, 375, 376, 377, 378, 379, 380, 381, 402, 403, 404, 405,\n",
      "         406, 407, 408, 430, 431, 432, 433, 434, 435, 436, 459, 460, 461, 462,\n",
      "         463, 464, 487, 488, 489, 490, 627, 628, 629, 630, 631, 632, 656, 657,\n",
      "         658, 209, 210, 211, 212, 213, 235, 236, 237, 238, 239, 240, 241, 242,\n",
      "         263, 264, 265, 268, 269, 270, 290, 291, 297, 298, 318, 324, 325, 326,\n",
      "         346, 352, 353, 354, 374, 379, 380, 381, 382, 407, 408, 409, 434, 435,\n",
      "         436, 437, 462, 463, 464, 465, 490, 491, 492, 518, 519, 520, 547, 548]]) 238\n",
      "238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:19<00:00, 38.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/64, Train Loss: 0.4246, Val Loss: 0.3889, Val Accuracy: 0.8938\n",
      "torch.Size([29036]) torch.Size([9982])\n",
      "tensor(69)\n",
      "tensor(0)\n",
      "tensor(0) tensor(0)\n",
      "Chosen edges to del emb: tensor([], size=(2, 0), dtype=torch.int32) 0\n",
      "Chosen edges to del: tensor([], size=(2, 0), dtype=torch.int64) 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 318/750 [00:09<00:13, 32.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_sparse_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\train\\train.py:103\u001b[39m, in \u001b[36mtrain_sparse_recursive\u001b[39m\u001b[34m(model, train_loader, val_loader, test_loader, hyperparams)\u001b[39m\n\u001b[32m    101\u001b[39m val_losses = []\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hyperparams[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     train_loss, train_time = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     val_loss, val_accuracy = eval_one_epoch(model, criterion, test_loader)\n\u001b[32m    105\u001b[39m     val_losses.append(val_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Coding\\PY\\self-expanding-nets\\senmodel\\train\\train.py:15\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, train_loader)\u001b[39m\n\u001b[32m     13\u001b[39m model.train()\n\u001b[32m     14\u001b[39m train_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding\\PY\\self-expanding-nets\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_sparse_recursive(sparse_model, train_loader, val_loader, val_loader, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247c3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-expanding-nets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
