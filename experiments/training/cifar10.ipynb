{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senmodel.model.utils import convert_dense_to_sparse_network, get_model_last_layer\n",
    "from senmodel.metrics.edge_finder import EdgeFinder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "782\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse',\n",
    "           'ship', 'truck')\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                  download=True, transform=transform)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# train_size = int(0.8 * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "# # train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "# train_dataset, val_dataset, test_dataset = random_split(train_dataset, [train_size // 2, val_size // 2, len(train_dataset) - (train_size // 2 + val_size // 2)])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandingHead(nn.Module):\n",
    "    def __init__(self, input_size: int = 64, hidden_size: int = 50, output_size: int = 10):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetExp(nn.Module):\n",
    "    def __init__(self, freeze_base: bool = False):\n",
    "        super().__init__()\n",
    "        self.base_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\",\n",
    "                                         \"cifar10_resnet20\", pretrained=True)\n",
    "        self.base_model = torch.nn.Sequential(\n",
    "            *(list(self.base_model.children())[:-1])\n",
    "        )\n",
    "        self.expanding_head = convert_dense_to_sparse_network(\n",
    "            ExpandingHead(input_size=64, hidden_size=50, output_size=10)\n",
    "        ).to(device)\n",
    "        # if freeze_base:\n",
    "        #     self.freeze(self.base_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.expanding_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fedor/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2109, -0.1162, -0.1401, -0.0713,  0.1549, -0.1437,  0.2410,  0.0094,\n",
       "         -0.0911,  0.0005]], grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rexp = ResnetExp()\n",
    "rexp = rexp.to(device)\n",
    "img = val_dataset[0][0].unsqueeze(0).to(device)\n",
    "rexp(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_replacement_func_new_layer(model, optim, val_loader, metric, choose_threshold, aggregation_mode='mean', len_choose=None):\n",
    "    layer = get_model_last_layer(model)\n",
    "    ef = EdgeFinder(metric, val_loader, device, aggregation_mode)\n",
    "    vals = ef.calculate_edge_metric_for_dataloader(model, len_choose, False)\n",
    "    print(\"Edge metrics:\", vals, max(vals, default=0), sum(vals))\n",
    "    chosen_edges = ef.choose_edges_threshold(model, choose_threshold, len_choose)\n",
    "    print(\"Chosen edges:\", chosen_edges, len(chosen_edges[0]))\n",
    "    layer.replace_many(*chosen_edges)\n",
    "\n",
    "    if len(chosen_edges[0]) > 0:\n",
    "        optim.add_param_group({'params': layer.embed_linears[-1].weight_values})\n",
    "    else:\n",
    "        print(\"Empty metric\")\n",
    "\n",
    "    return {'max': max(vals, default=0), 'sum': sum(vals), 'len': len(vals), 'len_choose': layer.count_replaces[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senmodel.model.utils import freeze_all_but_last, freeze_only_last\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_sparse_recursive(model, train_loader, val_loader, num_epochs, metric, edge_replacement_func=None,\n",
    "                           window_size=3, threshold=0.1, lr=5e-4, choose_threshold=0.3, aggregation_mode='mean', replace_all_epochs=3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_losses = []\n",
    "\n",
    "    len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            if len(len_choose) > replace_all_epochs and i > window_size:\n",
    "                freeze_all_but_last(model)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(all_targets, all_preds)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "        new_l = dict()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        if edge_replacement_func and len(val_losses) > window_size:\n",
    "            recent_changes = [abs(val_losses[i] - val_losses[i - 1]) for i in range(-window_size, 0)]\n",
    "            avg_change = sum(recent_changes) / window_size\n",
    "            if avg_change < threshold:\n",
    "                print(f\"{len_choose=}\")\n",
    "                len_ch = len_choose[-1] if len(len_choose) > replace_all_epochs else None\n",
    "                new_l = edge_replacement_func(model, optimizer, val_loader, metric, choose_threshold, aggregation_mode, len_ch)\n",
    "                # Замораживаем все слои кроме последнего\n",
    "                val_losses = []\n",
    "                len_choose = get_model_last_layer(model).count_replaces\n",
    "\n",
    "        wandb.log({'val_loss': val_loss, 'val_accuracy': val_accuracy, 'train_loss': train_loss} | new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senmodel.metrics.nonlinearity_metrics import *\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = [\n",
    "    AbsGradientEdgeMetric(criterion),\n",
    "    ReversedAbsGradientEdgeMetric(criterion),\n",
    "    SNIPMetric(criterion),\n",
    "    MagnitudeL2Metric(criterion),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: fedornigretuk. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\"num_epochs\": 50,\n",
    "               \"metric\": metrics[0],\n",
    "               \"aggregation_mode\": \"mean\",\n",
    "               \"choose_threshold\": 0.1,\n",
    "               \"window_size\": 3,\n",
    "               \"threshold\": 0.05,\n",
    "               \"lr\": 5e-4,\n",
    "               \"replace_all_epochs\": 2\n",
    "               }\n",
    "\n",
    "name = \", \".join(\n",
    "    f\"{key}: {value.__class__.__name__ if key == 'metric' else value}\"\n",
    "    for key, value in hyperparams.items()\n",
    ")\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\fedor/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Coding\\PY\\self-expanding-nets\\experiments\\training\\wandb\\run-20250217_132752-p5ch7l0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/p5ch7l0x' target=\"_blank\">cifar10, num_epochs: 50, metric: AbsGradientEdgeMetric, aggregation_mode: mean, choose_threshold: 0.1, window_size: 3, threshold: 0.05, lr: 0.0005, replace_all_epochs: 2</a></strong> to <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fedornigretuk/self-expanding-nets/runs/p5ch7l0x' target=\"_blank\">https://wandb.ai/fedornigretuk/self-expanding-nets/runs/p5ch7l0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [03:51<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sparse_model = ResnetExp()\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project=\"self-expanding-nets\",\n",
    "    name=f\"cifar10, {name}\",\n",
    "    tags=[\"complex model\", hyperparams[\"metric\"].__class__.__name__],\n",
    "    group=\"new freeze 2\"\n",
    ")\n",
    "\n",
    "train_sparse_recursive(sparse_model, train_loader, val_loader,\n",
    "                       edge_replacement_func=edge_replacement_func_new_layer, **hyperparams)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
